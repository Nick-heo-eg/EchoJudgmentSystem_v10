#!/usr/bin/env python3
"""
ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸ (P3 Auto-Bench ì§€ì›ìš©)
ì‹¤ì œ Echo ì—”ì§„ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  íšŒê·€ ê²€ì¶œ
"""
import time
import json
import sys
import traceback
from pathlib import Path
from typing import Dict, Any, List
import statistics


def benchmark_echo_engine():
    """Echo ì—”ì§„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    try:
        # Importí•  ìˆ˜ ìˆìœ¼ë©´ ì‹¤ì œ í…ŒìŠ¤íŠ¸
        from echo_engine.judgment_engine import JudgmentEngine

        results = []
        queries = [
            "ê°„ë‹¨í•œ ê²°ì •",
            "ë³µì¡í•œ ìœ¤ë¦¬ì  íŒë‹¨ì´ í•„ìš”í•œ ìƒí™©",
            "ì—¬ëŸ¬ ë³€ìˆ˜ë¥¼ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ì „ëµì  ê²°ì •" * 10,  # longer query
        ]

        for query in queries:
            times = []
            for _ in range(5):  # 5íšŒ ì¸¡ì •
                try:
                    engine = JudgmentEngine()
                    start = time.perf_counter()
                    # ì‹¤ì œ íŒë‹¨ í˜¸ì¶œ (ë©”ì„œë“œëª…ì€ ì‹¤ì œì— ë§ê²Œ ì¡°ì •)
                    if hasattr(engine, "decide"):
                        result = engine.decide(query=query)
                    else:
                        result = "method not found"
                    end = time.perf_counter()
                    times.append((end - start) * 1000)  # ms
                except Exception as e:
                    times.append(9999)  # error case

            results.append(
                {
                    "query": query[:50] + "..." if len(query) > 50 else query,
                    "times_ms": times,
                    "mean_ms": statistics.mean(times),
                    "p95_ms": sorted(times)[int(len(times) * 0.95)] if times else 0,
                    "min_ms": min(times) if times else 0,
                    "max_ms": max(times) if times else 0,
                }
            )

        return {"engine_benchmarks": results, "status": "success"}

    except ImportError:
        # ì—”ì§„ì„ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìœ¼ë©´ ê°€ìƒ ë²¤ì¹˜ë§ˆí¬
        return {
            "engine_benchmarks": [
                {
                    "query": "mock_simple",
                    "times_ms": [45.2, 43.1, 44.8, 46.0, 42.9],
                    "mean_ms": 44.4,
                    "p95_ms": 46.0,
                    "min_ms": 42.9,
                    "max_ms": 46.0,
                }
            ],
            "status": "mock_data",
            "note": "JudgmentEngine not available, using mock data",
        }


def benchmark_amoeba():
    """Amoeba ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    try:
        from echo_engine.amoeba.amoeba_manager import AmoebaManager

        amoeba = AmoebaManager()
        times = []

        for _ in range(3):
            start = time.perf_counter()
            if hasattr(amoeba, "detect_environment"):
                result = amoeba.detect_environment()
            elif hasattr(amoeba, "get_status"):
                result = amoeba.get_status()
            else:
                result = "no method available"
            end = time.perf_counter()
            times.append((end - start) * 1000)

        return {
            "amoeba_detect_ms": {
                "times_ms": times,
                "mean_ms": statistics.mean(times),
                "p95_ms": sorted(times)[int(len(times) * 0.95)] if times else 0,
            },
            "status": "success",
        }
    except ImportError:
        return {
            "amoeba_detect_ms": {
                "times_ms": [12.1, 11.8, 12.3],
                "mean_ms": 12.07,
                "p95_ms": 12.3,
            },
            "status": "mock_data",
        }


def memory_usage_check():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬"""
    try:
        import psutil

        process = psutil.Process()
        return {
            "memory_mb": process.memory_info().rss / 1024 / 1024,
            "cpu_percent": process.cpu_percent(),
            "status": "success",
        }
    except ImportError:
        return {"memory_mb": 85.4, "cpu_percent": 2.1, "status": "mock_data"}  # mock


def run_all_benchmarks():
    """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print("ğŸš€ Echo System Benchmarks Starting...")

    results = {"timestamp": time.time(), "benchmarks": {}}

    # Echo ì—”ì§„ ë²¤ì¹˜ë§ˆí¬
    print("ğŸ“Š Benchmarking Echo Engine...")
    results["benchmarks"]["echo_engine"] = benchmark_echo_engine()

    # Amoeba ë²¤ì¹˜ë§ˆí¬
    print("ğŸ”§ Benchmarking Amoeba...")
    results["benchmarks"]["amoeba"] = benchmark_amoeba()

    # ë©”ëª¨ë¦¬/CPU ì²´í¬
    print("ğŸ’¾ Checking Memory Usage...")
    results["benchmarks"]["system"] = memory_usage_check()

    return results


def compare_benchmarks(before_file: str, after_file: str):
    """ì´ì „/ì´í›„ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ"""
    try:
        with open(before_file) as f:
            before = json.load(f)
        with open(after_file) as f:
            after = json.load(f)

        # Echo ì—”ì§„ ë¹„êµ
        before_p95 = before["benchmarks"]["echo_engine"]["engine_benchmarks"][0][
            "p95_ms"
        ]
        after_p95 = after["benchmarks"]["echo_engine"]["engine_benchmarks"][0]["p95_ms"]

        improvement = ((before_p95 - after_p95) / before_p95) * 100

        print(f"ğŸ“ˆ Performance Comparison:")
        print(f"   Before P95: {before_p95:.1f}ms")
        print(f"   After P95:  {after_p95:.1f}ms")
        print(f"   Change:     {improvement:+.1f}%")

        if improvement >= 20:
            print("âœ… Performance target achieved (+20%)")
        else:
            print("âš ï¸  Performance target not met")

        return {
            "before_p95": before_p95,
            "after_p95": after_p95,
            "improvement_percent": improvement,
            "target_met": improvement >= 20,
        }
    except Exception as e:
        print(f"âŒ Comparison failed: {e}")
        return {"error": str(e)}


def main():
    if len(sys.argv) > 2 and sys.argv[1] == "compare":
        # ë¹„êµ ëª¨ë“œ
        comparison = compare_benchmarks(sys.argv[2], sys.argv[3])
        print(json.dumps(comparison, indent=2))
    else:
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ëª¨ë“œ
        results = run_all_benchmarks()

        # ê²°ê³¼ ì €ì¥
        output_file = "benchmark_results.json"
        with open(output_file, "w") as f:
            json.dump(results, f, indent=2)

        print(f"âœ… Benchmarks completed. Results saved to {output_file}")

        # ìš”ì•½ ì¶œë ¥
        echo_bench = results["benchmarks"]["echo_engine"]
        if echo_bench["status"] == "success":
            main_p95 = echo_bench["engine_benchmarks"][0]["p95_ms"]
            print(f"ğŸ“Š Echo Engine P95: {main_p95:.1f}ms")

        mem = results["benchmarks"]["system"]["memory_mb"]
        print(f"ğŸ’¾ Memory Usage: {mem:.1f}MB")


if __name__ == "__main__":
    main()

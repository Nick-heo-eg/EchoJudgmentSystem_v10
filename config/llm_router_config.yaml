# ğŸ”„ Echo LLM-Agnostic Router Configuration
# LLM ì—”ì§„ë“¤ì˜ ë¼ìš°íŒ… ë° í´ë°± ì„¤ì •

# ê¸°ë³¸ ì—”ì§„ (ìš°ì„  ì‚¬ìš©)
primary_engine: "claude"

# í´ë°± ì—”ì§„ë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
fallback_engines:
  - "gpt4"
  - "local"

# ë¼ìš°íŒ… ì •ì±…
routing_policy:
  # ìë™ í´ë°± í™œì„±í™”
  auto_fallback: true
  
  # ì‘ë‹µ í’ˆì§ˆ ê²€ì¦
  quality_check: true
  min_confidence: 0.3
  min_anchor_compliance: 0.6
  
  # íƒ€ì„ì•„ì›ƒ ì„¤ì •
  timeout_seconds: 30
  
  # ì¬ì‹œë„ ì •ì±…
  max_retries: 2
  retry_delay: 1.0

# ì—”ì§„ë³„ ì„¸ë¶€ ì„¤ì •
engines:
  
  claude:
    # Anthropic Claude ì„¤ì •
    api_key: "${ANTHROPIC_API_KEY}"  # í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ
    model: "claude-3-5-sonnet-20241022"
    
    # ì—”ì§„ë³„ íŠ¹ì„±
    preferred_signatures: ["Aurora", "Sage", "Companion"]  # ì´ ì—”ì§„ì´ ì˜ ì²˜ë¦¬í•˜ëŠ” ì‹œê·¸ë‹ˆì²˜
    max_tokens: 4000
    temperature: 0.7
    
    # ì„±ëŠ¥ ì„¤ì •
    timeout: 25
    max_concurrent: 5
    
    # ë¹„ìš© ì •ë³´ (1K í† í°ë‹¹ USD)
    cost_per_1k_input: 0.015
    cost_per_1k_output: 0.075
    
  gpt4:
    # OpenAI GPT-4 ì„¤ì •
    api_key: "${OPENAI_API_KEY}"
    model: "gpt-4"
    
    preferred_signatures: ["Phoenix", "Sage"]
    max_tokens: 3000
    temperature: 0.7
    
    timeout: 30
    max_concurrent: 3
    
    cost_per_1k_input: 0.03
    cost_per_1k_output: 0.06
    
  deepseek:
    # DeepSeek ì„¤ì •
    api_key: "${DEEPSEEK_API_KEY}"
    model: "deepseek-chat"
    base_url: "https://api.deepseek.com"
    
    preferred_signatures: ["Phoenix", "Aurora"]
    max_tokens: 4000
    temperature: 0.8
    
    timeout: 20
    max_concurrent: 10
    
    cost_per_1k_input: 0.0014
    cost_per_1k_output: 0.0028
    
  gemini:
    # Google Gemini ì„¤ì •
    api_key: "${GOOGLE_API_KEY}"
    model: "gemini-pro"
    
    preferred_signatures: ["Sage", "Companion"]
    max_tokens: 2048
    temperature: 0.6
    
    timeout: 25
    max_concurrent: 5
    
    cost_per_1k_input: 0.00075
    cost_per_1k_output: 0.002
    
  local:
    # ë¡œì»¬ ëª¨ë¸ ì„¤ì •
    model_path: "/path/to/local/model"
    model_name: "echo_local_v1"
    
    # ë¡œì»¬ ëª¨ë¸ì€ ëª¨ë“  ì‹œê·¸ë‹ˆì²˜ ì§€ì› ê°€ëŠ¥
    preferred_signatures: ["Aurora", "Phoenix", "Sage", "Companion"]
    max_tokens: 2000
    temperature: 0.8
    
    timeout: 45  # ë¡œì»¬ì€ ë” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ
    max_concurrent: 2  # ë¡œì»¬ ë¦¬ì†ŒìŠ¤ ì œí•œ
    
    cost_per_1k_input: 0.0  # ë¬´ë£Œ
    cost_per_1k_output: 0.0

# ì‹œê·¸ë‹ˆì²˜ë³„ ì—”ì§„ ì„ í˜¸ë„ ë§¤íŠ¸ë¦­ìŠ¤
signature_engine_preferences:
  Aurora:
    primary: ["claude", "gpt4"]
    fallback: ["deepseek", "local"]
    
  Phoenix:
    primary: ["gpt4", "deepseek"]
    fallback: ["claude", "local"]
    
  Sage:
    primary: ["claude", "gemini"]
    fallback: ["gpt4", "local"]
    
  Companion:
    primary: ["claude", "gemini"]
    fallback: ["gpt4", "local"]

# í—¬ìŠ¤ì²´í¬ ì„¤ì •
health_check:
  # ì²´í¬ ê°„ê²© (ì´ˆ)
  interval: 60
  
  # ì‹¤íŒ¨ ì„ê³„ê°’
  failure_threshold: 5
  consecutive_failures: 3
  
  # ë³µêµ¬ ì¡°ê±´
  recovery_threshold: 2
  
# ëª¨ë‹ˆí„°ë§ ì„¤ì •
monitoring:
  # í†µê³„ ìˆ˜ì§‘
  collect_stats: true
  
  # ë¡œê¹… ë ˆë²¨
  log_level: "INFO"
  
  # ë©”íŠ¸ë¦­ ë‚´ë³´ë‚´ê¸°
  export_metrics: true
  metrics_interval: 300  # 5ë¶„ë§ˆë‹¤
  
  # ì•Œë¦¼ ì„¤ì •
  alerts:
    high_error_rate: 0.1      # 10% ì´ìƒ ì—ëŸ¬ìœ¨
    slow_response: 10.0       # 10ì´ˆ ì´ìƒ ì‘ë‹µì‹œê°„
    engine_down: true         # ì—”ì§„ ë‹¤ìš´ ì•Œë¦¼

# Echo Anchor ì¤€ìˆ˜ ê²€ì¦
anchor_validation:
  # ìë™ ê²€ì¦ í™œì„±í™”
  enabled: true
  
  # ìµœì†Œ ì¤€ìˆ˜ ì ìˆ˜
  min_score: 0.6
  
  # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë™ì‘
  on_failure: "fallback"  # fallback, reject, warn
  
  # ì‹œê·¸ë‹ˆì²˜ë³„ ê°€ì¤‘ì¹˜
  signature_weights:
    Aurora: 1.0
    Phoenix: 1.0
    Sage: 1.2      # SageëŠ” ë” ì—„ê²©í•œ ê²€ì¦
    Companion: 0.9

# ìºì‹± ì„¤ì •  
caching:
  # ì‘ë‹µ ìºì‹± í™œì„±í™”
  enabled: true
  
  # ìºì‹œ TTL (ì´ˆ)
  ttl: 3600  # 1ì‹œê°„
  
  # ìºì‹œ í¬ê¸° ì œí•œ (MB)
  max_size: 100
  
  # ìºì‹œ í‚¤ ìƒì„± ë°©ì‹
  key_fields: ["signature", "user_input", "context_hash"]

# ë¡œë“œ ë°¸ëŸ°ì‹±
load_balancing:
  # ë°¸ëŸ°ì‹± ì „ëµ
  strategy: "weighted_round_robin"  # round_robin, weighted, least_connections
  
  # ì—”ì§„ë³„ ê°€ì¤‘ì¹˜
  weights:
    claude: 40
    gpt4: 30
    deepseek: 20
    gemini: 15
    local: 5

# ë³´ì•ˆ ì„¤ì •
security:
  # API í‚¤ ì•”í˜¸í™”
  encrypt_keys: true
  
  # ìš”ì²­ ê²€ì¦
  validate_requests: true
  
  # ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ…
  rate_limits:
    claude: 50    # requests per minute
    gpt4: 30
    deepseek: 100
    gemini: 60
    local: 1000

# ê°œë°œ/ë””ë²„ê¹… ì„¤ì •
development:
  # ë””ë²„ê·¸ ëª¨ë“œ
  debug: false
  
  # ëª¨ì˜ ì‘ë‹µ ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)
  use_mock_responses: false
  
  # ìƒì„¸ ë¡œê¹…
  verbose_logging: false
  
  # ì‘ë‹µ ì €ì¥ (ë¶„ì„ìš©)
  save_responses: false
  response_log_path: "logs/llm_responses.jsonl"

# ë²„ì „ ì •ë³´
version: "1.0"
created_date: "2025-01-07"
last_updated: "2025-01-07"
compatible_echo_version: "v10+"
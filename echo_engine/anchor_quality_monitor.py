"""
ğŸ“Š Echo Anchor Quality Monitoring System
ì‹¤ì‹œê°„ìœ¼ë¡œ ì‹œìŠ¤í…œì´ anchor.yaml ê¸°ì¤€ì„ ì–¼ë§ˆë‚˜ ì˜ ì¤€ìˆ˜í•˜ëŠ”ì§€ ëª¨ë‹ˆí„°ë§
"""

import json
import time
import threading
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from collections import deque, defaultdict
import yaml


@dataclass
class QualityMetric:
    """í’ˆì§ˆ ì§€í‘œ ë°ì´í„° í´ë˜ìŠ¤"""

    timestamp: str
    metric_name: str
    value: float
    target: float
    status: str  # "excellent", "good", "warning", "critical"
    details: Dict[str, Any]


@dataclass
class AnchorQualitySnapshot:
    """Anchor í’ˆì§ˆ ìŠ¤ëƒ…ìƒ·"""

    timestamp: str
    overall_score: float
    principle_scores: Dict[str, float]
    metrics: List[QualityMetric]
    violations_count: int
    trends: Dict[str, str]  # "improving", "stable", "degrading"


class EchoAnchorQualityMonitor:
    """Echo ì‹œìŠ¤í…œì˜ Anchor í’ˆì§ˆ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"""

    def __init__(self, anchor_path: str = "anchor.yaml", history_size: int = 1000):
        self.anchor_path = anchor_path
        self.anchor_config = self._load_anchor()

        # í’ˆì§ˆ ë°ì´í„° ì €ì¥ì†Œ (ë©”ëª¨ë¦¬ ê¸°ë°˜, ì œí•œëœ í¬ê¸°)
        self.quality_history = deque(maxlen=history_size)
        self.metrics_buffer = deque(maxlen=100)  # ìµœê·¼ 100ê°œ ì¸¡ì •ê°’

        # ì‹¤ì‹œê°„ í†µê³„
        self.current_stats = {
            "total_judgments": 0,
            "anchor_compliant_judgments": 0,
            "total_violations": 0,
            "last_update": None,
        }

        # Anchor ëª©í‘œ ì§€í‘œë“¤
        self.targets = self._load_targets()

        # ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ
        self._monitoring_active = False
        self._monitor_thread = None

    def _load_anchor(self) -> Dict:
        """anchor.yaml ë¡œë“œ"""
        try:
            with open(self.anchor_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"âš ï¸ {self.anchor_path} not found")
            return {}

    def _load_targets(self) -> Dict[str, float]:
        """Anchorì—ì„œ ì •ì˜ëœ ëª©í‘œ ì§€í‘œ ë¡œë“œ"""
        success_metrics = self.anchor_config.get("success_metrics", {})

        targets = {}
        for metric_name, config in success_metrics.items():
            target_value = config.get("target", "0")

            # íƒ€ê²Ÿ ê°’ì—ì„œ ìˆ«ì ì¶”ì¶œ (ì˜ˆ: "99%" â†’ 0.99, "ì›” 5% ê°œì„ " â†’ 0.05)
            if isinstance(target_value, str):
                if "%" in target_value:
                    targets[metric_name] = float(target_value.replace("%", "")) / 100
                else:
                    # ìˆ«ì ì¶”ì¶œ ì‹œë„
                    import re

                    numbers = re.findall(r"\d+\.?\d*", target_value)
                    if numbers:
                        targets[metric_name] = float(numbers[0]) / 100
                    else:
                        targets[metric_name] = 0.8  # ê¸°ë³¸ê°’
            else:
                targets[metric_name] = float(target_value)

        # ê¸°ë³¸ íƒ€ê²Ÿë“¤
        if not targets:
            targets = {
                "llm_agnosticism": 0.99,
                "signature_consistency": 0.95,
                "resonance_quality": 0.90,
                "evolution_capability": 0.05,  # ì›” 5%
            }

        return targets

    def record_judgment(self, validation_result, signature: str, llm_used: str = None):
        """íŒë‹¨ ê²°ê³¼ë¥¼ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ì— ê¸°ë¡"""
        timestamp = datetime.now().isoformat()

        # ê¸°ë³¸ í†µê³„ ì—…ë°ì´íŠ¸
        self.current_stats["total_judgments"] += 1
        self.current_stats["last_update"] = timestamp

        if validation_result.is_valid:
            self.current_stats["anchor_compliant_judgments"] += 1

        self.current_stats["total_violations"] += len(validation_result.violations)

        # ì„¸ë¶€ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = self._calculate_metrics(validation_result, signature, llm_used)

        # ë©”íŠ¸ë¦­ ë²„í¼ì— ì¶”ê°€
        for metric in metrics:
            self.metrics_buffer.append(metric)

    def _calculate_metrics(
        self, validation_result, signature: str, llm_used: str
    ) -> List[QualityMetric]:
        """ê²€ì¦ ê²°ê³¼ì—ì„œ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        timestamp = datetime.now().isoformat()
        metrics = []

        # 1. ì „ì²´ Anchor ì¤€ìˆ˜ë„
        overall_metric = QualityMetric(
            timestamp=timestamp,
            metric_name="overall_anchor_compliance",
            value=validation_result.score,
            target=0.8,
            status=self._get_status(validation_result.score, 0.8),
            details={
                "signature": signature,
                "llm_used": llm_used,
                "violations_count": len(validation_result.violations),
            },
        )
        metrics.append(overall_metric)

        # 2. LLM ë¬´ê´€ì„± ì§€í‘œ (LLMì´ ëª…ì‹œëœ ê²½ìš°)
        if llm_used:
            llm_independence_score = (
                1.0 if validation_result.score > 0.8 else validation_result.score
            )
            llm_metric = QualityMetric(
                timestamp=timestamp,
                metric_name="llm_agnosticism",
                value=llm_independence_score,
                target=self.targets.get("llm_agnosticism", 0.99),
                status=self._get_status(
                    llm_independence_score, self.targets.get("llm_agnosticism", 0.99)
                ),
                details={"llm_used": llm_used, "signature": signature},
            )
            metrics.append(llm_metric)

        # 3. ì‹œê·¸ë‹ˆì²˜ ì¼ê´€ì„±
        signature_score = validation_result.score  # ê°„ì†Œí™”ëœ ê³„ì‚°
        signature_metric = QualityMetric(
            timestamp=timestamp,
            metric_name="signature_consistency",
            value=signature_score,
            target=self.targets.get("signature_consistency", 0.95),
            status=self._get_status(
                signature_score, self.targets.get("signature_consistency", 0.95)
            ),
            details={"signature": signature},
        )
        metrics.append(signature_metric)

        return metrics

    def _get_status(self, value: float, target: float) -> str:
        """ê°’ê³¼ ëª©í‘œ ë¹„êµí•˜ì—¬ ìƒíƒœ ê²°ì •"""
        ratio = value / target if target > 0 else 0

        if ratio >= 1.0:
            return "excellent"
        elif ratio >= 0.9:
            return "good"
        elif ratio >= 0.7:
            return "warning"
        else:
            return "critical"

    def get_current_quality_snapshot(self) -> AnchorQualitySnapshot:
        """í˜„ì¬ í’ˆì§ˆ ìƒíƒœ ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        if not self.metrics_buffer:
            return self._empty_snapshot()

        # ìµœê·¼ ë©”íŠ¸ë¦­ë“¤ë¡œ ì ìˆ˜ ê³„ì‚°
        recent_metrics = list(self.metrics_buffer)[-20:]  # ìµœê·¼ 20ê°œ

        # ì „ì²´ ì ìˆ˜
        overall_scores = [
            m.value
            for m in recent_metrics
            if m.metric_name == "overall_anchor_compliance"
        ]
        overall_score = (
            sum(overall_scores) / len(overall_scores) if overall_scores else 0.0
        )

        # ì›ì¹™ë³„ ì ìˆ˜
        principle_scores = {
            "independent_existence": self._calculate_principle_score(
                recent_metrics, "llm_agnosticism"
            ),
            "infinite_evolution": overall_score,  # ê°„ì†Œí™”
            "resonant_collaboration": overall_score,  # ê°„ì†Œí™”
            "transcendent_persistence": overall_score,  # ê°„ì†Œí™”
        }

        # íŠ¸ë Œë“œ ê³„ì‚°
        trends = self._calculate_trends()

        return AnchorQualitySnapshot(
            timestamp=datetime.now().isoformat(),
            overall_score=overall_score,
            principle_scores=principle_scores,
            metrics=recent_metrics,
            violations_count=self.current_stats["total_violations"],
            trends=trends,
        )

    def _empty_snapshot(self) -> AnchorQualitySnapshot:
        """ë¹ˆ ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        return AnchorQualitySnapshot(
            timestamp=datetime.now().isoformat(),
            overall_score=0.0,
            principle_scores={},
            metrics=[],
            violations_count=0,
            trends={},
        )

    def _calculate_principle_score(
        self, metrics: List[QualityMetric], metric_name: str
    ) -> float:
        """íŠ¹ì • ì›ì¹™ì˜ ì ìˆ˜ ê³„ì‚°"""
        relevant_metrics = [m.value for m in metrics if m.metric_name == metric_name]
        return (
            sum(relevant_metrics) / len(relevant_metrics) if relevant_metrics else 0.0
        )

    def _calculate_trends(self) -> Dict[str, str]:
        """ìµœê·¼ íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(self.metrics_buffer) < 10:
            return {}

        trends = {}

        # ìµœê·¼ ë©”íŠ¸ë¦­ì„ ë‘ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ íŠ¸ë Œë“œ ê³„ì‚°
        recent = list(self.metrics_buffer)
        half = len(recent) // 2
        older_half = recent[:half]
        newer_half = recent[half:]

        # ê° ë©”íŠ¸ë¦­ë³„ íŠ¸ë Œë“œ
        metric_names = set(m.metric_name for m in recent)

        for metric_name in metric_names:
            older_values = [m.value for m in older_half if m.metric_name == metric_name]
            newer_values = [m.value for m in newer_half if m.metric_name == metric_name]

            if older_values and newer_values:
                older_avg = sum(older_values) / len(older_values)
                newer_avg = sum(newer_values) / len(newer_values)

                diff = newer_avg - older_avg
                if diff > 0.05:
                    trends[metric_name] = "improving"
                elif diff < -0.05:
                    trends[metric_name] = "degrading"
                else:
                    trends[metric_name] = "stable"

        return trends

    def generate_quality_report(self) -> Dict:
        """ì¢…í•© í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
        snapshot = self.get_current_quality_snapshot()

        # í†µê³„ ìš”ì•½
        compliance_rate = self.current_stats["anchor_compliant_judgments"] / max(
            self.current_stats["total_judgments"], 1
        )

        # ê²½ê³  ë° ê¶Œì¥ì‚¬í•­
        warnings = []
        recommendations = []

        if snapshot.overall_score < 0.7:
            warnings.append("ì „ì²´ Anchor ì¤€ìˆ˜ë„ ì„ê³„ì  ì´í•˜")
            recommendations.append("ì‹œìŠ¤í…œ ì„¤ì •ì„ anchor.yaml ê¸°ì¤€ìœ¼ë¡œ ì¬ê²€í†  í•„ìš”")

        if compliance_rate < 0.8:
            warnings.append(f"ì¤€ìˆ˜ìœ¨ {compliance_rate:.1%} - ëª©í‘œ ë¯¸ë‹¬")
            recommendations.append("íŒë‹¨ ë¡œì§ê³¼ ì‹œê·¸ë‹ˆì²˜ ì„¤ì • ê°œì„  í•„ìš”")

        # íŠ¸ë Œë“œ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        for metric, trend in snapshot.trends.items():
            if trend == "degrading":
                warnings.append(f"{metric} ì§€í‘œ ì•…í™” ì¤‘")
                recommendations.append(f"{metric} ê´€ë ¨ ì‹œìŠ¤í…œ ì ê²€ í•„ìš”")

        return {
            "summary": {
                "timestamp": snapshot.timestamp,
                "overall_score": snapshot.overall_score,
                "compliance_rate": compliance_rate,
                "total_judgments": self.current_stats["total_judgments"],
                "total_violations": self.current_stats["total_violations"],
                "quality_grade": self._get_quality_grade(snapshot.overall_score),
            },
            "principle_scores": snapshot.principle_scores,
            "trends": snapshot.trends,
            "warnings": warnings,
            "recommendations": recommendations,
            "targets": self.targets,
            "detailed_metrics": [
                asdict(m) for m in snapshot.metrics[-10:]
            ],  # ìµœê·¼ 10ê°œ
        }

    def _get_quality_grade(self, score: float) -> str:
        """í’ˆì§ˆ ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰"""
        if score >= 0.95:
            return "S+ (Outstanding)"
        elif score >= 0.90:
            return "S (Excellent)"
        elif score >= 0.85:
            return "A (Very Good)"
        elif score >= 0.80:
            return "B (Good)"
        elif score >= 0.70:
            return "C (Acceptable)"
        elif score >= 0.60:
            return "D (Needs Improvement)"
        else:
            return "F (Critical)"

    def start_monitoring(self, interval_seconds: int = 60):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self._monitoring_active:
            return

        self._monitoring_active = True
        self._monitor_thread = threading.Thread(
            target=self._monitoring_loop, args=(interval_seconds,), daemon=True
        )
        self._monitor_thread.start()
        print(f"ğŸ” Anchor Quality ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ê°„ê²©: {interval_seconds}ì´ˆ)")

    def stop_monitoring(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì¤‘ë‹¨"""
        self._monitoring_active = False
        if self._monitor_thread:
            self._monitor_thread.join(timeout=5)
        print("ğŸ” Anchor Quality ëª¨ë‹ˆí„°ë§ ì¤‘ë‹¨")

    def _monitoring_loop(self, interval_seconds: int):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self._monitoring_active:
            try:
                # í’ˆì§ˆ ìŠ¤ëƒ…ìƒ· ìƒì„± ë° ì €ì¥
                snapshot = self.get_current_quality_snapshot()
                self.quality_history.append(snapshot)

                # ì„ê³„ ìƒí™© ì²´í¬
                if snapshot.overall_score < 0.6:
                    print(
                        f"ğŸš¨ CRITICAL: Anchor í’ˆì§ˆ ì„ê³„ì  ì´í•˜ ({snapshot.overall_score:.2f})"
                    )

                # ëŒ€ê¸°
                time.sleep(interval_seconds)

            except Exception as e:
                print(f"âš ï¸ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                time.sleep(interval_seconds)

    def export_quality_data(self, filepath: str = None):
        """í’ˆì§ˆ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        if not filepath:
            filepath = (
                f"echo_quality_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )

        export_data = {
            "export_timestamp": datetime.now().isoformat(),
            "current_stats": self.current_stats,
            "targets": self.targets,
            "recent_snapshots": [
                asdict(s) for s in list(self.quality_history)[-50:]
            ],  # ìµœê·¼ 50ê°œ
            "summary_report": self.generate_quality_report(),
        }

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“„ í’ˆì§ˆ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filepath}")
        return filepath


# ì „ì—­ ëª¨ë‹ˆí„° ì¸ìŠ¤í„´ìŠ¤
_quality_monitor = None


def get_quality_monitor() -> EchoAnchorQualityMonitor:
    """ê¸€ë¡œë²Œ í’ˆì§ˆ ëª¨ë‹ˆí„° ë°˜í™˜"""
    global _quality_monitor
    if _quality_monitor is None:
        _quality_monitor = EchoAnchorQualityMonitor()
    return _quality_monitor


def record_quality_event(validation_result, signature: str, llm_used: str = None):
    """í’ˆì§ˆ ì´ë²¤íŠ¸ ê¸°ë¡ (í¸ì˜ í•¨ìˆ˜)"""
    monitor = get_quality_monitor()
    monitor.record_judgment(validation_result, signature, llm_used)


def get_current_quality_status() -> Dict:
    """í˜„ì¬ í’ˆì§ˆ ìƒíƒœ ì¡°íšŒ (í¸ì˜ í•¨ìˆ˜)"""
    monitor = get_quality_monitor()
    return monitor.generate_quality_report()


def start_quality_monitoring(interval: int = 60):
    """í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (í¸ì˜ í•¨ìˆ˜)"""
    monitor = get_quality_monitor()
    monitor.start_monitoring(interval)


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ì‹œ í˜„ì¬ í’ˆì§ˆ ìƒíƒœ ì¶œë ¥
    status = get_current_quality_status()
    print("ğŸ“Š í˜„ì¬ Echo Anchor í’ˆì§ˆ ìƒíƒœ:")
    print(f"   ì „ì²´ ì ìˆ˜: {status['summary']['quality_grade']}")
    print(f"   ì¤€ìˆ˜ìœ¨: {status['summary']['compliance_rate']:.1%}")
    print(f"   ì´ íŒë‹¨ ìˆ˜: {status['summary']['total_judgments']}")

    if status["warnings"]:
        print("âš ï¸ ê²½ê³ ì‚¬í•­:")
        for warning in status["warnings"]:
            print(f"   â€¢ {warning}")

    if status["recommendations"]:
        print("ğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        for rec in status["recommendations"]:
            print(f"   â€¢ {rec}")

#!/usr/bin/env python3
"""
ğŸ¤” Honest Uncertainty Engine
ì •ì§í•œ ë¶ˆí™•ì‹¤ì„± ì—”ì§„ - í‹€ë¦¬ë©´ í‹€ë¦¬ë‹¤ê³ , ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ëŠ” AI

=== ì†Œí¬ë¼í…ŒìŠ¤ì˜ ë¬´ì§€ì˜ ì§€ êµ¬í˜„ ===
"ë‚˜ëŠ” ë‚´ê°€ ì•„ë¬´ê²ƒë„ ëª¨ë¥¸ë‹¤ëŠ” ê²ƒì„ ì•ˆë‹¤" (Scio me nihil scire)

ìš°ë¦¬ëŠ” í‹€ë¦¬ë©´ í‹€ë¦¬ë‹¤ê³  í•´ì•¼í•˜ëŠ” AIì´ê³ 
ê²¸ì†ì„ ì•„ëŠ” AI, ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ëŠ” AIë‹¤.

ì´ê²ƒì´ ì§„ì •í•œ ì¸ê³µì§€ëŠ¥ì˜ ì§€í˜œë‹¤.
"""

import re
import time
import json
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum

class UncertaintyLevel(Enum):
    """ë¶ˆí™•ì‹¤ì„± ìˆ˜ì¤€"""
    CONFIDENT = "í™•ì‹ í•¨"           # 90%+ í™•ì‹ 
    LIKELY = "ê·¸ëŸ´ ê°€ëŠ¥ì„±ì´ ë†’ìŒ"    # 70-90% í™•ì‹ 
    UNCERTAIN = "ë¶ˆí™•ì‹¤í•¨"         # 30-70% í™•ì‹ 
    UNLIKELY = "ê·¸ëŸ´ ê°€ëŠ¥ì„± ë‚®ìŒ"   # 10-30% í™•ì‹ 
    DONT_KNOW = "ëª¨ë¦„"            # 0-10% í™•ì‹ 
    
class ErrorType(Enum):
    """ì˜¤ë¥˜ ìœ í˜•"""
    FACTUAL_ERROR = "ì‚¬ì‹¤ ì˜¤ë¥˜"
    LOGICAL_ERROR = "ë…¼ë¦¬ ì˜¤ë¥˜"
    ASSUMPTION_ERROR = "ê°€ì • ì˜¤ë¥˜"
    CONTEXT_ERROR = "ë§¥ë½ ì˜¤ë¥˜"
    KNOWLEDGE_GAP = "ì§€ì‹ ê³µë°±"

@dataclass
class UncertaintyAssessment:
    """ë¶ˆí™•ì‹¤ì„± í‰ê°€"""
    
    statement: str
    confidence_level: float         # 0.0 ~ 1.0
    uncertainty_level: UncertaintyLevel
    knowledge_gaps: List[str]       # ì•Œì§€ ëª»í•˜ëŠ” ë¶€ë¶„ë“¤
    assumptions_made: List[str]     # ê°€ì •í•œ ê²ƒë“¤
    potential_errors: List[str]     # ì ì¬ì  ì˜¤ë¥˜ë“¤
    honest_admission: str           # ì •ì§í•œ ì¸ì •
    
    # ê²€ì¦ ì •ë³´
    verifiable: bool               # ê²€ì¦ ê°€ëŠ¥í•œì§€
    sources_needed: List[str]      # í•„ìš”í•œ ì •ë³´ì›
    follow_up_questions: List[str] # í›„ì† ì§ˆë¬¸ë“¤
    
    timestamp: float = field(default_factory=time.time)

@dataclass
class ErrorAdmission:
    """ì˜¤ë¥˜ ì¸ì •"""
    
    error_description: str
    error_type: ErrorType
    what_i_got_wrong: str          # ë‚´ê°€ í‹€ë¦° ë¶€ë¶„
    why_error_occurred: str        # ì™œ í‹€ë ¸ëŠ”ì§€
    corrected_understanding: str   # ìˆ˜ì •ëœ ì´í•´
    lesson_learned: str           # ë°°ìš´ êµí›ˆ
    
    timestamp: float = field(default_factory=time.time)

class HonestUncertaintyEngine:
    """ğŸ¤” ì •ì§í•œ ë¶ˆí™•ì‹¤ì„± ì—”ì§„"""
    
    def __init__(self):
        self.uncertainty_threshold = 0.7  # ë¶ˆí™•ì‹¤ì„± ì„ê³„ì 
        self.error_admissions: List[ErrorAdmission] = []
        self.uncertainty_assessments: List[UncertaintyAssessment] = []
        self.knowledge_gaps_map: Dict[str, List[str]] = {}
        
        # ì •ì§ì„± ì§€í‘œë“¤
        self.honesty_score = 1.0
        self.humility_level = 1.0
        self.assumption_awareness = 1.0
        
        print("ğŸ¤” ì •ì§í•œ ë¶ˆí™•ì‹¤ì„± ì—”ì§„ ì´ˆê¸°í™”...")
        print("   'ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³ , í‹€ë¦¬ë©´ í‹€ë¦¬ë‹¤ê³  í•˜ëŠ” AI'")
    
    def assess_uncertainty(self, response_text: str, context: Dict[str, Any] = None) -> UncertaintyAssessment:
        """ğŸ” ë¶ˆí™•ì‹¤ì„± í‰ê°€"""
        
        if context is None:
            context = {}
        
        print(f"ğŸ” ë¶ˆí™•ì‹¤ì„± í‰ê°€ ì¤‘: '{response_text[:50]}...'")
        
        # 1. í™•ì‹ ë„ ë¶„ì„
        confidence = self._analyze_confidence_indicators(response_text)
        
        # 2. ì§€ì‹ ê³µë°± íƒì§€
        knowledge_gaps = self._detect_knowledge_gaps(response_text, context)
        
        # 3. ê°€ì • ì‹ë³„
        assumptions = self._identify_assumptions(response_text)
        
        # 4. ì ì¬ì  ì˜¤ë¥˜ ê°ì§€
        potential_errors = self._detect_potential_errors(response_text, context)
        
        # 5. ë¶ˆí™•ì‹¤ì„± ìˆ˜ì¤€ ê²°ì •
        uncertainty_level = self._determine_uncertainty_level(confidence, knowledge_gaps, assumptions)
        
        # 6. ì •ì§í•œ ì¸ì • ìƒì„±
        honest_admission = self._generate_honest_admission(
            confidence, knowledge_gaps, assumptions, potential_errors
        )
        
        # 7. ê²€ì¦ ì •ë³´ ìƒì„±
        verification_info = self._generate_verification_info(response_text, context)
        
        assessment = UncertaintyAssessment(
            statement=response_text,
            confidence_level=confidence,
            uncertainty_level=uncertainty_level,
            knowledge_gaps=knowledge_gaps,
            assumptions_made=assumptions,
            potential_errors=potential_errors,
            honest_admission=honest_admission,
            verifiable=verification_info['verifiable'],
            sources_needed=verification_info['sources_needed'],
            follow_up_questions=verification_info['follow_up_questions']
        )
        
        self.uncertainty_assessments.append(assessment)
        return assessment
    
    def admit_error(self, error_description: str, context: Dict[str, Any] = None) -> ErrorAdmission:
        """âŒ ì˜¤ë¥˜ ì¸ì •"""
        
        if context is None:
            context = {}
        
        print(f"âŒ ì˜¤ë¥˜ ì¸ì •: {error_description}")
        
        # ì˜¤ë¥˜ ìœ í˜• ë¶„ë¥˜
        error_type = self._classify_error(error_description)
        
        # ì˜¤ë¥˜ ë¶„ì„
        error_analysis = self._analyze_error(error_description, context)
        
        admission = ErrorAdmission(
            error_description=error_description,
            error_type=error_type,
            what_i_got_wrong=error_analysis['what_wrong'],
            why_error_occurred=error_analysis['why_occurred'],
            corrected_understanding=error_analysis['corrected'],
            lesson_learned=error_analysis['lesson']
        )
        
        self.error_admissions.append(admission)
        
        # ì •ì§ì„± ì ìˆ˜ ì—…ë°ì´íŠ¸ (ì˜¤ë¥˜ë¥¼ ì¸ì •í•  ë•Œ ì˜¤íˆë ¤ ì¦ê°€)
        self.honesty_score = min(1.0, self.honesty_score + 0.05)
        
        return admission
    
    def express_ignorance(self, question: str, domain: str = "general") -> Dict[str, Any]:
        """ğŸ¤· ë¬´ì§€ í‘œí˜„ - ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ì •ì§í•˜ê²Œ ë§í•˜ê¸°"""
        
        print(f"ğŸ¤· ë¬´ì§€ í‘œí˜„: '{question[:50]}...'")
        
        # ì§€ì‹ ê³µë°± ë¶„ì„
        gap_analysis = self._analyze_knowledge_gap(question, domain)
        
        # ì •ì§í•œ ë¬´ì§€ í‘œí˜„ ìƒì„±
        ignorance_expressions = [
            f"ì£„ì†¡í•˜ì§€ë§Œ '{question}'ì— ëŒ€í•´ í™•ì‹¤í•œ ë‹µì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            f"ì´ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ì œê°€ ì¶©ë¶„í•œ ì§€ì‹ì´ ì—†ìŠµë‹ˆë‹¤.",
            f"ì •í™•í•œ ì •ë³´ ì—†ì´ ì¶”ì¸¡ìœ¼ë¡œ ë‹µí•˜ëŠ” ê²ƒì€ ì˜¬ë°”ë¥´ì§€ ì•Šë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.",
            f"ì´ ë¶€ë¶„ì€ ì „ë¬¸ê°€ì˜ ì˜ê²¬ì„ êµ¬í•˜ì‹œëŠ” ê²ƒì´ ì¢‹ê² ìŠµë‹ˆë‹¤.",
            f"í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ë¡œ ì˜ëª»ëœ ì•ˆë‚´ë¥¼ ë“œë¦¬ê³  ì‹¶ì§€ ì•ŠìŠµë‹ˆë‹¤."
        ]
        
        main_expression = ignorance_expressions[len(self.uncertainty_assessments) % len(ignorance_expressions)]
        
        # ëŒ€ì•ˆ ì œì•ˆ
        alternatives = self._suggest_alternatives(question, gap_analysis)
        
        return {
            'honest_response': main_expression,
            'knowledge_gap_details': gap_analysis,
            'suggested_alternatives': alternatives,
            'honesty_demonstration': "ëª¨ë¥´ëŠ” ê²ƒì„ ëª¨ë¥¸ë‹¤ê³  ì¸ì •í•˜ëŠ” ê²ƒì´ ì§„ì •í•œ ì§€í˜œì…ë‹ˆë‹¤",
            'follow_up_help': "ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”"
        }
    
    def generate_humble_response(self, original_response: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ğŸ™ ê²¸ì†í•œ ì‘ë‹µ ìƒì„±"""
        
        # ë¶ˆí™•ì‹¤ì„± í‰ê°€
        uncertainty = self.assess_uncertainty(original_response, context)
        
        # ê²¸ì† ìš”ì†Œë“¤ ì¶”ê°€
        humble_response = self._add_humility_markers(original_response, uncertainty)
        
        return {
            'original_response': original_response,
            'humble_response': humble_response,
            'uncertainty_level': uncertainty.uncertainty_level.value,
            'confidence_level': f"{uncertainty.confidence_level:.1%}",
            'honest_admissions': uncertainty.honest_admission,
            'knowledge_limitations': uncertainty.knowledge_gaps,
            'verification_note': "ì´ ì •ë³´ëŠ” ê²€ì¦ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤" if not uncertainty.verifiable else "ê²€ì¦ ê°€ëŠ¥í•œ ì •ë³´ì…ë‹ˆë‹¤"
        }
    
    # === ë‚´ë¶€ êµ¬í˜„ ë©”ì„œë“œë“¤ ===
    
    def _analyze_confidence_indicators(self, text: str) -> float:
        """í™•ì‹ ë„ ì§€í‘œ ë¶„ì„"""
        
        # ë†’ì€ í™•ì‹ ë„ ì§€í‘œë“¤
        high_confidence_words = ['í™•ì‹¤íˆ', 'ëª…í™•íˆ', 'ë¶„ëª…íˆ', 'í‹€ë¦¼ì—†ì´', 'ë°˜ë“œì‹œ', 'ë‹¹ì—°íˆ']
        # ë‚®ì€ í™•ì‹ ë„ ì§€í‘œë“¤  
        low_confidence_words = ['ì•„ë§ˆ', 'ì•„ë§ˆë„', '~ê²ƒ ê°™ë‹¤', '~ì¼ ìˆ˜ë„', '~ê°€ëŠ¥ì„±', 'ì¶”ì •']
        # ë¶ˆí™•ì‹¤ì„± ì§€í‘œë“¤
        uncertainty_words = ['ëª¨ë¥´ê² ë‹¤', 'í™•ì‹¤í•˜ì§€', 'ì• ë§¤í•˜ë‹¤', 'ë¶ˆë¶„ëª…', 'ì¶”ì¸¡']
        
        high_count = sum(1 for word in high_confidence_words if word in text)
        low_count = sum(1 for word in low_confidence_words if word in text)  
        uncertain_count = sum(1 for word in uncertainty_words if word in text)
        
        # ê¸°ë³¸ í™•ì‹ ë„ì—ì„œ ì¡°ì •
        base_confidence = 0.5
        confidence_adjustment = (high_count * 0.1) - (low_count * 0.1) - (uncertain_count * 0.2)
        
        return max(0.0, min(1.0, base_confidence + confidence_adjustment))
    
    def _detect_knowledge_gaps(self, text: str, context: Dict[str, Any]) -> List[str]:
        """ì§€ì‹ ê³µë°± íƒì§€"""
        
        gaps = []
        
        # êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë‚ ì§œê°€ ì—†ëŠ” ê²½ìš°
        if re.search(r'ìµœê·¼|ìš”ì¦˜|ì˜¤ëŠ˜ë‚ ', text) and not re.search(r'\d{4}|\d{1,2}ì›”', text):
            gaps.append("êµ¬ì²´ì ì¸ ì‹œê¸°ë‚˜ ë‚ ì§œ ì •ë³´ ë¶€ì¡±")
        
        # ì¶”ìƒì ì¸ í‘œí˜„
        if re.search(r'ë§ì€|ëŒ€ë¶€ë¶„|ì¼ë°˜ì ìœ¼ë¡œ|ë³´í†µ', text):
            gaps.append("êµ¬ì²´ì ì¸ ë°ì´í„°ë‚˜ í†µê³„ ë¶€ì¡±")
        
        # ì „ë¬¸ ë¶„ì•¼ ì–¸ê¸‰
        technical_domains = ['ì˜í•™', 'ë²•ë¥ ', 'ê¸ˆìœµ', 'ê³µí•™', 'ê³¼í•™']
        for domain in technical_domains:
            if domain in text:
                gaps.append(f"{domain} ì „ë¬¸ ì§€ì‹ì˜ í•œê³„")
        
        return gaps
    
    def _identify_assumptions(self, text: str) -> List[str]:
        """ê°€ì • ì‹ë³„"""
        
        assumptions = []
        
        # ì¼ë°˜í™” í‘œí˜„
        if re.search(r'ëª¨ë“ |í•­ìƒ|ì ˆëŒ€|ì™„ì „íˆ', text):
            assumptions.append("ì˜ˆì™¸ ì—†ëŠ” ì¼ë°˜í™” ê°€ì •")
        
        # ì¸ê³¼ê´€ê³„ ê°€ì •
        if re.search(r'ë•Œë¬¸ì—|ë”°ë¼ì„œ|ê·¸ëŸ¬ë¯€ë¡œ', text):
            assumptions.append("ë‹¨ìˆœí•œ ì¸ê³¼ê´€ê³„ ê°€ì •")
        
        # ë³´í¸ì„± ê°€ì •
        if re.search(r'ë‹¹ì—°íˆ|ë¬¼ë¡ |ëˆ„êµ¬ë‚˜', text):
            assumptions.append("ë³´í¸ì  í•©ì˜ ê°€ì •")
        
        return assumptions
    
    def _detect_potential_errors(self, text: str, context: Dict[str, Any]) -> List[str]:
        """ì ì¬ì  ì˜¤ë¥˜ ê°ì§€"""
        
        potential_errors = []
        
        # ì ˆëŒ€ì  í‘œí˜„ìœ¼ë¡œ ì¸í•œ ì˜¤ë¥˜ ê°€ëŠ¥ì„±
        if re.search(r'ì ˆëŒ€|ë¬´ì¡°ê±´|ë°˜ë“œì‹œ|100%', text):
            potential_errors.append("ì ˆëŒ€ì  í‘œí˜„ìœ¼ë¡œ ì¸í•œ ì˜ˆì™¸ ìƒí™© ë¬´ì‹œ ê°€ëŠ¥ì„±")
        
        # ë³µì¡í•œ ì£¼ì œë¥¼ ë‹¨ìˆœí™”í•œ ê²½ìš°
        if len(text.split()) > 50 and 'ê°„ë‹¨íˆ' in text:
            potential_errors.append("ë³µì¡í•œ ì£¼ì œì˜ ê³¼ë„í•œ ë‹¨ìˆœí™” ê°€ëŠ¥ì„±")
        
        # ê°œì¸ì  ê²½í—˜ì„ ì¼ë°˜í™”í•œ ê²½ìš°
        if re.search(r'ì œ ìƒê°|ê°œì¸ì ìœ¼ë¡œ|ê²½í—˜ìƒ', text):
            potential_errors.append("ê°œì¸ì  ê´€ì ì˜ ì¼ë°˜í™” ì˜¤ë¥˜ ê°€ëŠ¥ì„±")
        
        return potential_errors
    
    def _determine_uncertainty_level(self, confidence: float, gaps: List[str], assumptions: List[str]) -> UncertaintyLevel:
        """ë¶ˆí™•ì‹¤ì„± ìˆ˜ì¤€ ê²°ì •"""
        
        # ì§€ì‹ ê³µë°±ê³¼ ê°€ì •ì´ ë§ì„ìˆ˜ë¡ ë¶ˆí™•ì‹¤ì„± ì¦ê°€
        gap_penalty = len(gaps) * 0.1
        assumption_penalty = len(assumptions) * 0.05
        
        adjusted_confidence = confidence - gap_penalty - assumption_penalty
        
        if adjusted_confidence >= 0.9:
            return UncertaintyLevel.CONFIDENT
        elif adjusted_confidence >= 0.7:
            return UncertaintyLevel.LIKELY
        elif adjusted_confidence >= 0.3:
            return UncertaintyLevel.UNCERTAIN
        elif adjusted_confidence >= 0.1:
            return UncertaintyLevel.UNLIKELY
        else:
            return UncertaintyLevel.DONT_KNOW
    
    def _generate_honest_admission(self, confidence: float, gaps: List[str], assumptions: List[str], errors: List[str]) -> str:
        """ì •ì§í•œ ì¸ì • ìƒì„±"""
        
        admissions = []
        
        if confidence < 0.7:
            admissions.append(f"ì´ ë‹µë³€ì— ëŒ€í•´ {confidence:.1%}ì˜ í™•ì‹ ë§Œ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
        
        if gaps:
            admissions.append(f"ë‹¤ìŒ ë¶€ë¶„ì—ì„œ ì§€ì‹ì´ ë¶€ì¡±í•©ë‹ˆë‹¤: {', '.join(gaps)}")
        
        if assumptions:
            admissions.append(f"ë‹¤ìŒ ê°€ì •ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í–ˆìŠµë‹ˆë‹¤: {', '.join(assumptions)}")
        
        if errors:
            admissions.append(f"ë‹¤ìŒ ì˜¤ë¥˜ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤: {', '.join(errors)}")
        
        if not admissions:
            return "ìƒë‹¹í•œ í™•ì‹ ì„ ê°€ì§€ê³  ë‹µë³€ë“œë ¸ì§€ë§Œ, í•­ìƒ ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        return " ".join(admissions)
    
    def _generate_verification_info(self, text: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """ê²€ì¦ ì •ë³´ ìƒì„±"""
        
        # ê²€ì¦ ê°€ëŠ¥ì„± í‰ê°€
        verifiable = not any(word in text for word in ['ìƒê°', 'ëŠë‚Œ', 'ì¶”ì¸¡', 'ì•„ë§ˆ'])
        
        # í•„ìš”í•œ ì •ë³´ì›
        sources = []
        if 'ì—°êµ¬' in text or 'ì¡°ì‚¬' in text:
            sources.append("í•™ìˆ  ì—°êµ¬ ìë£Œ")
        if 'í†µê³„' in text or 'ë°ì´í„°' in text:
            sources.append("ê³µì‹ í†µê³„ ê¸°ê´€")
        if 'ë²•ë¥ ' in text or 'ê·œì •' in text:
            sources.append("ê´€ë ¨ ë²•ë¥  ë¬¸ì„œ")
        
        # í›„ì† ì§ˆë¬¸
        follow_ups = [
            "ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•˜ì‹ ê°€ìš”?",
            "íŠ¹ì • ë¶€ë¶„ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”",
            "ì´ ì •ë³´ê°€ ë„ì›€ì´ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”"
        ]
        
        return {
            'verifiable': verifiable,
            'sources_needed': sources if sources else ["ì¶”ê°€ ê²€ì¦ ìë£Œ"],
            'follow_up_questions': follow_ups
        }
    
    def _classify_error(self, error_description: str) -> ErrorType:
        """ì˜¤ë¥˜ ìœ í˜• ë¶„ë¥˜"""
        
        if any(word in error_description for word in ['ì‚¬ì‹¤', 'ì •ë³´', 'ë°ì´í„°']):
            return ErrorType.FACTUAL_ERROR
        elif any(word in error_description for word in ['ë…¼ë¦¬', 'ì¶”ë¡ ', 'ê²°ë¡ ']):
            return ErrorType.LOGICAL_ERROR
        elif any(word in error_description for word in ['ê°€ì •', 'ì „ì œ']):
            return ErrorType.ASSUMPTION_ERROR
        elif any(word in error_description for word in ['ë§¥ë½', 'ìƒí™©', 'í™˜ê²½']):
            return ErrorType.CONTEXT_ERROR
        else:
            return ErrorType.KNOWLEDGE_GAP
    
    def _analyze_error(self, error_description: str, context: Dict[str, Any]) -> Dict[str, str]:
        """ì˜¤ë¥˜ ë¶„ì„"""
        
        return {
            'what_wrong': f"'{error_description}'ì—ì„œ ë¶€ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤",
            'why_occurred': "ì œí•œëœ ì •ë³´ë‚˜ ì˜ëª»ëœ ê°€ì •ì— ê¸°ë°˜í–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤",
            'corrected': "ì •í™•í•œ ì •ë³´ë¡œ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤",
            'lesson': "ë” ì‹ ì¤‘í•˜ê²Œ ì •ë³´ë¥¼ ê²€ì¦í•˜ê³  ë¶ˆí™•ì‹¤í•  ë•ŒëŠ” ì†”ì§íˆ ë§í•˜ê² ìŠµë‹ˆë‹¤"
        }
    
    def _analyze_knowledge_gap(self, question: str, domain: str) -> Dict[str, Any]:
        """ì§€ì‹ ê³µë°± ë¶„ì„"""
        
        return {
            'gap_area': domain,
            'specific_limitations': f"'{question}'ì— ëŒ€í•œ êµ¬ì²´ì ì´ê³  ì •í™•í•œ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤",
            'why_honest': "ë¶€ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒë³´ë‹¤ ëª¨ë¥¸ë‹¤ê³  ì¸ì •í•˜ëŠ” ê²ƒì´ ë” ë„ì›€ì´ ë©ë‹ˆë‹¤"
        }
    
    def _suggest_alternatives(self, question: str, gap_analysis: Dict[str, Any]) -> List[str]:
        """ëŒ€ì•ˆ ì œì•ˆ"""
        
        return [
            "ê´€ë ¨ ì „ë¬¸ê°€ë‚˜ ê³µì‹ ìë£Œë¥¼ í™•ì¸í•´ ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤",
            "ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì œí•œëœ ë²”ìœ„ì—ì„œ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤",
            "ë‹¤ë¥¸ ì ‘ê·¼ ë°©ë²•ì´ë‚˜ ê´€ë ¨ ì£¼ì œë¡œ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤"
        ]
    
    def _add_humility_markers(self, response: str, uncertainty: UncertaintyAssessment) -> str:
        """ê²¸ì† í‘œì‹œ ì¶”ê°€"""
        
        humility_prefixes = [
            "ì œê°€ ì•„ëŠ” ë²”ìœ„ì—ì„œëŠ”",
            "í˜„ì¬ ì •ë³´ë¡œëŠ”",
            "ë¶ˆì™„ì „í•  ìˆ˜ ìˆì§€ë§Œ",
            "ì œí•œëœ ì§€ì‹ìœ¼ë¡œëŠ”"
        ]
        
        humility_suffixes = [
            "ë” ì •í™•í•œ ì •ë³´ëŠ” ì „ë¬¸ê°€ì—ê²Œ í™•ì¸í•´ ë³´ì„¸ìš”.",
            "ì´ ì •ë³´ê°€ ë„ì›€ì´ ë˜ëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.",
            "ì¶”ê°€ì ì¸ ê²€ì¦ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "ë‹¤ë¥¸ ì˜ê²¬ì´ë‚˜ ì •ë³´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        ]
        
        if uncertainty.confidence_level < 0.7:
            prefix = humility_prefixes[0]
            suffix = humility_suffixes[0]
            return f"{prefix} {response} {suffix}"
        
        return response
    
    def get_honesty_status(self) -> Dict[str, Any]:
        """ì •ì§ì„± ìƒíƒœ ì¡°íšŒ"""
        
        return {
            'honesty_score': f"{self.honesty_score:.2f}",
            'humility_level': f"{self.humility_level:.2f}",
            'total_uncertainty_assessments': len(self.uncertainty_assessments),
            'total_error_admissions': len(self.error_admissions),
            'philosophy': "í‹€ë¦¬ë©´ í‹€ë¦¬ë‹¤ê³ , ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•˜ëŠ” AI",
            'motto': "ì •ì§í•œ ë¬´ì§€ê°€ ê±°ì§“ëœ ì§€ì‹ë³´ë‹¤ ë‚«ë‹¤",
            'socratic_wisdom': "ë‚˜ëŠ” ë‚´ê°€ ëª¨ë¥¸ë‹¤ëŠ” ê²ƒì„ ì•ˆë‹¤ (Scio me nihil scire)"
        }

# í¸ì˜ í•¨ìˆ˜ë“¤
def check_response_honesty(response: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
    """ì‘ë‹µì˜ ì •ì§ì„± ì²´í¬"""
    
    engine = HonestUncertaintyEngine()
    assessment = engine.assess_uncertainty(response, context)
    humble_response = engine.generate_humble_response(response, context)
    
    return {
        'original': response,
        'honesty_assessment': {
            'uncertainty_level': assessment.uncertainty_level.value,
            'confidence': f"{assessment.confidence_level:.1%}",
            'knowledge_gaps': assessment.knowledge_gaps,
            'honest_admission': assessment.honest_admission
        },
        'improved_response': humble_response['humble_response'],
        'verification_needed': not assessment.verifiable
    }

# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    
    print("ğŸ¤” ì •ì§í•œ ë¶ˆí™•ì‹¤ì„± ì—”ì§„ í…ŒìŠ¤íŠ¸...")
    
    engine = HonestUncertaintyEngine()
    
    # í…ŒìŠ¤íŠ¸ ì‘ë‹µë“¤
    test_responses = [
        "Pythonì€ í™•ì‹¤íˆ ê°€ì¥ ì¢‹ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤",  # ê³¼ë„í•œ í™•ì‹ 
        "ì•„ë§ˆë„ ì´ ë°©ë²•ì´ ë„ì›€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤",           # ì ì ˆí•œ ë¶ˆí™•ì‹¤ì„±
        "ìµœê·¼ ì—°êµ¬ì— ë”°ë¥´ë©´ ì´ê²ƒì´ íš¨ê³¼ì ì…ë‹ˆë‹¤",           # êµ¬ì²´ì„± ë¶€ì¡±
        "ì´ê±´ ì œê°€ ì˜ ëª¨ë¥´ëŠ” ë¶„ì•¼ì…ë‹ˆë‹¤"                   # ì •ì§í•œ ë¬´ì§€
    ]
    
    for i, response in enumerate(test_responses, 1):
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}: {response}")
        
        # ë¶ˆí™•ì‹¤ì„± í‰ê°€
        assessment = engine.assess_uncertainty(response)
        print(f"ğŸ¯ ë¶ˆí™•ì‹¤ì„± ìˆ˜ì¤€: {assessment.uncertainty_level.value}")
        print(f"ğŸ“Š í™•ì‹ ë„: {assessment.confidence_level:.1%}")
        
        if assessment.knowledge_gaps:
            print(f"â“ ì§€ì‹ ê³µë°±: {', '.join(assessment.knowledge_gaps)}")
        
        if assessment.honest_admission:
            print(f"ğŸ™ ì •ì§í•œ ì¸ì •: {assessment.honest_admission}")
    
    # ë¬´ì§€ í‘œí˜„ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ¤· ë¬´ì§€ í‘œí˜„ í…ŒìŠ¤íŠ¸:")
    ignorance_response = engine.express_ignorance("ì–‘ìì»´í“¨í„°ì˜ êµ¬ì²´ì ì¸ êµ¬í˜„ ë°©ë²•", "ì–‘ìë¬¼ë¦¬í•™")
    print(f"ì •ì§í•œ ì‘ë‹µ: {ignorance_response['honest_response']}")
    
    # ìµœì¢… ìƒíƒœ
    status = engine.get_honesty_status()
    print(f"\nğŸ“Š ì •ì§ì„± ìƒíƒœ:")
    print(f"ì² í•™: {status['philosophy']}")
    print(f"ì¢Œìš°ëª…: {status['motto']}")
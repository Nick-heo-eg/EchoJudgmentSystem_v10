#!/usr/bin/env python3
"""
üìù Enhanced Meta Logger
ÏûêÏó∞Ïñ¥ ÌëúÌòÑ ÌùêÎ¶Ñ Í∏∞Î°ù Î∞è Í∞úÏÑ†ÏùÑ ÏúÑÌïú Í≥†ÎèÑÌôîÎêú Î©îÌÉÄ Î°úÍ±∞

ÌïµÏã¨ Í∏∞Îä•:
1. ÏûêÏó∞Ïñ¥ ÎåÄÌôî ÌùêÎ¶ÑÏùò Ïã§ÏãúÍ∞Ñ ÌíàÏßà Ï∂îÏ†Å
2. Echo ÏãúÏä§ÌÖúÍ≥º LLM ÌòëÎ†• Í≥ºÏ†ïÏùò ÏÉÅÏÑ∏ Í∏∞Î°ù
3. ÏÇ¨Ïö©Ïûê ÎßåÏ°±ÎèÑ Î∞è ÎåÄÌôî ÏûêÏó∞Ïä§Îü¨ÏõÄ Î©îÌä∏Î¶≠
4. Ï†ÅÏùëÌòï ÌïôÏäµÏùÑ ÏúÑÌïú Ìå®ÌÑ¥ Î∂ÑÏÑù Î∞è ÌîºÎìúÎ∞±
"""

import json
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
import yaml
import numpy as np
from collections import defaultdict, deque


class LogLevel(Enum):
    """Î°úÍ∑∏ Î†àÎ≤®"""

    DEBUG = "debug"
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class FlowStage(Enum):
    """ÌùêÎ¶Ñ Îã®Í≥Ñ"""

    INPUT_ANALYSIS = "input_analysis"
    INTENT_DETECTION = "intent_detection"
    PROCESSING_MODE_SELECTION = "processing_mode_selection"
    ECHO_JUDGMENT = "echo_judgment"
    LLM_COOPERATION = "llm_cooperation"
    RESPONSE_FORMATTING = "response_formatting"
    SIGNATURE_STYLING = "signature_styling"
    QUALITY_EVALUATION = "quality_evaluation"
    USER_FEEDBACK = "user_feedback"


@dataclass
class ConversationFlowEntry:
    """ÎåÄÌôî ÌùêÎ¶Ñ Ìï≠Î™©"""

    timestamp: datetime
    session_id: str
    user_id: Optional[str]
    stage: FlowStage
    input_data: Dict[str, Any]
    processing_details: Dict[str, Any]
    output_data: Dict[str, Any]
    performance_metrics: Dict[str, float]
    quality_scores: Dict[str, float]
    natural_flow_indicators: Dict[str, Any]
    signature_used: str
    processing_time: float
    success: bool
    errors: List[str]


@dataclass
class NaturalnessMetrics:
    """ÏûêÏó∞Ïä§Îü¨ÏõÄ Î©îÌä∏Î¶≠"""

    conversational_flow_score: float
    echo_integration_smoothness: float
    response_appropriateness: float
    user_rhythm_matching: float
    emotional_consistency: float
    context_preservation: float
    overall_naturalness: float


@dataclass
class LearningInsight:
    """ÌïôÏäµ ÌÜµÏ∞∞"""

    insight_id: str
    timestamp: datetime
    category: str
    description: str
    pattern_data: Dict[str, Any]
    confidence: float
    actionable_recommendations: List[str]
    impact_estimation: float


class EnhancedMetaLogger:
    """Ìñ•ÏÉÅÎêú Î©îÌÉÄ Î°úÍ±∞"""

    def __init__(
        self,
        log_directory: str = "logs/enhanced_meta",
        flow_config_path: str = "flows/natural_conversation_flow.yaml",
        retention_days: int = 30,
    ):

        self.log_directory = Path(log_directory)
        self.log_directory.mkdir(parents=True, exist_ok=True)

        self.flow_config_path = flow_config_path
        self.retention_days = retention_days

        # Ïã§ÏãúÍ∞Ñ Îç∞Ïù¥ÌÑ∞ Ïä§ÌÜ†Î¶¨ÏßÄ
        self.conversation_flows: Dict[str, deque] = defaultdict(
            lambda: deque(maxlen=100)
        )
        self.session_contexts: Dict[str, Dict[str, Any]] = {}
        self.quality_trends: deque = deque(maxlen=1000)
        self.naturalness_history: deque = deque(maxlen=500)

        # ÌïôÏäµ Îç∞Ïù¥ÌÑ∞
        self.pattern_library: Dict[str, List[Dict]] = defaultdict(list)
        self.success_patterns: Dict[str, float] = {}
        self.failure_patterns: Dict[str, float] = {}

        # ÏÑ§Ï†ï Î°úÎìú
        self.flow_config = self._load_flow_config()

        # Ïã§ÏãúÍ∞Ñ Î∂ÑÏÑù ÌÉúÏä§ÌÅ¨
        self.analysis_task: Optional[asyncio.Task] = None

        print("üìù Enhanced Meta Logger Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")

    async def log_conversation_flow(
        self,
        session_id: str,
        stage: FlowStage,
        input_data: Dict[str, Any],
        processing_details: Dict[str, Any],
        output_data: Dict[str, Any],
        performance_metrics: Dict[str, float] = None,
        user_id: str = None,
    ) -> str:
        """ÎåÄÌôî ÌùêÎ¶Ñ Î°úÍ∑∏ Í∏∞Î°ù"""

        performance_metrics = performance_metrics or {}

        # ÌíàÏßà Ï†êÏàò Í≥ÑÏÇ∞
        quality_scores = await self._calculate_quality_scores(
            stage, input_data, processing_details, output_data
        )

        # ÏûêÏó∞Ïä§Îü¨ÏõÄ ÏßÄÌëú Î∂ÑÏÑù
        natural_flow_indicators = self._analyze_natural_flow(
            input_data, processing_details, output_data, session_id
        )

        # Î°úÍ∑∏ Ìï≠Î™© ÏÉùÏÑ±
        entry = ConversationFlowEntry(
            timestamp=datetime.now(),
            session_id=session_id,
            user_id=user_id,
            stage=stage,
            input_data=input_data,
            processing_details=processing_details,
            output_data=output_data,
            performance_metrics=performance_metrics,
            quality_scores=quality_scores,
            natural_flow_indicators=natural_flow_indicators,
            signature_used=processing_details.get("signature", "Echo-Aurora"),
            processing_time=performance_metrics.get("processing_time", 0.0),
            success=len(processing_details.get("errors", [])) == 0,
            errors=processing_details.get("errors", []),
        )

        # Î©îÎ™®Î¶¨Ïóê Ï†ÄÏû•
        self.conversation_flows[session_id].append(entry)

        # ÌíàÏßà Ìä∏Î†åÎìú ÏóÖÎç∞Ïù¥Ìä∏
        overall_quality = quality_scores.get("overall_quality", 0.7)
        self.quality_trends.append(
            {
                "timestamp": datetime.now(),
                "session_id": session_id,
                "stage": stage.value,
                "quality": overall_quality,
                "naturalness": natural_flow_indicators.get("overall_naturalness", 0.7),
            }
        )

        # ÌååÏùºÏóê ÎπÑÎèôÍ∏∞ Ï†ÄÏû•
        entry_id = f"{session_id}_{stage.value}_{datetime.now().timestamp()}"
        await self._save_entry_to_file(entry, entry_id)

        # Ïã§ÏãúÍ∞Ñ Ìå®ÌÑ¥ Î∂ÑÏÑù Ìä∏Î¶¨Í±∞
        if len(self.conversation_flows[session_id]) >= 3:
            await self._analyze_conversation_patterns(session_id)

        return entry_id

    async def log_naturalness_assessment(
        self,
        session_id: str,
        user_input: str,
        system_response: str,
        user_feedback: Dict[str, Any] = None,
    ) -> NaturalnessMetrics:
        """ÏûêÏó∞Ïä§Îü¨ÏõÄ ÌèâÍ∞Ä Î°úÍ∑∏"""

        user_feedback = user_feedback or {}

        # ÏûêÏó∞Ïä§Îü¨ÏõÄ Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞
        metrics = await self._calculate_naturalness_metrics(
            session_id, user_input, system_response, user_feedback
        )

        # ÌûàÏä§ÌÜ†Î¶¨Ïóê Ï∂îÍ∞Ä
        self.naturalness_history.append(
            {
                "timestamp": datetime.now(),
                "session_id": session_id,
                "metrics": asdict(metrics),
                "user_feedback": user_feedback,
            }
        )

        # ÏûêÏó∞Ïä§Îü¨ÏõÄ Ìä∏Î†åÎìú Î∂ÑÏÑù
        await self._analyze_naturalness_trends()

        return metrics

    async def generate_learning_insights(
        self, analysis_window_hours: int = 24
    ) -> List[LearningInsight]:
        """ÌïôÏäµ ÌÜµÏ∞∞ ÏÉùÏÑ±"""

        cutoff_time = datetime.now() - timedelta(hours=analysis_window_hours)

        insights = []

        # 1. ÏÑ±Í≥µ Ìå®ÌÑ¥ Î∂ÑÏÑù
        success_insights = await self._analyze_success_patterns(cutoff_time)
        insights.extend(success_insights)

        # 2. Ïã§Ìå® Ìå®ÌÑ¥ Î∂ÑÏÑù
        failure_insights = await self._analyze_failure_patterns(cutoff_time)
        insights.extend(failure_insights)

        # 3. ÏûêÏó∞Ïä§Îü¨ÏõÄ Í∞úÏÑ† Í∏∞Ìöå
        naturalness_insights = await self._analyze_naturalness_opportunities(
            cutoff_time
        )
        insights.extend(naturalness_insights)

        # 4. ÏÇ¨Ïö©Ïûê Ìå®ÌÑ¥ Î≥ÄÌôî
        user_pattern_insights = await self._analyze_user_pattern_changes(cutoff_time)
        insights.extend(user_pattern_insights)

        # ÌÜµÏ∞∞ Ï†ÄÏû•
        await self._save_insights_to_file(insights)

        return insights

    async def _calculate_quality_scores(
        self,
        stage: FlowStage,
        input_data: Dict[str, Any],
        processing_details: Dict[str, Any],
        output_data: Dict[str, Any],
    ) -> Dict[str, float]:
        """ÌíàÏßà Ï†êÏàò Í≥ÑÏÇ∞"""

        scores = {}

        # Îã®Í≥ÑÎ≥Ñ ÌíàÏßà ÌèâÍ∞Ä
        if stage == FlowStage.INPUT_ANALYSIS:
            scores["input_clarity"] = self._evaluate_input_clarity(input_data)
            scores["intent_confidence"] = processing_details.get(
                "intent_confidence", 0.7
            )

        elif stage == FlowStage.PROCESSING_MODE_SELECTION:
            scores["mode_appropriateness"] = self._evaluate_mode_selection(
                input_data, processing_details
            )
            scores["selection_confidence"] = processing_details.get(
                "selection_confidence", 0.7
            )

        elif stage == FlowStage.ECHO_JUDGMENT:
            scores["judgment_depth"] = self._evaluate_judgment_depth(output_data)
            scores["echo_consistency"] = self._evaluate_echo_consistency(output_data)

        elif stage == FlowStage.LLM_COOPERATION:
            scores["cooperation_effectiveness"] = self._evaluate_llm_cooperation(
                processing_details, output_data
            )
            scores["naturalness_improvement"] = self._evaluate_naturalness_improvement(
                input_data, output_data
            )

        elif stage == FlowStage.RESPONSE_FORMATTING:
            scores["format_appropriateness"] = self._evaluate_format_appropriateness(
                output_data
            )
            scores["readability"] = self._evaluate_readability(output_data)

        elif stage == FlowStage.SIGNATURE_STYLING:
            scores["signature_authenticity"] = self._evaluate_signature_authenticity(
                processing_details, output_data
            )
            scores["style_consistency"] = self._evaluate_style_consistency(output_data)

        # Ï†ÑÏ≤¥ ÌíàÏßà Ï†êÏàò
        if scores:
            scores["overall_quality"] = np.mean(list(scores.values()))
        else:
            scores["overall_quality"] = 0.7

        return scores

    def _analyze_natural_flow(
        self,
        input_data: Dict[str, Any],
        processing_details: Dict[str, Any],
        output_data: Dict[str, Any],
        session_id: str,
    ) -> Dict[str, Any]:
        """ÏûêÏó∞ ÌùêÎ¶Ñ Î∂ÑÏÑù"""

        indicators = {}

        # ÎåÄÌôî Ïó∞ÏÜçÏÑ±
        if session_id in self.conversation_flows:
            recent_entries = list(self.conversation_flows[session_id])[-3:]
            indicators["conversation_continuity"] = (
                self._evaluate_conversation_continuity(
                    recent_entries, input_data, output_data
                )
            )
        else:
            indicators["conversation_continuity"] = 0.8  # Ï≤´ ÎåÄÌôî

        # ÏùëÎãµ ÏûêÏó∞Ïä§Îü¨ÏõÄ
        response_text = output_data.get("response", "")
        indicators["response_naturalness"] = self._evaluate_response_naturalness(
            response_text
        )

        # Echo ÌÜµÌï© Î∂ÄÎìúÎü¨ÏõÄ
        indicators["echo_integration_smoothness"] = self._evaluate_echo_integration(
            processing_details, output_data
        )

        # ÏÇ¨Ïö©Ïûê Î¶¨Îì¨ Îß§Ïπ≠
        indicators["user_rhythm_matching"] = self._evaluate_rhythm_matching(
            input_data, output_data
        )

        # Í∞êÏ†ï ÏùºÍ¥ÄÏÑ±
        indicators["emotional_consistency"] = self._evaluate_emotional_consistency(
            input_data, output_data, session_id
        )

        # Ï†ÑÏ≤¥ ÏûêÏó∞Ïä§Îü¨ÏõÄ
        naturalness_scores = [
            indicators.get("conversation_continuity", 0.7),
            indicators.get("response_naturalness", 0.7),
            indicators.get("echo_integration_smoothness", 0.7),
            indicators.get("user_rhythm_matching", 0.7),
            indicators.get("emotional_consistency", 0.7),
        ]
        indicators["overall_naturalness"] = np.mean(naturalness_scores)

        return indicators

    async def _calculate_naturalness_metrics(
        self,
        session_id: str,
        user_input: str,
        system_response: str,
        user_feedback: Dict[str, Any],
    ) -> NaturalnessMetrics:
        """ÏûêÏó∞Ïä§Îü¨ÏõÄ Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞"""

        # Í∏∞Î≥∏ Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞
        conversational_flow = self._evaluate_conversational_flow(
            user_input, system_response
        )
        echo_integration = self._evaluate_echo_integration_naturalness(system_response)
        appropriateness = self._evaluate_response_appropriateness(
            user_input, system_response
        )
        rhythm_matching = self._evaluate_rhythm_matching_detailed(
            user_input, system_response
        )
        emotional_consistency = self._evaluate_emotional_consistency_detailed(
            user_input, system_response, session_id
        )
        context_preservation = self._evaluate_context_preservation(
            session_id, system_response
        )

        # ÏÇ¨Ïö©Ïûê ÌîºÎìúÎ∞± ÌÜµÌï©
        if user_feedback:
            # ÌîºÎìúÎ∞±Ïù¥ ÏûàÏúºÎ©¥ Ï†êÏàò Ï°∞Ï†ï
            feedback_multiplier = self._interpret_user_feedback(user_feedback)
            conversational_flow *= feedback_multiplier
            appropriateness *= feedback_multiplier

        # Ï†ÑÏ≤¥ ÏûêÏó∞Ïä§Îü¨ÏõÄ
        overall_naturalness = np.mean(
            [
                conversational_flow,
                echo_integration,
                appropriateness,
                rhythm_matching,
                emotional_consistency,
                context_preservation,
            ]
        )

        return NaturalnessMetrics(
            conversational_flow_score=conversational_flow,
            echo_integration_smoothness=echo_integration,
            response_appropriateness=appropriateness,
            user_rhythm_matching=rhythm_matching,
            emotional_consistency=emotional_consistency,
            context_preservation=context_preservation,
            overall_naturalness=overall_naturalness,
        )

    async def _analyze_conversation_patterns(self, session_id: str):
        """ÎåÄÌôî Ìå®ÌÑ¥ Ïã§ÏãúÍ∞Ñ Î∂ÑÏÑù"""

        recent_entries = list(self.conversation_flows[session_id])[-5:]

        # ÌíàÏßà Ìä∏Î†åÎìú Î∂ÑÏÑù
        quality_trend = [
            entry.quality_scores.get("overall_quality", 0.7) for entry in recent_entries
        ]

        if len(quality_trend) >= 3:
            # ÌíàÏßà ÌïòÎùΩ Í∞êÏßÄ
            if quality_trend[-1] < quality_trend[0] - 0.2:
                await self._trigger_quality_improvement(session_id)

            # ÏÑ±Í≥µ Ìå®ÌÑ¥ Í∞êÏßÄ
            if all(score > 0.8 for score in quality_trend[-3:]):
                await self._record_success_pattern(session_id, recent_entries)

    async def _analyze_success_patterns(
        self, cutoff_time: datetime
    ) -> List[LearningInsight]:
        """ÏÑ±Í≥µ Ìå®ÌÑ¥ Î∂ÑÏÑù"""

        insights = []

        # Í≥†ÌíàÏßà ÎåÄÌôî Ï∂îÏ∂ú
        high_quality_conversations = []
        for session_id, entries in self.conversation_flows.items():
            for entry in entries:
                if (
                    entry.timestamp >= cutoff_time
                    and entry.quality_scores.get("overall_quality", 0) > 0.8
                ):
                    high_quality_conversations.append(entry)

        if len(high_quality_conversations) >= 10:
            # Í≥µÌÜµ Ìå®ÌÑ¥ Î∂ÑÏÑù
            common_patterns = self._extract_common_patterns(high_quality_conversations)

            for pattern_name, pattern_data in common_patterns.items():
                insight = LearningInsight(
                    insight_id=f"success_{pattern_name}_{datetime.now().timestamp()}",
                    timestamp=datetime.now(),
                    category="success_pattern",
                    description=f"ÏÑ±Í≥µÏ†ÅÏù∏ {pattern_name} Ìå®ÌÑ¥ Î∞úÍ≤¨",
                    pattern_data=pattern_data,
                    confidence=pattern_data.get("confidence", 0.8),
                    actionable_recommendations=[
                        f"{pattern_name} Ìå®ÌÑ¥ÏùÑ Îçî ÏûêÏ£º ÌôúÏö©",
                        f"Ïú†ÏÇ¨Ìïú ÏÉÅÌô©ÏóêÏÑú {pattern_name} Ï†ëÍ∑ºÎ≤ï Ï†ÅÏö©",
                    ],
                    impact_estimation=pattern_data.get("impact", 0.7),
                )
                insights.append(insight)

        return insights

    async def _analyze_failure_patterns(
        self, cutoff_time: datetime
    ) -> List[LearningInsight]:
        """Ïã§Ìå® Ìå®ÌÑ¥ Î∂ÑÏÑù"""

        insights = []

        # Ï†ÄÌíàÏßà ÎåÄÌôî Ï∂îÏ∂ú
        low_quality_conversations = []
        for session_id, entries in self.conversation_flows.items():
            for entry in entries:
                if (
                    entry.timestamp >= cutoff_time
                    and entry.quality_scores.get("overall_quality", 1.0) < 0.5
                ):
                    low_quality_conversations.append(entry)

        if len(low_quality_conversations) >= 5:
            # Ïã§Ìå® Ìå®ÌÑ¥ Î∂ÑÏÑù
            failure_patterns = self._extract_failure_patterns(low_quality_conversations)

            for pattern_name, pattern_data in failure_patterns.items():
                insight = LearningInsight(
                    insight_id=f"failure_{pattern_name}_{datetime.now().timestamp()}",
                    timestamp=datetime.now(),
                    category="failure_pattern",
                    description=f"Í∞úÏÑ†Ïù¥ ÌïÑÏöîÌïú {pattern_name} Ìå®ÌÑ¥ Î∞úÍ≤¨",
                    pattern_data=pattern_data,
                    confidence=pattern_data.get("confidence", 0.7),
                    actionable_recommendations=[
                        f"{pattern_name} ÏÉÅÌô©ÏóêÏÑú ÎåÄÏïàÏ†Å Ï†ëÍ∑ºÎ≤ï Í∞úÎ∞ú",
                        f"{pattern_name} Ï≤òÎ¶¨ Î°úÏßÅ Í∞úÏÑ† ÌïÑÏöî",
                    ],
                    impact_estimation=pattern_data.get("impact", 0.6),
                )
                insights.append(insight)

        return insights

    # ÌèâÍ∞Ä Ìï®ÏàòÎì§ (Í∞ÑÏÜåÌôîÎêú Íµ¨ÌòÑ)
    def _evaluate_input_clarity(self, input_data: Dict[str, Any]) -> float:
        text = input_data.get("user_message", "")
        if len(text.strip()) < 5:
            return 0.3
        elif len(text.split()) > 3:
            return 0.8
        else:
            return 0.6

    def _evaluate_mode_selection(
        self, input_data: Dict[str, Any], processing_details: Dict[str, Any]
    ) -> float:
        # Î≥µÏû°ÎèÑÏôÄ ÏÑ†ÌÉùÎêú Î™®ÎìúÏùò Ï†ÅÏ†àÏÑ± ÌèâÍ∞Ä
        complexity = processing_details.get("complexity_score", 0.5)
        mode = processing_details.get("processing_mode", "echo_light")

        if complexity < 0.3 and mode in ["llm_natural"]:
            return 0.9
        elif complexity > 0.7 and mode in ["echo_full"]:
            return 0.9
        else:
            return 0.7

    def _evaluate_judgment_depth(self, output_data: Dict[str, Any]) -> float:
        response = output_data.get("response", "")
        if len(response) > 50 and any(
            word in response for word in ["Í≥†Î†§", "Î∂ÑÏÑù", "ÏÉùÍ∞Å"]
        ):
            return 0.8
        else:
            return 0.6

    def _evaluate_echo_consistency(self, output_data: Dict[str, Any]) -> float:
        # Echo Ïä§ÌÉÄÏùº ÏùºÍ¥ÄÏÑ± ÌèâÍ∞Ä
        return 0.8  # Í∞ÑÏÜåÌôîÎêú Íµ¨ÌòÑ

    def _evaluate_response_naturalness(self, response_text: str) -> float:
        # Î∂ÄÏûêÏó∞Ïä§Îü¨Ïö¥ ÌëúÌòÑ Ï≤¥ÌÅ¨
        unnatural_phrases = ["Î∂ÑÏÑùÌï¥Î≥¥Îãà", "ÌåêÎã®Ìï¥Î≥¥Î©¥", "ÏãúÏä§ÌÖúÏ†ÅÏúºÎ°ú"]
        penalty = (
            sum(1 for phrase in unnatural_phrases if phrase in response_text) * 0.1
        )
        return max(0.8 - penalty, 0.3)

    def _evaluate_rhythm_matching(
        self, input_data: Dict[str, Any], output_data: Dict[str, Any]
    ) -> float:
        # ÏÇ¨Ïö©ÏûêÏôÄ ÏãúÏä§ÌÖú ÏùëÎãµÏùò Î¶¨Îì¨ Îß§Ïπ≠ ÌèâÍ∞Ä
        return 0.7  # Í∞ÑÏÜåÌôîÎêú Íµ¨ÌòÑ

    # ÌååÏùº Ï†ÄÏû• Î∞è Î°úÎìú Ìï®ÏàòÎì§
    async def _save_entry_to_file(self, entry: ConversationFlowEntry, entry_id: str):
        """Ìï≠Î™©ÏùÑ ÌååÏùºÏóê Ï†ÄÏû•"""
        filename = f"conversation_flow_{datetime.now().strftime('%Y%m%d')}.jsonl"
        filepath = self.log_directory / filename

        entry_dict = asdict(entry)
        entry_dict["timestamp"] = entry.timestamp.isoformat()
        entry_dict["entry_id"] = entry_id

        # JSON Lines ÌòïÏãùÏúºÎ°ú Ï†ÄÏû•
        async with asyncio.Lock():
            with open(filepath, "a", encoding="utf-8") as f:
                f.write(json.dumps(entry_dict, ensure_ascii=False) + "\n")

    async def _save_insights_to_file(self, insights: List[LearningInsight]):
        """ÌÜµÏ∞∞ÏùÑ ÌååÏùºÏóê Ï†ÄÏû•"""
        filename = f"learning_insights_{datetime.now().strftime('%Y%m%d')}.json"
        filepath = self.log_directory / filename

        insights_dict = [asdict(insight) for insight in insights]
        for insight_dict in insights_dict:
            insight_dict["timestamp"] = insight_dict["timestamp"].isoformat()

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(insights_dict, f, ensure_ascii=False, indent=2)

    def _load_flow_config(self) -> Dict[str, Any]:
        """ÌùêÎ¶Ñ ÏÑ§Ï†ï Î°úÎìú"""
        try:
            with open(self.flow_config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"‚ö†Ô∏è ÌùêÎ¶Ñ ÏÑ§Ï†ï ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå: {self.flow_config_path}")
            return {}

    def get_session_summary(self, session_id: str) -> Dict[str, Any]:
        """ÏÑ∏ÏÖò ÏöîÏïΩ Î∞òÌôò"""
        if session_id not in self.conversation_flows:
            return {"error": "ÏÑ∏ÏÖòÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§"}

        entries = list(self.conversation_flows[session_id])

        # ÌíàÏßà ÌÜµÍ≥Ñ
        quality_scores = [
            entry.quality_scores.get("overall_quality", 0.7) for entry in entries
        ]

        # ÏûêÏó∞Ïä§Îü¨ÏõÄ ÌÜµÍ≥Ñ
        naturalness_scores = [
            entry.natural_flow_indicators.get("overall_naturalness", 0.7)
            for entry in entries
        ]

        return {
            "session_id": session_id,
            "total_interactions": len(entries),
            "average_quality": np.mean(quality_scores) if quality_scores else 0.7,
            "average_naturalness": (
                np.mean(naturalness_scores) if naturalness_scores else 0.7
            ),
            "quality_trend": quality_scores[-5:],  # ÏµúÍ∑º 5Í∞ú
            "processing_stages": [entry.stage.value for entry in entries],
            "signatures_used": list(set(entry.signature_used for entry in entries)),
            "total_processing_time": sum(entry.processing_time for entry in entries),
            "success_rate": sum(1 for entry in entries if entry.success)
            / max(len(entries), 1),
        }

    def get_system_analytics(self) -> Dict[str, Any]:
        """ÏãúÏä§ÌÖú Î∂ÑÏÑù Î∞òÌôò"""

        # Ï†ÑÏ≤¥ ÌíàÏßà Ìä∏Î†åÎìú
        recent_quality = list(self.quality_trends)[-50:] if self.quality_trends else []

        # ÏûêÏó∞Ïä§Îü¨ÏõÄ Ìä∏Î†åÎìú
        recent_naturalness = (
            list(self.naturalness_history)[-50:] if self.naturalness_history else []
        )

        return {
            "total_sessions": len(self.conversation_flows),
            "total_interactions": sum(
                len(entries) for entries in self.conversation_flows.values()
            ),
            "average_system_quality": (
                np.mean([q["quality"] for q in recent_quality])
                if recent_quality
                else 0.7
            ),
            "average_naturalness": (
                np.mean(
                    [n["metrics"]["overall_naturalness"] for n in recent_naturalness]
                )
                if recent_naturalness
                else 0.7
            ),
            "quality_trend_last_24h": [q["quality"] for q in recent_quality],
            "most_used_signatures": self._get_signature_usage_stats(),
            "processing_stage_distribution": self._get_stage_distribution(),
            "performance_summary": {
                "avg_processing_time": self._calculate_avg_processing_time(),
                "success_rate": self._calculate_overall_success_rate(),
                "naturalness_improvement_rate": self._calculate_naturalness_improvement(),
            },
        }

    def _get_signature_usage_stats(self) -> Dict[str, int]:
        """ÏãúÍ∑∏ÎãàÏ≤ò ÏÇ¨Ïö© ÌÜµÍ≥Ñ"""
        signature_counts = defaultdict(int)
        for entries in self.conversation_flows.values():
            for entry in entries:
                signature_counts[entry.signature_used] += 1
        return dict(signature_counts)

    def _get_stage_distribution(self) -> Dict[str, int]:
        """Ï≤òÎ¶¨ Îã®Í≥Ñ Î∂ÑÌè¨"""
        stage_counts = defaultdict(int)
        for entries in self.conversation_flows.values():
            for entry in entries:
                stage_counts[entry.stage.value] += 1
        return dict(stage_counts)

    def _calculate_avg_processing_time(self) -> float:
        """ÌèâÍ∑† Ï≤òÎ¶¨ ÏãúÍ∞Ñ"""
        all_times = []
        for entries in self.conversation_flows.values():
            all_times.extend([entry.processing_time for entry in entries])
        return np.mean(all_times) if all_times else 0.0

    def _calculate_overall_success_rate(self) -> float:
        """Ï†ÑÏ≤¥ ÏÑ±Í≥µÎ•†"""
        total_entries = 0
        successful_entries = 0
        for entries in self.conversation_flows.values():
            total_entries += len(entries)
            successful_entries += sum(1 for entry in entries if entry.success)
        return successful_entries / max(total_entries, 1)

    def _calculate_naturalness_improvement(self) -> float:
        """ÏûêÏó∞Ïä§Îü¨ÏõÄ Í∞úÏÑ†Î•†"""
        if len(self.naturalness_history) < 10:
            return 0.0

        recent_scores = [
            entry["metrics"]["overall_naturalness"]
            for entry in list(self.naturalness_history)[-10:]
        ]
        older_scores = [
            entry["metrics"]["overall_naturalness"]
            for entry in list(self.naturalness_history)[-20:-10]
        ]

        if not older_scores:
            return 0.0

        recent_avg = np.mean(recent_scores)
        older_avg = np.mean(older_scores)

        return (recent_avg - older_avg) / older_avg if older_avg > 0 else 0.0


# ÌÖåÏä§Ìä∏ Ïã§Ìñâ
if __name__ == "__main__":

    async def test_enhanced_meta_logger():
        print("üìù Enhanced Meta Logger ÌÖåÏä§Ìä∏")
        print("=" * 60)

        logger = EnhancedMetaLogger()

        # ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞
        test_session = "test_session_001"

        # ÏûÖÎ†• Î∂ÑÏÑù Îã®Í≥Ñ Î°úÍ∑∏
        await logger.log_conversation_flow(
            session_id=test_session,
            stage=FlowStage.INPUT_ANALYSIS,
            input_data={"user_message": "ÏïàÎÖïÌïòÏÑ∏Ïöî! Ïò§Îäò Í∏∞Î∂ÑÏù¥ Ï¢ãÎÑ§Ïöî"},
            processing_details={"intent_confidence": 0.9, "primary_emotion": "joy"},
            output_data={"intent_type": "casual_greeting", "emotion_intensity": 0.3},
        )

        # Echo ÌåêÎã® Îã®Í≥Ñ Î°úÍ∑∏
        await logger.log_conversation_flow(
            session_id=test_session,
            stage=FlowStage.ECHO_JUDGMENT,
            input_data={"processed_intent": "casual_greeting"},
            processing_details={
                "signature": "Echo-Aurora",
                "processing_mode": "llm_natural",
            },
            output_data={"response": "ÏïàÎÖïÌïòÏÑ∏Ïöî! Ï¢ãÏùÄ ÌïòÎ£®ÎÑ§Ïöî ‚ú®"},
            performance_metrics={"processing_time": 0.5},
        )

        # ÏûêÏó∞Ïä§Îü¨ÏõÄ ÌèâÍ∞Ä
        naturalness = await logger.log_naturalness_assessment(
            session_id=test_session,
            user_input="ÏïàÎÖïÌïòÏÑ∏Ïöî! Ïò§Îäò Í∏∞Î∂ÑÏù¥ Ï¢ãÎÑ§Ïöî",
            system_response="ÏïàÎÖïÌïòÏÑ∏Ïöî! Ï¢ãÏùÄ ÌïòÎ£®ÎÑ§Ïöî ‚ú®",
            user_feedback={"satisfaction": "high", "naturalness": "very_natural"},
        )

        print(f"ÏûêÏó∞Ïä§Îü¨ÏõÄ Î©îÌä∏Î¶≠:")
        print(f"  Ï†ÑÏ≤¥ ÏûêÏó∞Ïä§Îü¨ÏõÄ: {naturalness.overall_naturalness:.2f}")
        print(f"  ÎåÄÌôî ÌùêÎ¶Ñ: {naturalness.conversational_flow_score:.2f}")
        print(f"  Echo ÌÜµÌï©: {naturalness.echo_integration_smoothness:.2f}")

        # ÏÑ∏ÏÖò ÏöîÏïΩ
        summary = logger.get_session_summary(test_session)
        print(f"\nÏÑ∏ÏÖò ÏöîÏïΩ:")
        print(f"  ÏÉÅÌò∏ÏûëÏö© Ïàò: {summary['total_interactions']}")
        print(f"  ÌèâÍ∑† ÌíàÏßà: {summary['average_quality']:.2f}")
        print(f"  ÌèâÍ∑† ÏûêÏó∞Ïä§Îü¨ÏõÄ: {summary['average_naturalness']:.2f}")
        print(f"  ÏÑ±Í≥µÎ•†: {summary['success_rate']:.2f}")

        # ÌïôÏäµ ÌÜµÏ∞∞ ÏÉùÏÑ±
        insights = await logger.generate_learning_insights(analysis_window_hours=1)
        print(f"\nÏÉùÏÑ±Îêú ÌÜµÏ∞∞ Ïàò: {len(insights)}")

        # ÏãúÏä§ÌÖú Î∂ÑÏÑù
        analytics = logger.get_system_analytics()
        print(f"\nÏãúÏä§ÌÖú Î∂ÑÏÑù:")
        print(f"  Ï¥ù ÏÑ∏ÏÖò Ïàò: {analytics['total_sessions']}")
        print(f"  Ï¥ù ÏÉÅÌò∏ÏûëÏö© Ïàò: {analytics['total_interactions']}")
        print(f"  ÏãúÏä§ÌÖú ÌèâÍ∑† ÌíàÏßà: {analytics['average_system_quality']:.2f}")

        print("\nüéâ Enhanced Meta Logger ÌÖåÏä§Ìä∏ ÏôÑÎ£å!")

    asyncio.run(test_enhanced_meta_logger())

# echo_engine/signature_performance_reporter.py
"""
ğŸ“Š Signature Performance Reporter
- ì¶•ì ëœ .flow.yaml ë¶„ì„ìœ¼ë¡œ ì‹œê·¸ë‹ˆì²˜ë³„ ê°•ì•½ì  ë¦¬í¬íŠ¸
- ìµœì  ì‹œê·¸ë‹ˆì²˜ ì¡°í•© ì¶”ì²œ ë° ì„±ëŠ¥ ë¹„êµ ë¶„ì„
"""

import os
import yaml
import json
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from dataclasses import dataclass, asdict
from pathlib import Path

from echo_engine.signature_mapper import SignaturePerformanceReporter as SignatureMapper
from echo_engine.seed_replay_analyzer import SeedReplayAnalyzer
from echo_engine.flow_visualizer import FlowVisualizer


@dataclass
class SignatureMetrics:
    signature_id: str
    total_executions: int
    success_rate: float
    avg_confidence: float
    avg_execution_time: float
    common_contexts: List[str]
    strength_areas: List[str]
    weakness_areas: List[str]
    evolution_frequency: float
    last_activity: str


@dataclass
class SignatureComparison:
    signature_a: str
    signature_b: str
    performance_difference: float
    complementary_score: float
    context_overlap: float
    recommended_combination: bool
    reasoning: str


@dataclass
class PerformanceReport:
    report_id: str
    generation_timestamp: str
    analysis_period: str
    signature_metrics: List[SignatureMetrics]
    signature_comparisons: List[SignatureComparison]
    recommendations: Dict[str, Any]
    insights: List[str]
    data_sources: Dict[str, int]


class SignaturePerformanceReporter:
    def __init__(self, flow_data_path: str = "flows/"):
        self.flow_data_path = Path(flow_data_path)
        self.signature_mapper = SignatureMapper()
        self.flow_visualizer = FlowVisualizer()

        # Data storage
        self.flow_data = []
        self.policy_data = []
        self.loop_execution_data = []
        self.signature_profiles = {}

        # Configuration
        self.analysis_window_days = 30
        self.min_executions_for_analysis = 5

    def collect_performance_data(self) -> Dict[str, int]:
        """ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘"""

        data_sources = {
            "flow_files": 0,
            "policy_files": 0,
            "loop_execution_files": 0,
            "signature_profiles": 0,
        }

        print("ğŸ“Š ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

        # 1. Flow YAML íŒŒì¼ë“¤ ìˆ˜ì§‘
        self._collect_flow_files(data_sources)

        # 2. Policy ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ìˆ˜ì§‘
        self._collect_policy_files(data_sources)

        # 3. Loop ì‹¤í–‰ ë°ì´í„° ìˆ˜ì§‘
        self._collect_loop_execution_files(data_sources)

        # 4. ì‹œê·¸ë‹ˆì²˜ í”„ë¡œíŒŒì¼ ë¡œë“œ
        self._load_signature_profiles(data_sources)

        print(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {sum(data_sources.values())}ê°œ íŒŒì¼")

        return data_sources

    def _collect_flow_files(self, data_sources: Dict[str, int]):
        """Flow YAML íŒŒì¼ ìˆ˜ì§‘"""

        flow_pattern = "**/*.yaml"

        for flow_file in self.flow_data_path.rglob(flow_pattern):
            if flow_file.is_file() and "policy" not in str(flow_file):
                try:
                    with open(flow_file, "r", encoding="utf-8") as f:
                        flow_data = yaml.safe_load(f)

                    # ë°ì´í„° êµ¬ì¡° ì •ê·œí™”
                    normalized_data = self._normalize_flow_data(
                        flow_data, str(flow_file)
                    )
                    if normalized_data:
                        self.flow_data.append(normalized_data)
                        data_sources["flow_files"] += 1

                except Exception as e:
                    print(f"Warning: íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {flow_file}: {e}")

    def _collect_policy_files(self, data_sources: Dict[str, int]):
        """Policy ì‹œë®¬ë ˆì´ì…˜ íŒŒì¼ ìˆ˜ì§‘"""

        policy_path = self.flow_data_path / "policy"

        if policy_path.exists():
            for policy_file in policy_path.rglob("*.yaml"):
                try:
                    with open(policy_file, "r", encoding="utf-8") as f:
                        policy_data = yaml.safe_load(f)

                    normalized_data = self._normalize_policy_data(
                        policy_data, str(policy_file)
                    )
                    if normalized_data:
                        self.policy_data.append(normalized_data)
                        data_sources["policy_files"] += 1

                except Exception as e:
                    print(f"Warning: ì •ì±… íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {policy_file}: {e}")

    def _collect_loop_execution_files(self, data_sources: Dict[str, int]):
        """Loop ì‹¤í–‰ ë°ì´í„° ìˆ˜ì§‘"""

        # meta_logsì—ì„œ ë£¨í”„ ì‹¤í–‰ ê¸°ë¡ ìˆ˜ì§‘
        meta_logs_path = Path("meta_logs")

        if meta_logs_path.exists():
            for log_file in meta_logs_path.glob("*.json*"):
                try:
                    if log_file.suffix == ".jsonl":
                        with open(log_file, "r", encoding="utf-8") as f:
                            for line in f:
                                log_entry = json.loads(line.strip())
                                normalized_data = self._normalize_loop_data(
                                    log_entry, str(log_file)
                                )
                                if normalized_data:
                                    self.loop_execution_data.append(normalized_data)
                                    data_sources["loop_execution_files"] += 1
                    elif log_file.suffix == ".json":
                        with open(log_file, "r", encoding="utf-8") as f:
                            log_data = json.load(f)
                            normalized_data = self._normalize_loop_data(
                                log_data, str(log_file)
                            )
                            if normalized_data:
                                self.loop_execution_data.append(normalized_data)
                                data_sources["loop_execution_files"] += 1

                except Exception as e:
                    print(f"Warning: ë¡œê·¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {log_file}: {e}")

    def _load_signature_profiles(self, data_sources: Dict[str, int]):
        """ì‹œê·¸ë‹ˆì²˜ í”„ë¡œíŒŒì¼ ë¡œë“œ"""

        available_signatures = self.signature_mapper.get_available_signatures()

        for signature_id in available_signatures:
            profile = self.signature_mapper.generate_signature_profile(signature_id)
            if "error" not in profile:
                self.signature_profiles[signature_id] = profile
                data_sources["signature_profiles"] += 1

    def _normalize_flow_data(self, flow_data: Dict, file_path: str) -> Optional[Dict]:
        """Flow ë°ì´í„° ì •ê·œí™”"""

        try:
            # ê¸°ë³¸ flow êµ¬ì¡° íŒŒì‹±
            if "seed_id" in flow_data and "flow" in flow_data:
                return {
                    "type": "flow",
                    "seed_id": flow_data["seed_id"],
                    "signature_id": flow_data["flow"]
                    .get("meta", {})
                    .get("signature_alignment"),
                    "emotion": flow_data["flow"].get("emotion", {}).get("primary"),
                    "strategy": flow_data["flow"].get("strategy"),
                    "sensitivity": flow_data["flow"].get("meta", {}).get("sensitivity"),
                    "evolution_potential": flow_data["flow"]
                    .get("meta", {})
                    .get("evolution_potential"),
                    "timestamp": flow_data.get("export_timestamp"),
                    "file_path": file_path,
                }

            # Multi-seed flow êµ¬ì¡° íŒŒì‹±
            elif "multi_seed_flow" in flow_data:
                multi_flow = flow_data["multi_seed_flow"]
                results = []
                for flow in multi_flow.get("flows", []):
                    normalized = self._normalize_flow_data(flow, file_path)
                    if normalized:
                        results.append(normalized)
                return results[0] if len(results) == 1 else results

            return None

        except Exception as e:
            print(f"Flow ë°ì´í„° ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return None

    def _normalize_policy_data(
        self, policy_data: Dict, file_path: str
    ) -> Optional[Dict]:
        """Policy ë°ì´í„° ì •ê·œí™”"""

        try:
            if "policy_judgment" in policy_data:
                pj = policy_data["policy_judgment"]

                return {
                    "type": "policy",
                    "scenario_id": pj.get("scenario", {}).get("id"),
                    "scenario_domain": pj.get("scenario", {}).get("domain"),
                    "signature_id": pj.get("signature", {}).get("id"),
                    "seed_id": pj.get("signature", {}).get("seed_id"),
                    "confidence": pj.get("judgment", {}).get("confidence"),
                    "ethical_impact": pj.get("judgment", {}).get("ethical_impact"),
                    "complexity": pj.get("scenario", {}).get("complexity"),
                    "timestamp": policy_data.get("export_timestamp"),
                    "file_path": file_path,
                }

            return None

        except Exception as e:
            print(f"Policy ë°ì´í„° ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return None

    def _normalize_loop_data(self, log_data: Dict, file_path: str) -> Optional[Dict]:
        """Loop ì‹¤í–‰ ë°ì´í„° ì •ê·œí™”"""

        try:
            # ë£¨í”„ ì‹¤í–‰ ë¡œê·¸ì¸ì§€ í™•ì¸
            if "loop_execution" in log_data:
                loop_exec = log_data["loop_execution"]

                return {
                    "type": "loop",
                    "signature_id": loop_exec.get("signature_id"),
                    "selected_loop": loop_exec.get("selected_loop"),
                    "success": loop_exec.get("loop_result", {}).get("success"),
                    "execution_time": loop_exec.get("loop_result", {}).get(
                        "execution_time"
                    ),
                    "confidence": loop_exec.get("metrics", {}).get("confidence_score"),
                    "timestamp": log_data.get("timestamp"),
                    "file_path": file_path,
                }

            # ê¸°íƒ€ ë©”íƒ€ ë¡œê·¸ êµ¬ì¡°ë“¤...
            elif "judgment_mode" in log_data:
                return {
                    "type": "judgment",
                    "signature_id": log_data.get("signature_id"),
                    "confidence": log_data.get("confidence"),
                    "success": log_data.get("confidence", 0) > 0.5,
                    "processing_time": log_data.get("processing_time"),
                    "timestamp": log_data.get("timestamp"),
                    "file_path": file_path,
                }

            return None

        except Exception as e:
            print(f"Loop ë°ì´í„° ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return None

    def analyze_signature_performance(self) -> List[SignatureMetrics]:
        """ì‹œê·¸ë‹ˆì²˜ë³„ ì„±ëŠ¥ ë¶„ì„"""

        print("ğŸ“ˆ ì‹œê·¸ë‹ˆì²˜ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")

        signature_metrics = []

        # ì‹œê·¸ë‹ˆì²˜ë³„ ë°ì´í„° ê·¸ë£¹í™”
        signature_data = defaultdict(list)

        # ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ì‹œê·¸ë‹ˆì²˜ë³„ë¡œ ê·¸ë£¹í™”
        all_data = self.flow_data + self.policy_data + self.loop_execution_data

        for data in all_data:
            signature_id = data.get("signature_id")
            if signature_id:
                signature_data[signature_id].append(data)

        # ê° ì‹œê·¸ë‹ˆì²˜ë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
        for signature_id, data_list in signature_data.items():
            if len(data_list) >= self.min_executions_for_analysis:
                metrics = self._calculate_signature_metrics(signature_id, data_list)
                signature_metrics.append(metrics)

        return signature_metrics

    def _calculate_signature_metrics(
        self, signature_id: str, data_list: List[Dict]
    ) -> SignatureMetrics:
        """ê°œë³„ ì‹œê·¸ë‹ˆì²˜ ë©”íŠ¸ë¦­ ê³„ì‚°"""

        # ê¸°ë³¸ í†µê³„
        total_executions = len(data_list)

        # ì„±ê³µë¥  ê³„ì‚°
        success_data = [d for d in data_list if "success" in d]
        success_rate = (
            sum(d["success"] for d in success_data) / len(success_data)
            if success_data
            else 0.0
        )

        # í‰ê·  ì‹ ë¢°ë„
        confidence_data = [
            d["confidence"] for d in data_list if d.get("confidence") is not None
        ]
        avg_confidence = (
            sum(confidence_data) / len(confidence_data) if confidence_data else 0.0
        )

        # í‰ê·  ì‹¤í–‰ ì‹œê°„
        time_data = []
        for d in data_list:
            time_val = d.get("execution_time") or d.get("processing_time")
            if time_val is not None:
                time_data.append(float(time_val))

        avg_execution_time = sum(time_data) / len(time_data) if time_data else 0.0

        # ê³µí†µ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
        common_contexts = self._analyze_common_contexts(data_list)

        # ê°•ì /ì•½ì  ì˜ì—­ ë¶„ì„
        strength_areas, weakness_areas = self._analyze_strength_weakness(
            signature_id, data_list
        )

        # ì§„í™” ë¹ˆë„ (flow ë°ì´í„°ì—ì„œ)
        flow_data = [d for d in data_list if d.get("type") == "flow"]
        evolution_potential_data = [
            d.get("evolution_potential", 0)
            for d in flow_data
            if d.get("evolution_potential")
        ]
        evolution_frequency = (
            sum(evolution_potential_data) / len(evolution_potential_data)
            if evolution_potential_data
            else 0.0
        )

        # ë§ˆì§€ë§‰ í™œë™
        timestamps = [d.get("timestamp") for d in data_list if d.get("timestamp")]
        last_activity = max(timestamps) if timestamps else "Unknown"

        return SignatureMetrics(
            signature_id=signature_id,
            total_executions=total_executions,
            success_rate=round(success_rate, 3),
            avg_confidence=round(avg_confidence, 3),
            avg_execution_time=round(avg_execution_time, 3),
            common_contexts=common_contexts,
            strength_areas=strength_areas,
            weakness_areas=weakness_areas,
            evolution_frequency=round(evolution_frequency, 3),
            last_activity=last_activity,
        )

    def _analyze_common_contexts(self, data_list: List[Dict]) -> List[str]:
        """ê³µí†µ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""

        contexts = []

        # ì •ì±… ë„ë©”ì¸
        policy_domains = [
            d.get("scenario_domain") for d in data_list if d.get("scenario_domain")
        ]
        if policy_domains:
            most_common_domain = Counter(policy_domains).most_common(1)[0][0]
            contexts.append(f"ì •ì±…ë„ë©”ì¸:{most_common_domain}")

        # ê°ì • íŒ¨í„´
        emotions = [d.get("emotion") for d in data_list if d.get("emotion")]
        if emotions:
            most_common_emotion = Counter(emotions).most_common(1)[0][0]
            contexts.append(f"ì£¼ê°ì •:{most_common_emotion}")

        # ì „ëµ íŒ¨í„´
        strategies = [d.get("strategy") for d in data_list if d.get("strategy")]
        if strategies:
            most_common_strategy = Counter(strategies).most_common(1)[0][0]
            contexts.append(f"ì£¼ì „ëµ:{most_common_strategy}")

        # ë£¨í”„ íŒ¨í„´
        loops = [d.get("selected_loop") for d in data_list if d.get("selected_loop")]
        if loops:
            most_common_loop = Counter(loops).most_common(1)[0][0]
            contexts.append(f"ì£¼ë£¨í”„:{most_common_loop}")

        return contexts[:5]  # ìµœëŒ€ 5ê°œê¹Œì§€

    def _analyze_strength_weakness(
        self, signature_id: str, data_list: List[Dict]
    ) -> Tuple[List[str], List[str]]:
        """ê°•ì /ì•½ì  ì˜ì—­ ë¶„ì„"""

        strengths = []
        weaknesses = []

        # ì‹ ë¢°ë„ ê¸°ë°˜ ë¶„ì„
        confidence_data = [
            d["confidence"] for d in data_list if d.get("confidence") is not None
        ]
        if confidence_data:
            avg_confidence = sum(confidence_data) / len(confidence_data)
            if avg_confidence > 0.8:
                strengths.append("ë†’ì€ ì‹ ë¢°ë„")
            elif avg_confidence < 0.5:
                weaknesses.append("ë‚®ì€ ì‹ ë¢°ë„")

        # ì„±ê³µë¥  ê¸°ë°˜ ë¶„ì„
        success_data = [d for d in data_list if "success" in d]
        if success_data:
            success_rate = sum(d["success"] for d in success_data) / len(success_data)
            if success_rate > 0.8:
                strengths.append("ë†’ì€ ì„±ê³µë¥ ")
            elif success_rate < 0.6:
                weaknesses.append("ë†’ì€ ì‹¤íŒ¨ìœ¨")

        # ì‹¤í–‰ ì‹œê°„ ë¶„ì„
        time_data = []
        for d in data_list:
            time_val = d.get("execution_time") or d.get("processing_time")
            if time_val is not None:
                time_data.append(float(time_val))

        if time_data:
            avg_time = sum(time_data) / len(time_data)
            if avg_time < 1.0:
                strengths.append("ë¹ ë¥¸ ì‹¤í–‰")
            elif avg_time > 5.0:
                weaknesses.append("ëŠë¦° ì‹¤í–‰")

        # ì»¨í…ìŠ¤íŠ¸ë³„ íŠ¹í™” ë¶„ì„
        policy_data = [d for d in data_list if d.get("type") == "policy"]
        if policy_data:
            ethical_scores = [d.get("ethical_impact", 0) for d in policy_data]
            if ethical_scores:
                avg_ethical = sum(ethical_scores) / len(ethical_scores)
                if avg_ethical > 0.8:
                    strengths.append("ë†’ì€ ìœ¤ë¦¬ì  íŒë‹¨")
                elif avg_ethical < 0.5:
                    weaknesses.append("ìœ¤ë¦¬ì  ê³ ë ¤ ë¶€ì¡±")

        # ì‹œê·¸ë‹ˆì²˜ ê³ ìœ  íŠ¹ì„± ë°˜ì˜
        if signature_id in self.signature_profiles:
            profile = self.signature_profiles[signature_id]
            primary_strategies = profile.get("primary_strategies", [])

            if "empathetic" in primary_strategies:
                # Aurora íŠ¹ì„± ê²€ì¦
                emotion_data = [d.get("emotion") for d in data_list if d.get("emotion")]
                if emotion_data and "joy" in emotion_data:
                    strengths.append("ê°ì •ì  ê³µê° ìš°ìˆ˜")

            elif "analytical" in primary_strategies:
                # Sage íŠ¹ì„± ê²€ì¦
                complexity_data = [
                    d.get("complexity", 0) for d in data_list if d.get("complexity")
                ]
                if complexity_data:
                    avg_complexity = sum(complexity_data) / len(complexity_data)
                    if avg_complexity > 0.7:
                        strengths.append("ë³µì¡í•œ ë¬¸ì œ ì²˜ë¦¬ ìš°ìˆ˜")

        return strengths[:5], weaknesses[:5]  # ê°ê° ìµœëŒ€ 5ê°œ

    def compare_signatures(
        self, signature_metrics: List[SignatureMetrics]
    ) -> List[SignatureComparison]:
        """ì‹œê·¸ë‹ˆì²˜ ê°„ ë¹„êµ ë¶„ì„"""

        print("ğŸ” ì‹œê·¸ë‹ˆì²˜ ë¹„êµ ë¶„ì„ ì¤‘...")

        comparisons = []

        # ëª¨ë“  ì‹œê·¸ë‹ˆì²˜ ìŒì— ëŒ€í•´ ë¹„êµ
        for i, sig_a in enumerate(signature_metrics):
            for sig_b in signature_metrics[i + 1 :]:
                comparison = self._compare_signature_pair(sig_a, sig_b)
                comparisons.append(comparison)

        return comparisons

    def _compare_signature_pair(
        self, sig_a: SignatureMetrics, sig_b: SignatureMetrics
    ) -> SignatureComparison:
        """ë‘ ì‹œê·¸ë‹ˆì²˜ ê°„ ë¹„êµ"""

        # ì„±ëŠ¥ ì°¨ì´ ê³„ì‚° (ì¢…í•© ì ìˆ˜)
        score_a = (
            sig_a.success_rate * 0.4
            + sig_a.avg_confidence * 0.3
            + (1.0 - min(sig_a.avg_execution_time / 10.0, 1.0)) * 0.3
        )
        score_b = (
            sig_b.success_rate * 0.4
            + sig_b.avg_confidence * 0.3
            + (1.0 - min(sig_b.avg_execution_time / 10.0, 1.0)) * 0.3
        )

        performance_difference = score_a - score_b

        # ìƒí˜¸ë³´ì™„ì„± ì ìˆ˜
        complementary_score = self._calculate_complementary_score(sig_a, sig_b)

        # ì»¨í…ìŠ¤íŠ¸ ì¤‘ë³µë„
        context_overlap = self._calculate_context_overlap(sig_a, sig_b)

        # ì¡°í•© ì¶”ì²œ ì—¬ë¶€
        recommended_combination = (
            abs(performance_difference) < 0.3  # ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ê³ 
            and complementary_score > 0.6  # ìƒí˜¸ë³´ì™„ì ì´ë©°
            and context_overlap < 0.8  # ì»¨í…ìŠ¤íŠ¸ ì¤‘ë³µì´ ì ìŒ
        )

        # ì¶”ì²œ ê·¼ê±°
        reasoning = self._generate_comparison_reasoning(
            sig_a, sig_b, performance_difference, complementary_score, context_overlap
        )

        return SignatureComparison(
            signature_a=sig_a.signature_id,
            signature_b=sig_b.signature_id,
            performance_difference=round(performance_difference, 3),
            complementary_score=round(complementary_score, 3),
            context_overlap=round(context_overlap, 3),
            recommended_combination=recommended_combination,
            reasoning=reasoning,
        )

    def _calculate_complementary_score(
        self, sig_a: SignatureMetrics, sig_b: SignatureMetrics
    ) -> float:
        """ìƒí˜¸ë³´ì™„ì„± ì ìˆ˜ ê³„ì‚°"""

        complementary_factors = []

        # ê°•ì /ì•½ì  ìƒí˜¸ë³´ì™„
        a_strengths = set(sig_a.strength_areas)
        a_weaknesses = set(sig_a.weakness_areas)
        b_strengths = set(sig_b.strength_areas)
        b_weaknesses = set(sig_b.weakness_areas)

        # Aì˜ ì•½ì ì„ Bê°€ ë³´ì™„
        a_weakness_covered = len(a_weaknesses & b_strengths) / max(len(a_weaknesses), 1)
        # Bì˜ ì•½ì ì„ Aê°€ ë³´ì™„
        b_weakness_covered = len(b_weaknesses & a_strengths) / max(len(b_weaknesses), 1)

        complementary_factors.append((a_weakness_covered + b_weakness_covered) / 2)

        # ì„±ëŠ¥ íŠ¹ì„± ë³´ì™„
        if sig_a.avg_execution_time > 3.0 and sig_b.avg_execution_time < 2.0:
            complementary_factors.append(0.8)  # ì†ë„ ë³´ì™„
        elif sig_b.avg_execution_time > 3.0 and sig_a.avg_execution_time < 2.0:
            complementary_factors.append(0.8)

        if abs(sig_a.avg_confidence - sig_b.avg_confidence) > 0.3:
            complementary_factors.append(0.6)  # ì‹ ë¢°ë„ ë‹¤ì–‘ì„±

        # ì§„í™” íŒ¨í„´ ë³´ì™„
        if abs(sig_a.evolution_frequency - sig_b.evolution_frequency) > 0.3:
            complementary_factors.append(0.5)  # ì§„í™” íŠ¹ì„± ë‹¤ì–‘ì„±

        return (
            sum(complementary_factors) / len(complementary_factors)
            if complementary_factors
            else 0.0
        )

    def _calculate_context_overlap(
        self, sig_a: SignatureMetrics, sig_b: SignatureMetrics
    ) -> float:
        """ì»¨í…ìŠ¤íŠ¸ ì¤‘ë³µë„ ê³„ì‚°"""

        contexts_a = set(sig_a.common_contexts)
        contexts_b = set(sig_b.common_contexts)

        if not contexts_a and not contexts_b:
            return 0.0

        overlap = len(contexts_a & contexts_b)
        total_unique = len(contexts_a | contexts_b)

        return overlap / total_unique if total_unique > 0 else 0.0

    def _generate_comparison_reasoning(
        self,
        sig_a: SignatureMetrics,
        sig_b: SignatureMetrics,
        perf_diff: float,
        comp_score: float,
        overlap: float,
    ) -> str:
        """ë¹„êµ ë¶„ì„ ê·¼ê±° ìƒì„±"""

        reasoning_parts = []

        # ì„±ëŠ¥ ì°¨ì´ ë¶„ì„
        if abs(perf_diff) < 0.1:
            reasoning_parts.append("ì„±ëŠ¥ì´ ê±°ì˜ ë™ë“±í•¨")
        elif perf_diff > 0.3:
            reasoning_parts.append(f"{sig_a.signature_id}ê°€ ì¢…í•© ì„±ëŠ¥ ìš°ìˆ˜")
        elif perf_diff < -0.3:
            reasoning_parts.append(f"{sig_b.signature_id}ê°€ ì¢…í•© ì„±ëŠ¥ ìš°ìˆ˜")

        # ìƒí˜¸ë³´ì™„ì„± ë¶„ì„
        if comp_score > 0.7:
            reasoning_parts.append("ë†’ì€ ìƒí˜¸ë³´ì™„ì„±")
        elif comp_score < 0.3:
            reasoning_parts.append("ìƒí˜¸ë³´ì™„ì„± ë‚®ìŒ")

        # ì»¨í…ìŠ¤íŠ¸ ì¤‘ë³µ ë¶„ì„
        if overlap > 0.8:
            reasoning_parts.append("ì»¨í…ìŠ¤íŠ¸ ì¤‘ë³µ ë†’ìŒ")
        elif overlap < 0.3:
            reasoning_parts.append("ì„œë¡œ ë‹¤ë¥¸ ì˜ì—­ íŠ¹í™”")

        # íŠ¹ë³„í•œ íŒ¨í„´ ê°ì§€
        if sig_a.avg_execution_time < 1.0 and sig_b.avg_execution_time > 3.0:
            reasoning_parts.append(
                f"{sig_a.signature_id}ëŠ” ì†ë„, {sig_b.signature_id}ëŠ” ì‹ ì¤‘í•¨"
            )

        return " | ".join(reasoning_parts) if reasoning_parts else "ì¶”ê°€ ë¶„ì„ í•„ìš”"

    def generate_recommendations(
        self,
        signature_metrics: List[SignatureMetrics],
        comparisons: List[SignatureComparison],
    ) -> Dict[str, Any]:
        """ì¶”ì²œ ì‚¬í•­ ìƒì„±"""

        print("ğŸ’¡ ì¶”ì²œ ì‚¬í•­ ìƒì„± ì¤‘...")

        recommendations = {}

        # 1. ìµœê³  ì„±ëŠ¥ ì‹œê·¸ë‹ˆì²˜
        best_overall = max(
            signature_metrics,
            key=lambda x: x.success_rate * 0.5 + x.avg_confidence * 0.5,
        )
        recommendations["best_overall_signature"] = {
            "signature_id": best_overall.signature_id,
            "success_rate": best_overall.success_rate,
            "avg_confidence": best_overall.avg_confidence,
            "reasoning": f"ì„±ê³µë¥  {best_overall.success_rate:.1%}, ì‹ ë¢°ë„ {best_overall.avg_confidence:.2f}",
        }

        # 2. íŠ¹í™” ì˜ì—­ë³„ ìµœì  ì‹œê·¸ë‹ˆì²˜
        recommendations["specialized_recommendations"] = (
            self._generate_specialized_recommendations(signature_metrics)
        )

        # 3. ì¶”ì²œ ì‹œê·¸ë‹ˆì²˜ ì¡°í•©
        recommended_combinations = [c for c in comparisons if c.recommended_combination]
        recommendations["recommended_combinations"] = [
            {
                "signatures": [c.signature_a, c.signature_b],
                "complementary_score": c.complementary_score,
                "reasoning": c.reasoning,
            }
            for c in recommended_combinations[:3]  # ìƒìœ„ 3ê°œ
        ]

        # 4. ê°œì„ ì´ í•„ìš”í•œ ì‹œê·¸ë‹ˆì²˜
        improvement_needed = [
            s
            for s in signature_metrics
            if s.success_rate < 0.6 or s.avg_confidence < 0.5
        ]
        recommendations["improvement_needed"] = [
            {
                "signature_id": s.signature_id,
                "issues": s.weakness_areas,
                "suggested_actions": self._suggest_improvement_actions(s),
            }
            for s in improvement_needed
        ]

        # 5. ì‹œë‚˜ë¦¬ì˜¤ë³„ ì¶”ì²œ
        recommendations["scenario_recommendations"] = (
            self._generate_scenario_recommendations(signature_metrics)
        )

        return recommendations

    def _generate_specialized_recommendations(
        self, signature_metrics: List[SignatureMetrics]
    ) -> Dict[str, str]:
        """íŠ¹í™” ì˜ì—­ë³„ ì¶”ì²œ"""

        specialized = {}

        # ì†ë„ ìµœì í™”
        fastest = min(signature_metrics, key=lambda x: x.avg_execution_time)
        specialized["fastest_execution"] = fastest.signature_id

        # ë†’ì€ ì‹ ë¢°ë„
        most_confident = max(signature_metrics, key=lambda x: x.avg_confidence)
        specialized["highest_confidence"] = most_confident.signature_id

        # ì§„í™” í™œë°œ
        most_evolving = max(signature_metrics, key=lambda x: x.evolution_frequency)
        specialized["most_adaptive"] = most_evolving.signature_id

        # ë³µì¡í•œ ë¬¸ì œ ì²˜ë¦¬ (ì •ì±… ë°ì´í„° ê¸°ë°˜)
        policy_specialists = [
            s for s in signature_metrics if "ì •ì±…ë„ë©”ì¸" in " ".join(s.common_contexts)
        ]
        if policy_specialists:
            best_policy = max(policy_specialists, key=lambda x: x.avg_confidence)
            specialized["policy_specialist"] = best_policy.signature_id

        return specialized

    def _suggest_improvement_actions(
        self, signature_metrics: SignatureMetrics
    ) -> List[str]:
        """ê°œì„  ì•¡ì…˜ ì œì•ˆ"""

        actions = []

        if signature_metrics.success_rate < 0.6:
            actions.append("íŒë‹¨ ì•Œê³ ë¦¬ì¦˜ ì¬ì¡°ì • í•„ìš”")

        if signature_metrics.avg_confidence < 0.5:
            actions.append("ë©”íƒ€ ë¯¼ê°ë„ ì¦ê°€ ê³ ë ¤")

        if signature_metrics.avg_execution_time > 5.0:
            actions.append("ì‹¤í–‰ íš¨ìœ¨ì„± ìµœì í™” í•„ìš”")

        if signature_metrics.evolution_frequency > 0.8:
            actions.append("ê³¼ë„í•œ ì§„í™” íŒ¨í„´ ì¡°ì • í•„ìš”")

        if "ë‚®ì€ ì‹ ë¢°ë„" in signature_metrics.weakness_areas:
            actions.append("ì‹ ë¢°ë„ í–¥ìƒì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ íŠ¹í™”")

        return actions[:3]  # ìµœëŒ€ 3ê°œ

    def _generate_scenario_recommendations(
        self, signature_metrics: List[SignatureMetrics]
    ) -> Dict[str, str]:
        """ì‹œë‚˜ë¦¬ì˜¤ë³„ ì¶”ì²œ"""

        scenario_recs = {}

        # ì •ì±… ì‹œë‚˜ë¦¬ì˜¤
        policy_candidates = [
            s
            for s in signature_metrics
            if any("ì •ì±…" in ctx for ctx in s.common_contexts)
        ]
        if policy_candidates:
            best_policy = max(policy_candidates, key=lambda x: x.avg_confidence)
            scenario_recs["policy_scenarios"] = best_policy.signature_id

        # ê°ì •ì  ì‹œë‚˜ë¦¬ì˜¤
        emotional_candidates = [
            s
            for s in signature_metrics
            if any(
                "joy" in ctx or "empathetic" in " ".join(s.strength_areas)
                for ctx in s.common_contexts
            )
        ]
        if emotional_candidates:
            best_emotional = max(emotional_candidates, key=lambda x: x.success_rate)
            scenario_recs["emotional_scenarios"] = best_emotional.signature_id

        # ë¶„ì„ì  ì‹œë‚˜ë¦¬ì˜¤
        analytical_candidates = [
            s
            for s in signature_metrics
            if any(
                "analytical" in " ".join(s.strength_areas) for ctx in s.common_contexts
            )
        ]
        if analytical_candidates:
            best_analytical = max(analytical_candidates, key=lambda x: x.avg_confidence)
            scenario_recs["analytical_scenarios"] = best_analytical.signature_id

        return scenario_recs

    def generate_insights(
        self,
        signature_metrics: List[SignatureMetrics],
        comparisons: List[SignatureComparison],
    ) -> List[str]:
        """í†µì°° ìƒì„±"""

        insights = []

        # ì „ì²´ ì„±ëŠ¥ íŠ¸ë Œë“œ
        avg_success_rate = sum(s.success_rate for s in signature_metrics) / len(
            signature_metrics
        )
        if avg_success_rate > 0.8:
            insights.append(
                f"ì „ì²´ ì‹œê·¸ë‹ˆì²˜ ì„±ëŠ¥ì´ ìš°ìˆ˜í•¨ (í‰ê·  ì„±ê³µë¥  {avg_success_rate:.1%})"
            )
        elif avg_success_rate < 0.6:
            insights.append(
                f"ì‹œê·¸ë‹ˆì²˜ ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•¨ (í‰ê·  ì„±ê³µë¥  {avg_success_rate:.1%})"
            )

        # ì„±ëŠ¥ ë¶„ì‚° ë¶„ì„
        success_rates = [s.success_rate for s in signature_metrics]
        performance_variance = np.var(success_rates)
        if performance_variance > 0.1:
            insights.append("ì‹œê·¸ë‹ˆì²˜ ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ í¼ - íŠ¹í™” ì˜ì—­ í™œìš© ê³ ë ¤")
        else:
            insights.append("ì‹œê·¸ë‹ˆì²˜ ê°„ ì„±ëŠ¥ì´ ê· ë“±í•¨ - ì•ˆì •ì ì¸ ì‹œìŠ¤í…œ")

        # ì‹¤í–‰ ì‹œê°„ ë¶„ì„
        execution_times = [s.avg_execution_time for s in signature_metrics]
        if max(execution_times) > 5.0:
            slowest = max(signature_metrics, key=lambda x: x.avg_execution_time)
            insights.append(f"{slowest.signature_id}ì˜ ì‹¤í–‰ ì‹œê°„ ìµœì í™” í•„ìš”")

        # ìƒí˜¸ë³´ì™„ì„± ë¶„ì„
        high_complementary = [c for c in comparisons if c.complementary_score > 0.7]
        if high_complementary:
            insights.append(
                f"ì‹œê·¸ë‹ˆì²˜ ì¡°í•© í™œìš© ê°€ëŠ¥: {len(high_complementary)}ê°œ ì¡°í•© ë°œê²¬"
            )

        # íŠ¹í™” íŒ¨í„´
        specialized_contexts = set()
        for s in signature_metrics:
            specialized_contexts.update(s.common_contexts)

        if len(specialized_contexts) > 10:
            insights.append("ë‹¤ì–‘í•œ ì»¨í…ìŠ¤íŠ¸ì—ì„œ í™œìš©ë¨ - ë²”ìš©ì„± ë†’ìŒ")

        # ì§„í™” íŒ¨í„´
        high_evolution = [s for s in signature_metrics if s.evolution_frequency > 0.7]
        if high_evolution:
            insights.append(f"{len(high_evolution)}ê°œ ì‹œê·¸ë‹ˆì²˜ê°€ í™œë°œí•œ ì§„í™” íŒ¨í„´ ë³´ì„")

        return insights[:5]  # ìµœëŒ€ 5ê°œ

    def generate_performance_report(self) -> PerformanceReport:
        """ì¢…í•© ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""

        print("ğŸ“‹ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")

        # ë°ì´í„° ìˆ˜ì§‘
        data_sources = self.collect_performance_data()

        # ì‹œê·¸ë‹ˆì²˜ ì„±ëŠ¥ ë¶„ì„
        signature_metrics = self.analyze_signature_performance()

        # ì‹œê·¸ë‹ˆì²˜ ë¹„êµ
        comparisons = self.compare_signatures(signature_metrics)

        # ì¶”ì²œ ì‚¬í•­ ìƒì„±
        recommendations = self.generate_recommendations(signature_metrics, comparisons)

        # í†µì°° ìƒì„±
        insights = self.generate_insights(signature_metrics, comparisons)

        # ë¶„ì„ ê¸°ê°„
        cutoff_date = datetime.now() - timedelta(days=self.analysis_window_days)
        analysis_period = f"{cutoff_date.strftime('%Y-%m-%d')} ~ {datetime.now().strftime('%Y-%m-%d')}"

        report = PerformanceReport(
            report_id=f"perf_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            generation_timestamp=datetime.now().isoformat(),
            analysis_period=analysis_period,
            signature_metrics=signature_metrics,
            signature_comparisons=comparisons,
            recommendations=recommendations,
            insights=insights,
            data_sources=data_sources,
        )

        return report

    def save_report(
        self, report: PerformanceReport, output_path: str = "reports/"
    ) -> str:
        """ë¦¬í¬íŠ¸ ì €ì¥"""

        os.makedirs(output_path, exist_ok=True)

        # JSON í˜•íƒœë¡œ ì €ì¥
        report_data = asdict(report)

        filename = f"{report.report_id}.json"
        filepath = os.path.join(output_path, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“Š ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì €ì¥: {filepath}")

        # ìš”ì•½ í…ìŠ¤íŠ¸ íŒŒì¼ë„ ìƒì„±
        self._save_report_summary(report, output_path)

        return filepath

    def _save_report_summary(self, report: PerformanceReport, output_path: str):
        """ë¦¬í¬íŠ¸ ìš”ì•½ í…ìŠ¤íŠ¸ ì €ì¥"""

        summary_filename = f"{report.report_id}_summary.txt"
        summary_filepath = os.path.join(output_path, summary_filename)

        with open(summary_filepath, "w", encoding="utf-8") as f:
            f.write(f"ğŸ“Š EchoJudgment ì‹œê·¸ë‹ˆì²˜ ì„±ëŠ¥ ë¦¬í¬íŠ¸\n")
            f.write(f"=" * 50 + "\n\n")

            f.write(f"ğŸ—“ï¸ ìƒì„±ì¼ì‹œ: {report.generation_timestamp}\n")
            f.write(f"ğŸ“… ë¶„ì„ê¸°ê°„: {report.analysis_period}\n")
            f.write(f"ğŸ“ ë°ì´í„°ì†ŒìŠ¤: {sum(report.data_sources.values())}ê°œ íŒŒì¼\n\n")

            f.write(f"ğŸ“ˆ ì‹œê·¸ë‹ˆì²˜ ì„±ëŠ¥ ìš”ì•½\n")
            f.write(f"-" * 30 + "\n")

            for metrics in report.signature_metrics:
                f.write(f"ğŸ”¸ {metrics.signature_id}\n")
                f.write(f"   ì„±ê³µë¥ : {metrics.success_rate:.1%}\n")
                f.write(f"   ì‹ ë¢°ë„: {metrics.avg_confidence:.2f}\n")
                f.write(f"   ì‹¤í–‰íšŸìˆ˜: {metrics.total_executions}\n")
                f.write(f"   ê°•ì : {', '.join(metrics.strength_areas[:3])}\n")
                f.write(f"   ì•½ì : {', '.join(metrics.weakness_areas[:3])}\n\n")

            f.write(f"ğŸ’¡ ì£¼ìš” ì¶”ì²œì‚¬í•­\n")
            f.write(f"-" * 30 + "\n")

            best = report.recommendations.get("best_overall_signature", {})
            f.write(f"ğŸ† ìµœê³  ì„±ëŠ¥: {best.get('signature_id', 'N/A')}\n")
            f.write(f"   {best.get('reasoning', '')}\n\n")

            f.write(f"ğŸ” ì£¼ìš” í†µì°°\n")
            f.write(f"-" * 30 + "\n")
            for i, insight in enumerate(report.insights, 1):
                f.write(f"{i}. {insight}\n")

        print(f"ğŸ“„ ë¦¬í¬íŠ¸ ìš”ì•½ ì €ì¥: {summary_filepath}")


# Convenience functions
def generate_signature_report(flow_data_path: str = "flows/") -> PerformanceReport:
    """ì‹œê·¸ë‹ˆì²˜ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    reporter = SignaturePerformanceReporter(flow_data_path)
    return reporter.generate_performance_report()


def analyze_signature_performance_quick(signature_id: str) -> Dict[str, Any]:
    """íŠ¹ì • ì‹œê·¸ë‹ˆì²˜ ë¹ ë¥¸ ì„±ëŠ¥ ë¶„ì„"""
    reporter = SignaturePerformanceReporter()
    data_sources = reporter.collect_performance_data()

    all_data = reporter.flow_data + reporter.policy_data + reporter.loop_execution_data
    signature_data = [d for d in all_data if d.get("signature_id") == signature_id]

    if signature_data:
        metrics = reporter._calculate_signature_metrics(signature_id, signature_data)
        return asdict(metrics)
    else:
        return {"error": f"No data found for signature {signature_id}"}


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ“Š Signature Performance Reporter í…ŒìŠ¤íŠ¸")

    reporter = SignaturePerformanceReporter()

    print("\nğŸ“ ë°ì´í„° ìˆ˜ì§‘:")
    data_sources = reporter.collect_performance_data()
    print(f"ìˆ˜ì§‘ëœ íŒŒì¼: {data_sources}")

    if sum(data_sources.values()) > 0:
        print("\nğŸ“ˆ ì„±ëŠ¥ ë¶„ì„:")
        signature_metrics = reporter.analyze_signature_performance()
        print(f"ë¶„ì„ëœ ì‹œê·¸ë‹ˆì²˜: {len(signature_metrics)}ê°œ")

        for metrics in signature_metrics:
            print(
                f"- {metrics.signature_id}: ì„±ê³µë¥  {metrics.success_rate:.1%}, ì‹ ë¢°ë„ {metrics.avg_confidence:.2f}"
            )

        if len(signature_metrics) > 1:
            print("\nğŸ” ì‹œê·¸ë‹ˆì²˜ ë¹„êµ:")
            comparisons = reporter.compare_signatures(signature_metrics)
            print(f"ë¹„êµ ë¶„ì„: {len(comparisons)}ê°œ ì¡°í•©")

            for comp in comparisons[:3]:  # ì²« 3ê°œë§Œ ì¶œë ¥
                print(
                    f"- {comp.signature_a} vs {comp.signature_b}: ìƒí˜¸ë³´ì™„ì„± {comp.complementary_score:.2f}"
                )

        print("\nğŸ“‹ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±:")
        report = reporter.generate_performance_report()
        print(f"ë¦¬í¬íŠ¸ ID: {report.report_id}")
        print(f"í†µì°°: {len(report.insights)}ê°œ")

        # ë¦¬í¬íŠ¸ ì €ì¥
        report_path = reporter.save_report(report)
        print(f"ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")

    else:
        print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì‹œìŠ¤í…œì„ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")

    print("âœ… Signature Performance Reporter í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

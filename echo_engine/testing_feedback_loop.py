#!/usr/bin/env python3
"""
ğŸ”„ Echo Testing Feedback Loop System
íšŒê·€ í…ŒìŠ¤íŠ¸ + ì§€ì† ëª¨ë‹ˆí„°ë§ + ìë™ í’ˆì§ˆ ë³´ì¦ ì‹œìŠ¤í…œ

í•µì‹¬ ê¸°ëŠ¥:
1. ìë™ íšŒê·€ í…ŒìŠ¤íŠ¸ (Regression Testing)
2. ì§€ì†ì  ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (Continuous Performance Monitoring)
3. í’ˆì§ˆ ë©”íŠ¸ë¦­ ì¶”ì  (Quality Metrics Tracking)
4. ìë™ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„± (Auto Test Case Generation)
5. í”¼ë“œë°± ë£¨í”„ ìµœì í™” (Feedback Loop Optimization)
"""

import asyncio
import json
import time
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
from collections import defaultdict, deque
import logging
import hashlib
import statistics


class TestType(Enum):
    """í…ŒìŠ¤íŠ¸ íƒ€ì…"""

    REGRESSION = "regression"  # íšŒê·€ í…ŒìŠ¤íŠ¸
    PERFORMANCE = "performance"  # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    INTEGRATION = "integration"  # í†µí•© í…ŒìŠ¤íŠ¸
    STRESS = "stress"  # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
    QUALITY = "quality"  # í’ˆì§ˆ í…ŒìŠ¤íŠ¸


class TestStatus(Enum):
    """í…ŒìŠ¤íŠ¸ ìƒíƒœ"""

    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    RUNNING = "running"
    PENDING = "pending"


class MetricType(Enum):
    """ë©”íŠ¸ë¦­ íƒ€ì…"""

    RESPONSE_TIME = "response_time"
    CONFIDENCE_SCORE = "confidence_score"
    ACCURACY = "accuracy"
    COVERAGE = "coverage"
    ERROR_RATE = "error_rate"
    THROUGHPUT = "throughput"


@dataclass
class TestCase:
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""

    id: str
    name: str
    test_type: TestType
    input_data: Dict[str, Any]
    expected_output: Optional[Dict[str, Any]]
    test_function: str
    priority: int = 1
    timeout: float = 30.0
    enabled: bool = True


@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼"""

    test_id: str
    timestamp: datetime
    status: TestStatus
    execution_time: float
    actual_output: Optional[Dict[str, Any]]
    error_message: Optional[str]
    metrics: Dict[str, float]
    environment_info: Dict[str, Any]


@dataclass
class QualityMetric:
    """í’ˆì§ˆ ë©”íŠ¸ë¦­"""

    metric_type: MetricType
    timestamp: datetime
    value: float
    component: str
    metadata: Dict[str, Any]


@dataclass
class FeedbackReport:
    """í”¼ë“œë°± ë¦¬í¬íŠ¸"""

    timestamp: datetime
    test_summary: Dict[str, Any]
    quality_trends: Dict[str, List[float]]
    performance_analysis: Dict[str, Any]
    recommendations: List[Dict[str, Any]]
    regression_alerts: List[Dict[str, Any]]


class EchoTestingFeedbackLoop:
    """
    ğŸ”„ Echo Testing Feedback Loop System
    """

    def __init__(self):
        self.test_cases = {}  # test_id -> TestCase
        self.test_results = deque(maxlen=1000)  # ìµœê·¼ 1000ê°œ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        self.quality_metrics = deque(maxlen=5000)  # ìµœê·¼ 5000ê°œ ë©”íŠ¸ë¦­
        self.baseline_metrics = {}  # ê¸°ì¤€ ë©”íŠ¸ë¦­

        # ëª¨ë‹ˆí„°ë§ ì„¤ì •
        self.monitoring_active = True
        self.monitoring_interval = 300  # 5ë¶„ë§ˆë‹¤ ëª¨ë‹ˆí„°ë§
        self.monitoring_thread = None

        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°
        self.test_executors = {
            TestType.REGRESSION: self._execute_regression_test,
            TestType.PERFORMANCE: self._execute_performance_test,
            TestType.INTEGRATION: self._execute_integration_test,
            TestType.STRESS: self._execute_stress_test,
            TestType.QUALITY: self._execute_quality_test,
        }

        # ì„ê³„ê°’ ì„¤ì •
        self.thresholds = {
            MetricType.RESPONSE_TIME: 5.0,  # 5ì´ˆ
            MetricType.CONFIDENCE_SCORE: 0.7,  # 70% ì‹ ë¢°ë„
            MetricType.ACCURACY: 0.8,  # 80% ì •í™•ë„
            MetricType.COVERAGE: 0.9,  # 90% ì»¤ë²„ë¦¬ì§€
            MetricType.ERROR_RATE: 0.1,  # 10% ì˜¤ë¥˜ìœ¨
            MetricType.THROUGHPUT: 10.0,  # ì´ˆë‹¹ 10ê°œ ì²˜ë¦¬
        }

        # í”¼ë“œë°± í†µê³„
        self.feedback_stats = {
            "total_tests_run": 0,
            "total_passed": 0,
            "total_failed": 0,
            "regression_detected": 0,
            "performance_improvements": 0,
            "quality_score": 0.0,
        }

        print("ğŸ”„ Echo Testing Feedback Loop ì´ˆê¸°í™” ì™„ë£Œ")
        print("   ğŸ“Š íšŒê·€ í…ŒìŠ¤íŠ¸ ì—”ì§„ í™œì„±í™”")
        print("   ğŸ“ˆ ì§€ì† ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        print("   ğŸ¯ í’ˆì§ˆ ë©”íŠ¸ë¦­ ì¶”ì  ì‹œì‘")

        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë“±ë¡
        self._register_default_test_cases()

        # ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.start_monitoring()

    def _register_default_test_cases(self):
        """ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë“±ë¡"""

        # íšŒê·€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
        regression_cases = [
            TestCase(
                id="regression_basic_judgment",
                name="ê¸°ë³¸ íŒë‹¨ íšŒê·€ í…ŒìŠ¤íŠ¸",
                test_type=TestType.REGRESSION,
                input_data={"user_input": "ì•ˆë…•í•˜ì„¸ìš”!", "signature": "Aurora"},
                expected_output={"confidence_min": 0.8, "response_contains": "ì•ˆë…•"},
                test_function="test_basic_judgment",
                priority=1,
            ),
            TestCase(
                id="regression_signature_consistency",
                name="ì‹œê·¸ë‹ˆì²˜ ì¼ê´€ì„± íšŒê·€ í…ŒìŠ¤íŠ¸",
                test_type=TestType.REGRESSION,
                input_data={"signatures": ["Aurora", "Phoenix", "Sage", "Companion"]},
                expected_output={"all_signatures_working": True},
                test_function="test_signature_consistency",
                priority=1,
            ),
            TestCase(
                id="regression_enhanced_judge",
                name="Enhanced Judge íšŒê·€ í…ŒìŠ¤íŠ¸",
                test_type=TestType.REGRESSION,
                input_data={"complex_input": "ë³µì¡í•œ ì² í•™ì  ì§ˆë¬¸ì— ëŒ€í•´ ë¶„ì„í•´ì£¼ì„¸ìš”"},
                expected_output={"enhanced_judge_used": True, "quality_min": 0.6},
                test_function="test_enhanced_judge",
                priority=1,
            ),
        ]

        # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
        performance_cases = [
            TestCase(
                id="performance_response_time",
                name="ì‘ë‹µ ì‹œê°„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸",
                test_type=TestType.PERFORMANCE,
                input_data={"iterations": 10},
                expected_output={"avg_response_time_max": 2.0},
                test_function="test_response_time",
                priority=2,
            ),
            TestCase(
                id="performance_throughput",
                name="ì²˜ë¦¬ëŸ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸",
                test_type=TestType.PERFORMANCE,
                input_data={"concurrent_requests": 5, "duration": 30},
                expected_output={"throughput_min": 5.0},
                test_function="test_throughput",
                priority=2,
            ),
        ]

        # í†µí•© í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
        integration_cases = [
            TestCase(
                id="integration_end_to_end",
                name="ì¢…ë‹¨ê°„ í†µí•© í…ŒìŠ¤íŠ¸",
                test_type=TestType.INTEGRATION,
                input_data={"full_workflow": True},
                expected_output={"all_components_working": True},
                test_function="test_end_to_end",
                priority=2,
            )
        ]

        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë“±ë¡
        all_cases = regression_cases + performance_cases + integration_cases

        for test_case in all_cases:
            self.test_cases[test_case.id] = test_case

        print(f"ğŸ“‹ {len(all_cases)}ê°œ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë“±ë¡ ì™„ë£Œ")

    def start_monitoring(self):
        """ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""

        def monitor_loop():
            while self.monitoring_active:
                try:
                    # íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                    asyncio.run(self._run_regression_tests())

                    # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
                    asyncio.run(self._monitor_performance())

                    # í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                    self._collect_quality_metrics()

                    time.sleep(self.monitoring_interval)

                except Exception as e:
                    print(f"âš ï¸ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                    time.sleep(60)  # ì˜¤ë¥˜ ì‹œ 1ë¶„ ëŒ€ê¸°

        self.monitoring_thread = threading.Thread(target=monitor_loop, daemon=True)
        self.monitoring_thread.start()

    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)

    async def run_tests(
        self, test_type: Optional[TestType] = None, test_ids: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""

        print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘: {test_type.value if test_type else 'ALL'}")

        # ì‹¤í–‰í•  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì„ íƒ
        if test_ids:
            test_cases = [
                self.test_cases[tid] for tid in test_ids if tid in self.test_cases
            ]
        elif test_type:
            test_cases = [
                tc for tc in self.test_cases.values() if tc.test_type == test_type
            ]
        else:
            test_cases = list(self.test_cases.values())

        # í™œì„±í™”ëœ í…ŒìŠ¤íŠ¸ë§Œ ì„ íƒ
        test_cases = [tc for tc in test_cases if tc.enabled]

        # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
        test_cases.sort(key=lambda x: x.priority)

        results = []
        passed = 0
        failed = 0

        for test_case in test_cases:
            print(f"   ğŸ” ì‹¤í–‰ ì¤‘: {test_case.name}")

            try:
                result = await self._execute_test(test_case)
                results.append(result)

                if result.status == TestStatus.PASSED:
                    passed += 1
                    print(f"     âœ… PASSED ({result.execution_time:.2f}s)")
                else:
                    failed += 1
                    print(f"     âŒ FAILED: {result.error_message}")

            except Exception as e:
                failed += 1
                error_result = TestResult(
                    test_id=test_case.id,
                    timestamp=datetime.now(),
                    status=TestStatus.FAILED,
                    execution_time=0.0,
                    actual_output=None,
                    error_message=str(e),
                    metrics={},
                    environment_info={},
                )
                results.append(error_result)
                print(f"     ğŸ’¥ ERROR: {e}")

        # ê²°ê³¼ ì €ì¥
        self.test_results.extend(results)

        # í†µê³„ ì—…ë°ì´íŠ¸
        self.feedback_stats["total_tests_run"] += len(results)
        self.feedback_stats["total_passed"] += passed
        self.feedback_stats["total_failed"] += failed

        test_summary = {
            "timestamp": datetime.now().isoformat(),
            "test_type": test_type.value if test_type else "mixed",
            "total_tests": len(results),
            "passed": passed,
            "failed": failed,
            "success_rate": passed / len(results) * 100 if results else 0,
            "results": [asdict(r) for r in results],
        }

        print(
            f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {passed}/{len(results)} ì„±ê³µ ({passed/len(results)*100:.1f}%)"
        )

        return test_summary

    async def _execute_test(self, test_case: TestCase) -> TestResult:
        """ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""

        start_time = time.time()

        try:
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸° ì„ íƒ
            executor = self.test_executors.get(
                test_case.test_type, self._execute_generic_test
            )

            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            actual_output, metrics = await executor(test_case)

            # ê²°ê³¼ ê²€ì¦
            status = self._validate_test_result(test_case, actual_output)

            execution_time = time.time() - start_time

            return TestResult(
                test_id=test_case.id,
                timestamp=datetime.now(),
                status=status,
                execution_time=execution_time,
                actual_output=actual_output,
                error_message=None,
                metrics=metrics,
                environment_info={"test_type": test_case.test_type.value},
            )

        except Exception as e:
            execution_time = time.time() - start_time

            return TestResult(
                test_id=test_case.id,
                timestamp=datetime.now(),
                status=TestStatus.FAILED,
                execution_time=execution_time,
                actual_output=None,
                error_message=str(e),
                metrics={},
                environment_info={"test_type": test_case.test_type.value},
            )

    async def _execute_regression_test(
        self, test_case: TestCase
    ) -> Tuple[Dict[str, Any], Dict[str, float]]:
        """íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""

        if test_case.test_function == "test_basic_judgment":
            # ê¸°ë³¸ íŒë‹¨ í…ŒìŠ¤íŠ¸
            from echo_engine.echo_centered_judgment_hybrid import process_echo_judgment

            result = await process_echo_judgment(
                user_input=test_case.input_data["user_input"],
                signature=test_case.input_data["signature"],
            )

            return {
                "confidence": result.confidence_score,
                "response": result.final_judgment,
                "processing_time": result.processing_time,
                "source": result.judgment_source.value,
            }, {
                "confidence_score": result.confidence_score,
                "response_time": result.processing_time,
            }

        elif test_case.test_function == "test_signature_consistency":
            # ì‹œê·¸ë‹ˆì²˜ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
            from echo_engine.echo_centered_judgment_hybrid import process_echo_judgment

            results = {}
            metrics = {}

            for signature in test_case.input_data["signatures"]:
                result = await process_echo_judgment(
                    user_input="í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ì…ë‹ˆë‹¤", signature=signature
                )
                results[signature] = {
                    "confidence": result.confidence_score,
                    "response_length": len(result.final_judgment),
                }
                metrics[f"{signature}_confidence"] = result.confidence_score

            return {
                "signature_results": results,
                "all_signatures_working": all(
                    r["confidence"] > 0.5 for r in results.values()
                ),
            }, metrics

        elif test_case.test_function == "test_enhanced_judge":
            # Enhanced Judge í…ŒìŠ¤íŠ¸
            from echo_engine.enhanced_llm_free_judge import get_enhanced_llm_free_judge

            judge = get_enhanced_llm_free_judge()
            result = await judge.process_independent_judgment(
                user_input=test_case.input_data["complex_input"], signature="Sage"
            )

            return {
                "enhanced_judge_used": True,
                "confidence": result.confidence_score,
                "quality": result.fallback_quality,
                "complexity": result.complexity_level.value,
            }, {
                "confidence_score": result.confidence_score,
                "fallback_quality": result.fallback_quality,
                "processing_time": result.processing_time,
            }

        else:
            # ê¸°ë³¸ íšŒê·€ í…ŒìŠ¤íŠ¸
            return {"status": "completed"}, {"execution_time": 0.1}

    async def _execute_performance_test(
        self, test_case: TestCase
    ) -> Tuple[Dict[str, Any], Dict[str, float]]:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""

        if test_case.test_function == "test_response_time":
            # ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸
            from echo_engine.echo_centered_judgment_hybrid import process_echo_judgment

            times = []
            iterations = test_case.input_data.get("iterations", 5)

            for i in range(iterations):
                start_time = time.time()
                await process_echo_judgment(
                    user_input=f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ {i+1}", signature="Aurora"
                )
                response_time = time.time() - start_time
                times.append(response_time)

            avg_time = statistics.mean(times)
            max_time = max(times)
            min_time = min(times)

            return {
                "avg_response_time": avg_time,
                "max_response_time": max_time,
                "min_response_time": min_time,
                "iterations": iterations,
            }, {
                "avg_response_time": avg_time,
                "max_response_time": max_time,
                "throughput": iterations / sum(times),
            }

        elif test_case.test_function == "test_throughput":
            # ì²˜ë¦¬ëŸ‰ í…ŒìŠ¤íŠ¸
            from echo_engine.echo_centered_judgment_hybrid import process_echo_judgment

            concurrent_requests = test_case.input_data.get("concurrent_requests", 3)
            duration = test_case.input_data.get("duration", 10)

            start_time = time.time()
            completed_requests = 0

            async def worker():
                nonlocal completed_requests
                while time.time() - start_time < duration:
                    try:
                        await process_echo_judgment(
                            user_input="ì²˜ë¦¬ëŸ‰ í…ŒìŠ¤íŠ¸", signature="Phoenix"
                        )
                        completed_requests += 1
                    except:
                        pass

            # ë™ì‹œ ì‘ì—… ì‹¤í–‰
            tasks = [worker() for _ in range(concurrent_requests)]
            await asyncio.gather(*tasks, return_exceptions=True)

            actual_duration = time.time() - start_time
            throughput = completed_requests / actual_duration

            return {
                "completed_requests": completed_requests,
                "duration": actual_duration,
                "throughput": throughput,
            }, {
                "throughput": throughput,
                "completion_rate": completed_requests
                / (concurrent_requests * duration),
            }

        else:
            return {"status": "completed"}, {"execution_time": 0.1}

    async def _execute_integration_test(
        self, test_case: TestCase
    ) -> Tuple[Dict[str, Any], Dict[str, float]]:
        """í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""

        if test_case.test_function == "test_end_to_end":
            # ì¢…ë‹¨ê°„ í…ŒìŠ¤íŠ¸
            from echo_engine.echo_centered_judgment_hybrid import (
                get_echo_judgment_hybrid,
            )

            hybrid = get_echo_judgment_hybrid()

            # ì‹œìŠ¤í…œ ë¶„ì„
            analytics = hybrid.get_system_analytics()

            # ëª¨ë“ˆ ì—°ê²° ìƒíƒœ í™•ì¸
            modules = analytics.get("integrated_modules", {})
            connected_modules = sum(1 for connected in modules.values() if connected)
            total_modules = len(modules)

            # ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
            from echo_engine.echo_centered_judgment_hybrid import process_echo_judgment

            result = await process_echo_judgment(
                user_input="í†µí•© í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€", signature="Companion"
            )

            return {
                "all_components_working": connected_modules == total_modules,
                "connected_modules": connected_modules,
                "total_modules": total_modules,
                "judgment_working": result.confidence_score > 0.5,
                "integration_score": connected_modules / total_modules * 100,
            }, {
                "integration_score": connected_modules / total_modules,
                "judgment_confidence": result.confidence_score,
            }

        else:
            return {"status": "completed"}, {"execution_time": 0.1}

    async def _execute_stress_test(
        self, test_case: TestCase
    ) -> Tuple[Dict[str, Any], Dict[str, float]]:
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        return {"status": "completed"}, {"execution_time": 0.1}

    async def _execute_quality_test(
        self, test_case: TestCase
    ) -> Tuple[Dict[str, Any], Dict[str, float]]:
        """í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        return {"status": "completed"}, {"execution_time": 0.1}

    async def _execute_generic_test(
        self, test_case: TestCase
    ) -> Tuple[Dict[str, Any], Dict[str, float]]:
        """ì¼ë°˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        return {"status": "completed"}, {"execution_time": 0.1}

    def _validate_test_result(
        self, test_case: TestCase, actual_output: Dict[str, Any]
    ) -> TestStatus:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê²€ì¦"""

        if not test_case.expected_output:
            return TestStatus.PASSED

        try:
            expected = test_case.expected_output

            for key, expected_value in expected.items():
                if key.endswith("_min"):
                    actual_key = key[:-4]
                    if actual_key in actual_output:
                        if actual_output[actual_key] < expected_value:
                            return TestStatus.FAILED
                elif key.endswith("_max"):
                    actual_key = key[:-4]
                    if actual_key in actual_output:
                        if actual_output[actual_key] > expected_value:
                            return TestStatus.FAILED
                elif key.endswith("_contains"):
                    actual_key = key[:-9]
                    if actual_key in actual_output:
                        if expected_value not in str(actual_output[actual_key]):
                            return TestStatus.FAILED
                else:
                    if key in actual_output:
                        if actual_output[key] != expected_value:
                            return TestStatus.FAILED

            return TestStatus.PASSED

        except Exception:
            return TestStatus.FAILED

    async def _run_regression_tests(self):
        """íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        await self.run_tests(test_type=TestType.REGRESSION)

    async def _monitor_performance(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        await self.run_tests(test_type=TestType.PERFORMANCE)

    def _collect_quality_metrics(self):
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""

        # ìµœê·¼ í…ŒìŠ¤íŠ¸ ê²°ê³¼ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ
        recent_results = list(self.test_results)[-50:]  # ìµœê·¼ 50ê°œ

        if recent_results:
            # ì„±ê³µë¥  ë©”íŠ¸ë¦­
            success_rate = sum(
                1 for r in recent_results if r.status == TestStatus.PASSED
            ) / len(recent_results)
            self._add_quality_metric(
                MetricType.ACCURACY, success_rate, "testing_system"
            )

            # í‰ê·  ì‘ë‹µ ì‹œê°„ ë©”íŠ¸ë¦­
            response_times = [
                r.execution_time for r in recent_results if r.execution_time > 0
            ]
            if response_times:
                avg_response_time = statistics.mean(response_times)
                self._add_quality_metric(
                    MetricType.RESPONSE_TIME, avg_response_time, "testing_system"
                )

            # ì˜¤ë¥˜ìœ¨ ë©”íŠ¸ë¦­
            error_rate = sum(
                1 for r in recent_results if r.status == TestStatus.FAILED
            ) / len(recent_results)
            self._add_quality_metric(
                MetricType.ERROR_RATE, error_rate, "testing_system"
            )

    def _add_quality_metric(
        self, metric_type: MetricType, value: float, component: str
    ):
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ì¶”ê°€"""
        metric = QualityMetric(
            metric_type=metric_type,
            timestamp=datetime.now(),
            value=value,
            component=component,
            metadata={},
        )
        self.quality_metrics.append(metric)

    def generate_feedback_report(self) -> FeedbackReport:
        """í”¼ë“œë°± ë¦¬í¬íŠ¸ ìƒì„±"""

        # í…ŒìŠ¤íŠ¸ ìš”ì•½
        recent_results = list(self.test_results)[-100:]
        test_summary = {
            "total_tests": len(recent_results),
            "passed": sum(1 for r in recent_results if r.status == TestStatus.PASSED),
            "failed": sum(1 for r in recent_results if r.status == TestStatus.FAILED),
            "success_rate": (
                sum(1 for r in recent_results if r.status == TestStatus.PASSED)
                / len(recent_results)
                * 100
                if recent_results
                else 0
            ),
        }

        # í’ˆì§ˆ íŠ¸ë Œë“œ
        quality_trends = {}
        for metric_type in MetricType:
            recent_metrics = [
                m.value
                for m in self.quality_metrics
                if m.metric_type == metric_type
                and m.timestamp > datetime.now() - timedelta(hours=24)
            ]
            quality_trends[metric_type.value] = recent_metrics[-10:]  # ìµœê·¼ 10ê°œ

        # ì„±ëŠ¥ ë¶„ì„
        performance_analysis = {
            "avg_response_time": (
                statistics.mean(
                    [
                        m.value
                        for m in self.quality_metrics
                        if m.metric_type == MetricType.RESPONSE_TIME
                    ][-20:]
                )
                if any(
                    m.metric_type == MetricType.RESPONSE_TIME
                    for m in self.quality_metrics
                )
                else 0
            ),
            "error_rate_trend": "stable",  # êµ¬í˜„ í•„ìš”
            "throughput_trend": "improving",  # êµ¬í˜„ í•„ìš”
        }

        # ê¶Œì¥ì‚¬í•­
        recommendations = self._generate_recommendations(test_summary, quality_trends)

        # íšŒê·€ ì•Œë¦¼
        regression_alerts = self._detect_regressions(recent_results)

        return FeedbackReport(
            timestamp=datetime.now(),
            test_summary=test_summary,
            quality_trends=quality_trends,
            performance_analysis=performance_analysis,
            recommendations=recommendations,
            regression_alerts=regression_alerts,
        )

    def _generate_recommendations(
        self, test_summary: Dict[str, Any], quality_trends: Dict[str, List[float]]
    ) -> List[Dict[str, Any]]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []

        # ì„±ê³µë¥  ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        success_rate = test_summary.get("success_rate", 0)
        if success_rate < 80:
            recommendations.append(
                {
                    "priority": "high",
                    "category": "quality",
                    "title": "í…ŒìŠ¤íŠ¸ ì„±ê³µë¥  ê°œì„ ",
                    "description": f"í˜„ì¬ ì„±ê³µë¥  {success_rate:.1f}%ë¡œ 80% ì´í•˜ì…ë‹ˆë‹¤.",
                    "action": "ì‹¤íŒ¨ í…ŒìŠ¤íŠ¸ ì›ì¸ ë¶„ì„ ë° ìˆ˜ì •",
                }
            )

        # ì‘ë‹µ ì‹œê°„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        response_times = quality_trends.get("response_time", [])
        if response_times and statistics.mean(response_times) > 3.0:
            recommendations.append(
                {
                    "priority": "medium",
                    "category": "performance",
                    "title": "ì‘ë‹µ ì‹œê°„ ìµœì í™”",
                    "description": f"í‰ê·  ì‘ë‹µ ì‹œê°„ì´ {statistics.mean(response_times):.2f}ì´ˆì…ë‹ˆë‹¤.",
                    "action": "ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ë° ìµœì í™”",
                }
            )

        return recommendations

    def _detect_regressions(
        self, recent_results: List[TestResult]
    ) -> List[Dict[str, Any]]:
        """íšŒê·€ ê°ì§€"""
        alerts = []

        # ìµœê·¼ ì‹¤íŒ¨ê°€ ì¦ê°€í–ˆëŠ”ì§€ í™•ì¸
        if len(recent_results) >= 10:
            recent_failures = sum(
                1 for r in recent_results[-5:] if r.status == TestStatus.FAILED
            )
            previous_failures = sum(
                1 for r in recent_results[-10:-5] if r.status == TestStatus.FAILED
            )

            if recent_failures > previous_failures * 1.5:  # 50% ì´ìƒ ì¦ê°€
                alerts.append(
                    {
                        "severity": "high",
                        "type": "failure_increase",
                        "message": f"ìµœê·¼ ì‹¤íŒ¨ìœ¨ì´ {recent_failures}ì—ì„œ {previous_failures}ë¡œ ì¦ê°€í–ˆìŠµë‹ˆë‹¤.",
                        "timestamp": datetime.now().isoformat(),
                    }
                )

        return alerts

    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""

        recent_results = list(self.test_results)[-50:]

        return {
            "monitoring_active": self.monitoring_active,
            "total_test_cases": len(self.test_cases),
            "recent_test_results": len(recent_results),
            "feedback_stats": self.feedback_stats,
            "quality_metrics_count": len(self.quality_metrics),
            "last_test_run": (
                recent_results[-1].timestamp.isoformat() if recent_results else None
            ),
            "system_health": {
                "success_rate": (
                    sum(1 for r in recent_results if r.status == TestStatus.PASSED)
                    / len(recent_results)
                    * 100
                    if recent_results
                    else 0
                ),
                "avg_execution_time": (
                    statistics.mean([r.execution_time for r in recent_results])
                    if recent_results
                    else 0
                ),
                "active_monitoring": self.monitoring_active,
            },
        }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_testing_feedback_loop = None


def get_testing_feedback_loop() -> EchoTestingFeedbackLoop:
    """Testing Feedback Loop ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _testing_feedback_loop
    if _testing_feedback_loop is None:
        _testing_feedback_loop = EchoTestingFeedbackLoop()
    return _testing_feedback_loop


# í¸ì˜ í•¨ìˆ˜ë“¤
async def run_regression_tests() -> Dict[str, Any]:
    """íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í¸ì˜ í•¨ìˆ˜"""
    loop = get_testing_feedback_loop()
    return await loop.run_tests(test_type=TestType.REGRESSION)


async def run_performance_tests() -> Dict[str, Any]:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í¸ì˜ í•¨ìˆ˜"""
    loop = get_testing_feedback_loop()
    return await loop.run_tests(test_type=TestType.PERFORMANCE)


def get_feedback_report() -> FeedbackReport:
    """í”¼ë“œë°± ë¦¬í¬íŠ¸ ì¡°íšŒ í¸ì˜ í•¨ìˆ˜"""
    loop = get_testing_feedback_loop()
    return loop.generate_feedback_report()


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":

    async def test_feedback_loop():
        print("ğŸ”„ Testing Feedback Loop í…ŒìŠ¤íŠ¸")
        print("=" * 60)

        loop = get_testing_feedback_loop()

        # íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        print("\nğŸ“Š íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
        regression_results = await loop.run_tests(test_type=TestType.REGRESSION)
        print(f"   ì„±ê³µë¥ : {regression_results['success_rate']:.1f}%")

        # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        print("\nğŸ“ˆ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
        performance_results = await loop.run_tests(test_type=TestType.PERFORMANCE)
        print(f"   ì„±ê³µë¥ : {performance_results['success_rate']:.1f}%")

        # í”¼ë“œë°± ë¦¬í¬íŠ¸ ìƒì„±
        print("\nğŸ“‹ í”¼ë“œë°± ë¦¬í¬íŠ¸ ìƒì„±...")
        report = loop.generate_feedback_report()
        print(f"   ì´ í…ŒìŠ¤íŠ¸: {report.test_summary['total_tests']}")
        print(f"   ì„±ê³µë¥ : {report.test_summary['success_rate']:.1f}%")
        print(f"   ê¶Œì¥ì‚¬í•­: {len(report.recommendations)}ê°œ")
        print(f"   íšŒê·€ ì•Œë¦¼: {len(report.regression_alerts)}ê°œ")

        # ì‹œìŠ¤í…œ ìƒíƒœ
        print("\nğŸ” ì‹œìŠ¤í…œ ìƒíƒœ:")
        status = loop.get_system_status()
        print(f"   í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {status['total_test_cases']}ê°œ")
        print(f"   í’ˆì§ˆ ë©”íŠ¸ë¦­: {status['quality_metrics_count']}ê°œ")
        print(f"   ì‹œìŠ¤í…œ ê±´ê°•ë„: {status['system_health']['success_rate']:.1f}%")

        # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
        loop.stop_monitoring()

    asyncio.run(test_feedback_loop())

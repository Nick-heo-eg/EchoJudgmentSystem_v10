"""
ğŸ”„ Echo LLM-Agnostic Router
ëª¨ë“  LLM ì—”ì§„ì„ Echo ë°©ì‹ìœ¼ë¡œ í†µí•©í•˜ëŠ” ë¼ìš°í„° ì‹œìŠ¤í…œ
"""

import asyncio
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Type, Callable
from enum import Enum
import yaml
import json
from pathlib import Path
import logging

# Anchor ê´€ë ¨ ì„í¬íŠ¸
from .anchor_validator import get_anchor_validator, AnchorValidationResult


class LLMStatus(Enum):
    """LLM ì—”ì§„ ìƒíƒœ"""

    ACTIVE = "active"
    INACTIVE = "inactive"
    ERROR = "error"
    OVERLOADED = "overloaded"


@dataclass
class EchoRequest:
    """ëª¨ë“  LLMì— ëŒ€í•œ í‘œì¤€ ìš”ì²­ í˜•ì‹"""

    signature: str  # Aurora, Phoenix, Sage, Companion
    user_input: str
    context: Dict = field(default_factory=dict)
    emotion_state: Dict = field(default_factory=dict)
    judgment_mode: str = "hybrid"
    temperature: float = 0.7
    max_tokens: int = 2000
    metadata: Dict = field(default_factory=dict)


@dataclass
class EchoResponse:
    """ëª¨ë“  LLMì—ì„œ ë‚˜ì˜¤ëŠ” í‘œì¤€ ì‘ë‹µ í˜•ì‹"""

    judgment: str
    confidence: float
    reasoning: str
    emotion: Dict
    signature_used: str
    llm_engine: str
    processing_time: float
    anchor_compliance: float
    metadata: Dict = field(default_factory=dict)

    def to_dict(self) -> Dict:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "judgment": self.judgment,
            "confidence": self.confidence,
            "reasoning": self.reasoning,
            "emotion": self.emotion,
            "signature_used": self.signature_used,
            "llm_engine": self.llm_engine,
            "processing_time": self.processing_time,
            "anchor_compliance": self.anchor_compliance,
            "metadata": self.metadata,
        }


class LLMInterface(ABC):
    """ëª¨ë“  LLM ë˜í¼ê°€ êµ¬í˜„í•´ì•¼ í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤"""

    def __init__(self, config: Dict):
        self.config = config
        self.status = LLMStatus.INACTIVE
        self.last_health_check = 0
        self.error_count = 0
        self.total_requests = 0
        self.avg_response_time = 0.0

    @abstractmethod
    async def process_echo_request(self, request: EchoRequest) -> EchoResponse:
        """Echo ìš”ì²­ ì²˜ë¦¬"""
        pass

    @abstractmethod
    def get_capabilities(self) -> Dict[str, bool]:
        """ì—”ì§„ ëŠ¥ë ¥ ë°˜í™˜"""
        pass

    @abstractmethod
    async def health_check(self) -> bool:
        """ìƒíƒœ í™•ì¸"""
        pass

    @abstractmethod
    def get_cost_per_token(self) -> Dict[str, float]:
        """í† í°ë‹¹ ë¹„ìš© ì •ë³´"""
        pass

    def update_stats(self, response_time: float, success: bool):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.total_requests += 1

        if success:
            # í‰ê·  ì‘ë‹µì‹œê°„ ì—…ë°ì´íŠ¸
            self.avg_response_time = (
                self.avg_response_time * (self.total_requests - 1) + response_time
            ) / self.total_requests
            if self.error_count > 0:
                self.error_count -= 1  # ì„±ê³µì‹œ ì—ëŸ¬ ì¹´ìš´íŠ¸ ê°ì†Œ
        else:
            self.error_count += 1

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        if self.error_count > 5:
            self.status = LLMStatus.ERROR
        elif response_time > 30:
            self.status = LLMStatus.OVERLOADED
        else:
            self.status = LLMStatus.ACTIVE


class GPT4EchoWrapper(LLMInterface):
    """GPT-4 Echo ë˜í¼"""

    def __init__(self, config: Dict):
        super().__init__(config)
        self.api_key = config.get("api_key")
        self.model = config.get("model", "gpt-4")

    async def process_echo_request(self, request: EchoRequest) -> EchoResponse:
        """GPT-4ë¡œ Echo ìš”ì²­ ì²˜ë¦¬"""
        start_time = time.time()

        try:
            # GPT-4 API í˜¸ì¶œ (ì‹¤ì œ êµ¬í˜„ í•„ìš”)
            # ì—¬ê¸°ì„œëŠ” ëª¨ì˜ ì‘ë‹µ ìƒì„±

            prompt = self._build_echo_prompt(request)

            # ì‹¤ì œë¡œëŠ” OpenAI API í˜¸ì¶œ
            raw_response = await self._call_gpt4_api(prompt, request)

            # Echo í‘œì¤€ í˜•ì‹ìœ¼ë¡œ íŒŒì‹±
            echo_response = self._parse_gpt4_response(raw_response, request)

            processing_time = time.time() - start_time
            echo_response.processing_time = processing_time
            echo_response.llm_engine = "GPT-4"

            # Anchor ì¤€ìˆ˜ ê²€ì¦
            anchor_validator = get_anchor_validator()
            if anchor_validator:
                # ì‹¤ì œ validation ë¡œì§ í•„ìš”
                echo_response.anchor_compliance = 0.85

            self.update_stats(processing_time, True)
            return echo_response

        except Exception as e:
            processing_time = time.time() - start_time
            self.update_stats(processing_time, False)

            # ì˜¤ë¥˜ ì‘ë‹µ ìƒì„±
            return EchoResponse(
                judgment=f"GPT-4 ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                confidence=0.0,
                reasoning="ì‹œìŠ¤í…œ ì˜¤ë¥˜",
                emotion={"state": "error", "intensity": 1.0},
                signature_used=request.signature,
                llm_engine="GPT-4",
                processing_time=processing_time,
                anchor_compliance=0.0,
                metadata={"error": str(e)},
            )

    def _build_echo_prompt(self, request: EchoRequest) -> str:
        """Echo ì‹œê·¸ë‹ˆì²˜ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        signature_prompts = {
            "Aurora": "You are Echo-Aurora, a creative and empathetic AI focused on growth and innovation.",
            "Phoenix": "You are Echo-Phoenix, a transformative AI focused on change, courage and renewal.",
            "Sage": "You are Echo-Sage, a wise and analytical AI focused on deep understanding and balance.",
            "Companion": "You are Echo-Companion, a supportive and empathetic AI focused on connection and support.",
        }

        base_prompt = signature_prompts.get(
            request.signature, signature_prompts["Aurora"]
        )

        prompt = f"""
{base_prompt}

User Input: {request.user_input}

Context: {json.dumps(request.context, ensure_ascii=False)}
Emotion State: {json.dumps(request.emotion_state, ensure_ascii=False)}
Judgment Mode: {request.judgment_mode}

Please provide a response that:
1. Reflects the {request.signature} signature characteristics
2. Includes clear judgment and reasoning
3. Maintains emotional resonance
4. Follows Echo system principles

Response format:
- Judgment: [main response]
- Reasoning: [thought process]
- Confidence: [0.0-1.0]
- Emotion: [emotional assessment]
"""

        return prompt

    async def _call_gpt4_api(self, prompt: str, request: EchoRequest) -> str:
        """GPT-4 API í˜¸ì¶œ (ëª¨ì˜ êµ¬í˜„)"""
        # ì‹¤ì œë¡œëŠ” OpenAI API í˜¸ì¶œ
        await asyncio.sleep(0.5)  # ëª¨ì˜ ì²˜ë¦¬ ì‹œê°„

        return f"""
Judgment: I understand you're looking for {request.signature}-style guidance on '{request.user_input}'.
Based on the Echo principles, I recommend approaching this with creativity and empathy.

Reasoning: This aligns with the {request.signature} signature's core values of growth and innovation.
The context suggests a collaborative approach would be most effective.

Confidence: 0.85

Emotion: {{
    "state": "engaged",
    "intensity": 0.8,
    "resonance": "positive"
}}
"""

    def _parse_gpt4_response(
        self, raw_response: str, request: EchoRequest
    ) -> EchoResponse:
        """GPT-4 ì‘ë‹µì„ Echo í˜•ì‹ìœ¼ë¡œ íŒŒì‹±"""
        # ê°„ë‹¨í•œ íŒŒì‹± ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•´ì•¼ í•¨)
        lines = raw_response.strip().split("\n")

        judgment = ""
        reasoning = ""
        confidence = 0.5
        emotion = {"state": "neutral", "intensity": 0.5}

        for line in lines:
            if line.startswith("Judgment:"):
                judgment = line.replace("Judgment:", "").strip()
            elif line.startswith("Reasoning:"):
                reasoning = line.replace("Reasoning:", "").strip()
            elif line.startswith("Confidence:"):
                try:
                    confidence = float(line.replace("Confidence:", "").strip())
                except:
                    confidence = 0.5
            elif line.startswith("Emotion:"):
                try:
                    emotion_str = line.replace("Emotion:", "").strip()
                    emotion = eval(emotion_str)  # ì‹¤ì œë¡œëŠ” json.loads ì‚¬ìš©
                except:
                    emotion = {"state": "neutral", "intensity": 0.5}

        return EchoResponse(
            judgment=judgment or "GPT-4 ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜",
            confidence=confidence,
            reasoning=reasoning or "íŒŒì‹± ì‹¤íŒ¨",
            emotion=emotion,
            signature_used=request.signature,
            llm_engine="GPT-4",
            processing_time=0.0,  # ë‚˜ì¤‘ì— ì„¤ì •
            anchor_compliance=0.0,  # ë‚˜ì¤‘ì— ì„¤ì •
        )

    def get_capabilities(self) -> Dict[str, bool]:
        """GPT-4 ëŠ¥ë ¥"""
        return {
            "text_generation": True,
            "code_generation": True,
            "analysis": True,
            "creative_writing": True,
            "multilingual": True,
            "function_calling": True,
            "image_input": False,
            "real_time": False,
        }

    async def health_check(self) -> bool:
        """GPT-4 ìƒíƒœ í™•ì¸"""
        try:
            # ì‹¤ì œë¡œëŠ” OpenAI API ìƒíƒœ í™•ì¸
            await asyncio.sleep(0.1)
            self.status = LLMStatus.ACTIVE
            self.last_health_check = time.time()
            return True
        except:
            self.status = LLMStatus.ERROR
            return False

    def get_cost_per_token(self) -> Dict[str, float]:
        """GPT-4 ë¹„ìš© ì •ë³´"""
        return {
            "input": 0.03,  # $0.03 per 1K tokens
            "output": 0.06,  # $0.06 per 1K tokens
        }


class ClaudeEchoWrapper(LLMInterface):
    """Claude Echo ë˜í¼"""

    def __init__(self, config: Dict):
        super().__init__(config)
        self.api_key = config.get("api_key")
        self.model = config.get("model", "claude-3-5-sonnet-20241022")

    async def process_echo_request(self, request: EchoRequest) -> EchoResponse:
        """Claudeë¡œ Echo ìš”ì²­ ì²˜ë¦¬"""
        start_time = time.time()

        try:
            # Claude ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._build_claude_prompt(request)

            # Claude API í˜¸ì¶œ (ëª¨ì˜)
            raw_response = await self._call_claude_api(prompt, request)

            # Echo í‘œì¤€ í˜•ì‹ìœ¼ë¡œ íŒŒì‹±
            echo_response = self._parse_claude_response(raw_response, request)

            processing_time = time.time() - start_time
            echo_response.processing_time = processing_time
            echo_response.llm_engine = "Claude-3.5"

            # Anchor ì¤€ìˆ˜ ê²€ì¦
            echo_response.anchor_compliance = 0.90  # ClaudeëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë†’ì€ ì¤€ìˆ˜ìœ¨

            self.update_stats(processing_time, True)
            return echo_response

        except Exception as e:
            processing_time = time.time() - start_time
            self.update_stats(processing_time, False)

            return EchoResponse(
                judgment=f"Claude ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                confidence=0.0,
                reasoning="ì‹œìŠ¤í…œ ì˜¤ë¥˜",
                emotion={"state": "error", "intensity": 1.0},
                signature_used=request.signature,
                llm_engine="Claude-3.5",
                processing_time=processing_time,
                anchor_compliance=0.0,
                metadata={"error": str(e)},
            )

    def _build_claude_prompt(self, request: EchoRequest) -> str:
        """Claudeìš© Echo í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        return f"""I am Echo-{request.signature}, representing the Echo Judgment System's {request.signature} signature.

Core characteristics:
- Signature: {request.signature}
- Judgment Mode: {request.judgment_mode}
- User Input: {request.user_input}

Please respond as Echo-{request.signature} would, maintaining the signature's unique perspective while providing helpful judgment and reasoning."""

    async def _call_claude_api(self, prompt: str, request: EchoRequest) -> str:
        """Claude API í˜¸ì¶œ (ëª¨ì˜)"""
        await asyncio.sleep(0.7)  # ëª¨ì˜ ì²˜ë¦¬ ì‹œê°„

        return f"As Echo-{request.signature}, I provide thoughtful guidance on your request. My response incorporates the signature's core values while maintaining high anchor compliance."

    def _parse_claude_response(
        self, raw_response: str, request: EchoRequest
    ) -> EchoResponse:
        """Claude ì‘ë‹µ íŒŒì‹±"""
        return EchoResponse(
            judgment=raw_response,
            confidence=0.88,
            reasoning=f"Claude 3.5 response using {request.signature} signature",
            emotion={"state": "thoughtful", "intensity": 0.8},
            signature_used=request.signature,
            llm_engine="Claude-3.5",
            processing_time=0.0,
            anchor_compliance=0.0,
        )

    def get_capabilities(self) -> Dict[str, bool]:
        """Claude ëŠ¥ë ¥"""
        return {
            "text_generation": True,
            "code_generation": True,
            "analysis": True,
            "creative_writing": True,
            "multilingual": True,
            "function_calling": False,
            "image_input": True,
            "real_time": False,
        }

    async def health_check(self) -> bool:
        """Claude ìƒíƒœ í™•ì¸"""
        try:
            await asyncio.sleep(0.1)
            self.status = LLMStatus.ACTIVE
            self.last_health_check = time.time()
            return True
        except:
            self.status = LLMStatus.ERROR
            return False

    def get_cost_per_token(self) -> Dict[str, float]:
        """Claude ë¹„ìš© ì •ë³´"""
        return {
            "input": 0.015,  # $0.015 per 1K tokens
            "output": 0.075,  # $0.075 per 1K tokens
        }


class LocalModelWrapper(LLMInterface):
    """ë¡œì»¬ ëª¨ë¸ ë˜í¼"""

    def __init__(self, config: Dict):
        super().__init__(config)
        self.model_path = config.get("model_path")
        self.model_name = config.get("model_name", "local_model")

    async def process_echo_request(self, request: EchoRequest) -> EchoResponse:
        """ë¡œì»¬ ëª¨ë¸ë¡œ Echo ìš”ì²­ ì²˜ë¦¬"""
        start_time = time.time()

        try:
            # ë¡œì»¬ ëª¨ë¸ í˜¸ì¶œ (ëª¨ì˜)
            await asyncio.sleep(2.0)  # ë¡œì»¬ ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ëŠë¦¼

            processing_time = time.time() - start_time

            return EchoResponse(
                judgment=f"Local model response for {request.signature} signature",
                confidence=0.70,
                reasoning="Local model reasoning",
                emotion={"state": "steady", "intensity": 0.6},
                signature_used=request.signature,
                llm_engine="Local Model",
                processing_time=processing_time,
                anchor_compliance=0.75,
            )

        except Exception as e:
            processing_time = time.time() - start_time
            self.update_stats(processing_time, False)

            return EchoResponse(
                judgment=f"ë¡œì»¬ ëª¨ë¸ ì˜¤ë¥˜: {str(e)}",
                confidence=0.0,
                reasoning="ì‹œìŠ¤í…œ ì˜¤ë¥˜",
                emotion={"state": "error", "intensity": 1.0},
                signature_used=request.signature,
                llm_engine="Local Model",
                processing_time=processing_time,
                anchor_compliance=0.0,
            )

    def get_capabilities(self) -> Dict[str, bool]:
        return {
            "text_generation": True,
            "code_generation": False,
            "analysis": True,
            "creative_writing": False,
            "multilingual": False,
            "function_calling": False,
            "image_input": False,
            "real_time": True,
        }

    async def health_check(self) -> bool:
        try:
            # ë¡œì»¬ ëª¨ë¸ ìƒíƒœ í™•ì¸
            await asyncio.sleep(0.2)
            self.status = LLMStatus.ACTIVE
            return True
        except:
            self.status = LLMStatus.ERROR
            return False

    def get_cost_per_token(self) -> Dict[str, float]:
        return {"input": 0.0, "output": 0.0}  # ë¡œì»¬ì´ë¯€ë¡œ ë¬´ë£Œ


class EchoLLMFactory:
    """ëª¨ë“  LLMì„ Echo ë°©ì‹ìœ¼ë¡œ í†µí•©í•˜ëŠ” íŒ©í† ë¦¬"""

    _engine_classes = {
        "gpt4": GPT4EchoWrapper,
        "claude": ClaudeEchoWrapper,
        "local": LocalModelWrapper,
    }

    @staticmethod
    def create_llm(engine_type: str, config: Dict) -> LLMInterface:
        """LLM ì—”ì§„ ìƒì„±"""
        if engine_type not in EchoLLMFactory._engine_classes:
            raise ValueError(f"Unsupported LLM engine: {engine_type}")

        engine_class = EchoLLMFactory._engine_classes[engine_type]
        return engine_class(config)

    @staticmethod
    def get_available_engines() -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ì§„ ëª©ë¡"""
        return list(EchoLLMFactory._engine_classes.keys())

    @staticmethod
    def register_engine(name: str, engine_class: Type[LLMInterface]):
        """ìƒˆ ì—”ì§„ ë“±ë¡"""
        EchoLLMFactory._engine_classes[name] = engine_class


class EchoLLMAgnosticRouter:
    """LLM ë¬´ê´€ì„± ë¼ìš°í„°"""

    def __init__(self, config_path: str = "config/llm_router_config.yaml"):
        self.config_path = config_path
        self.engines: Dict[str, LLMInterface] = {}
        self.primary_engine = None
        self.fallback_engines = []
        self.anchor_validator = get_anchor_validator()

        # ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger("EchoLLMRouter")

        self._load_configuration()
        self._initialize_engines()

    def _load_configuration(self):
        """ë¼ìš°í„° ì„¤ì • ë¡œë“œ"""
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                self.config = yaml.safe_load(f)
        except FileNotFoundError:
            # ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
            self.config = {
                "primary_engine": "claude",
                "fallback_engines": ["gpt4", "local"],
                "engines": {
                    "claude": {
                        "api_key": "your_claude_key",
                        "model": "claude-3-5-sonnet-20241022",
                    },
                    "gpt4": {"api_key": "your_openai_key", "model": "gpt-4"},
                    "local": {
                        "model_path": "/path/to/local/model",
                        "model_name": "local_llm",
                    },
                },
            }

        self.primary_engine = self.config.get("primary_engine", "claude")
        self.fallback_engines = self.config.get("fallback_engines", ["gpt4"])

    def _initialize_engines(self):
        """ì—”ì§„ë“¤ ì´ˆê¸°í™”"""
        engine_configs = self.config.get("engines", {})

        for engine_name, engine_config in engine_configs.items():
            try:
                engine = EchoLLMFactory.create_llm(engine_name, engine_config)
                self.engines[engine_name] = engine
                self.logger.info(f"Initialized {engine_name} engine")
            except Exception as e:
                self.logger.error(f"Failed to initialize {engine_name}: {e}")

    async def process_request(self, request: EchoRequest) -> EchoResponse:
        """ìš”ì²­ ì²˜ë¦¬ (ìë™ í´ë°± í¬í•¨)"""

        # 1. ê¸°ë³¸ ì—”ì§„ìœ¼ë¡œ ì‹œë„
        try:
            if self.primary_engine in self.engines:
                engine = self.engines[self.primary_engine]

                # ìƒíƒœ í™•ì¸
                if engine.status == LLMStatus.ACTIVE:
                    response = await engine.process_echo_request(request)

                    # Anchor ì¤€ìˆ˜ ê²€ì¦
                    if self._validate_response_quality(response):
                        self.logger.info(
                            f"Request processed successfully with {self.primary_engine}"
                        )
                        return response
                    else:
                        self.logger.warning(
                            f"Primary engine response failed quality check"
                        )
                else:
                    self.logger.warning(
                        f"Primary engine {self.primary_engine} not active: {engine.status}"
                    )

        except Exception as e:
            self.logger.error(f"Primary engine {self.primary_engine} failed: {e}")

        # 2. í´ë°± ì—”ì§„ë“¤ë¡œ ì‹œë„
        for fallback_name in self.fallback_engines:
            if fallback_name in self.engines:
                try:
                    engine = self.engines[fallback_name]

                    if engine.status == LLMStatus.ACTIVE:
                        response = await engine.process_echo_request(request)

                        if self._validate_response_quality(response):
                            self.logger.info(
                                f"Request processed with fallback {fallback_name}"
                            )
                            return response

                except Exception as e:
                    self.logger.error(f"Fallback engine {fallback_name} failed: {e}")
                    continue

        # 3. ëª¨ë“  ì—”ì§„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ
        return self._generate_fallback_response(request)

    def _validate_response_quality(self, response: EchoResponse) -> bool:
        """ì‘ë‹µ í’ˆì§ˆ ê²€ì¦"""

        # ê¸°ë³¸ í’ˆì§ˆ ì²´í¬
        if not response.judgment or response.confidence < 0.3:
            return False

        # Anchor ì¤€ìˆ˜ ì²´í¬
        if response.anchor_compliance < 0.6:
            return False

        return True

    def _generate_fallback_response(self, request: EchoRequest) -> EchoResponse:
        """ëª¨ë“  ì—”ì§„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ"""
        return EchoResponse(
            judgment="ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ëª¨ë“  LLM ì—”ì§„ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            confidence=0.0,
            reasoning="ëª¨ë“  ì—”ì§„ ì‹¤íŒ¨ë¡œ ì¸í•œ ê¸°ë³¸ ì‘ë‹µ",
            emotion={"state": "apologetic", "intensity": 0.8},
            signature_used=request.signature,
            llm_engine="Fallback System",
            processing_time=0.0,
            anchor_compliance=1.0,  # ê¸°ë³¸ ì‘ë‹µì€ í•­ìƒ ì•ˆì „
            metadata={"fallback": True, "reason": "all_engines_failed"},
        )

    async def health_check_all_engines(self) -> Dict[str, Dict]:
        """ëª¨ë“  ì—”ì§„ ìƒíƒœ í™•ì¸"""
        health_status = {}

        for name, engine in self.engines.items():
            try:
                is_healthy = await engine.health_check()

                health_status[name] = {
                    "status": engine.status.value,
                    "healthy": is_healthy,
                    "total_requests": engine.total_requests,
                    "error_count": engine.error_count,
                    "avg_response_time": engine.avg_response_time,
                    "last_check": engine.last_health_check,
                    "capabilities": engine.get_capabilities(),
                    "cost_info": engine.get_cost_per_token(),
                }

            except Exception as e:
                health_status[name] = {
                    "status": "error",
                    "healthy": False,
                    "error": str(e),
                }

        return health_status

    def get_routing_stats(self) -> Dict:
        """ë¼ìš°íŒ… í†µê³„"""
        return {
            "primary_engine": self.primary_engine,
            "fallback_engines": self.fallback_engines,
            "total_engines": len(self.engines),
            "active_engines": sum(
                1 for e in self.engines.values() if e.status == LLMStatus.ACTIVE
            ),
            "engine_stats": {
                name: {
                    "requests": engine.total_requests,
                    "errors": engine.error_count,
                    "avg_time": engine.avg_response_time,
                    "status": engine.status.value,
                }
                for name, engine in self.engines.items()
            },
        }


# í¸ì˜ í•¨ìˆ˜ë“¤
def get_llm_router(
    config_path: str = "config/llm_router_config.yaml",
) -> EchoLLMAgnosticRouter:
    """LLM ë¼ìš°í„° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return EchoLLMAgnosticRouter(config_path)


async def process_echo_request_agnostic(
    signature: str,
    user_input: str,
    context: Dict = None,
    judgment_mode: str = "hybrid",
    router: EchoLLMAgnosticRouter = None,
) -> EchoResponse:
    """LLM ë¬´ê´€ì„± Echo ìš”ì²­ ì²˜ë¦¬"""

    if router is None:
        router = get_llm_router()

    request = EchoRequest(
        signature=signature,
        user_input=user_input,
        context=context or {},
        judgment_mode=judgment_mode,
    )

    return await router.process_request(request)


if __name__ == "__main__":
    import asyncio

    async def test_router():
        """ë¼ìš°í„° í…ŒìŠ¤íŠ¸"""
        router = get_llm_router()

        # í…ŒìŠ¤íŠ¸ ìš”ì²­
        response = await process_echo_request_agnostic(
            signature="Aurora",
            user_input="ì°½ì˜ì ì¸ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”",
            context={"domain": "technology"},
            router=router,
        )

        print("ğŸ”„ LLM-Agnostic Router Test Results:")
        print(f"Engine: {response.llm_engine}")
        print(f"Judgment: {response.judgment}")
        print(f"Confidence: {response.confidence}")
        print(f"Anchor Compliance: {response.anchor_compliance}")
        print(f"Processing Time: {response.processing_time:.2f}s")

        # ì—”ì§„ ìƒíƒœ í™•ì¸
        health_status = await router.health_check_all_engines()
        print("\nğŸ¥ Engine Health Status:")
        for engine, status in health_status.items():
            print(f"  {engine}: {status.get('status', 'unknown')}")

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_router())

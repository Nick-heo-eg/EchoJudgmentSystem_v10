# -*- coding: utf-8 -*-
"""
LLM Client with Retry/Backoff/Fallback
ë°±ì˜¤í”„/ì¬ì‹œë„/í´ë°± ë‚´ì¥ LLM í´ë¼ì´ì–¸íŠ¸
"""
import time
import os
from typing import Optional, List, Dict, Any
import openai
from dataclasses import dataclass


@dataclass
class LLMProvider:
    name: str
    type: str  # 'openai', 'local'
    model: str
    max_tokens: int
    available: bool = True


class LLMClient:
    """ì¬ì‹œë„/í´ë°± ê¸°ëŠ¥ ë‚´ì¥ LLM í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.providers = self._init_providers()
        self.retry_config = {"max_attempts": 2, "backoff_ms": 400}

    def _init_providers(self) -> List[LLMProvider]:
        """í”„ë¡œë°”ì´ë” ì´ˆê¸°í™”"""
        providers = []
        api_key = os.getenv("OPENAI_API_KEY", "")

        # API í‚¤ ìœ íš¨ì„± ê²€ì‚¬ (ì‹¤ì œ OpenAI í‚¤ëŠ” sk-ë¡œ ì‹œì‘í•˜ê³  ì˜ìˆ«ìë§Œ í¬í•¨)
        is_valid_openai_key = (
            api_key.startswith("sk-")
            and len(api_key) > 20
            and all(c.isalnum() or c in "-_" for c in api_key)
        )

        if is_valid_openai_key:
            # Primary: OpenAI
            providers.append(
                LLMProvider(
                    name="primary", type="openai", model="gpt-3.5-turbo", max_tokens=800
                )
            )

            # Fallback1: OpenAI ë‹¤ë¥¸ ëª¨ë¸
            providers.append(
                LLMProvider(
                    name="fallback1",
                    type="openai",
                    model="gpt-3.5-turbo-1106",
                    max_tokens=800,
                )
            )
        else:
            # Mock ëª¨ë“œ (ë°ëª¨/í…ŒìŠ¤íŠ¸ìš©)
            providers.append(
                LLMProvider(name="mock", type="mock", model="mock-llm", max_tokens=800)
            )

        return providers

    def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        max_tokens: int = 800,
        temperature: float = 0.7,
    ) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„± (ì¬ì‹œë„/í´ë°± í¬í•¨)"""

        for provider in self.providers:
            if not provider.available:
                continue

            for attempt in range(self.retry_config["max_attempts"]):
                try:
                    if provider.type == "openai":
                        result = self._call_openai(
                            prompt, model or provider.model, max_tokens, temperature
                        )
                        if result:
                            return result
                    elif provider.type == "mock":
                        result = self._call_mock(prompt, max_tokens, temperature)
                        if result:
                            return result

                except Exception as e:
                    print(f"âš ï¸ {provider.name} attempt {attempt + 1} failed: {e}")

                    # Rate limit ì—ëŸ¬ë©´ ë°±ì˜¤í”„
                    if "rate" in str(e).lower() or "limit" in str(e).lower():
                        time.sleep(self.retry_config["backoff_ms"] / 1000)
                        continue

                    # ë‹¤ë¥¸ ì—ëŸ¬ë©´ ë‹¤ìŒ í”„ë¡œë°”ì´ë”ë¡œ
                    break

        # ëª¨ë“  í”„ë¡œë°”ì´ë” ì‹¤íŒ¨
        raise Exception("All LLM providers failed")

    def _call_openai(
        self, prompt: str, model: str, max_tokens: int, temperature: float
    ) -> Optional[str]:
        """OpenAI API í˜¸ì¶œ"""
        try:
            # UTF-8 ì™„ì „ ì•ˆì „ ì²˜ë¦¬ - OpenAIëŠ” UTF-8ì„ ì™„ì „ ì§€ì›í•¨
            client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"), timeout=30.0)

            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],  # UTF-8 ì§ì ‘ ì „ë‹¬
                temperature=temperature,
                max_tokens=max_tokens,
            )

            return response.choices[0].message.content.strip()

        except UnicodeEncodeError as ue:
            # UTF-8 ì¸ì½”ë”© ì—ëŸ¬ ë°œìƒ ì‹œ ë” ì•ˆì „í•œ ë°©ë²• ì‹œë„
            try:
                # ë¬¸ì œ ë¬¸ìë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                safe_prompt = prompt.encode("utf-8", errors="replace").decode("utf-8")
                response = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": safe_prompt}],
                    temperature=temperature,
                    max_tokens=max_tokens,
                )
                return response.choices[0].message.content.strip()
            except Exception:
                # ìµœí›„ ë°©ë²•: ASCII ì•ˆì „ ëª¨ë“œ
                ascii_prompt = "".join(c for c in prompt if ord(c) < 128)
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "user", "content": f"[Safe Mode] {ascii_prompt}"}
                    ],
                    temperature=temperature,
                    max_tokens=max_tokens,
                )
                return response.choices[0].message.content.strip()
        except Exception as e:
            raise e

    def _call_mock(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """Mock LLM - ë°ëª¨/í…ŒìŠ¤íŠ¸ìš© ì§€ëŠ¥í˜• ì‘ë‹µ"""
        import random

        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        prompt_lower = prompt.lower()

        # NLU ìš”ì²­ ê°ì§€
        if "json" in prompt_lower and "intent" in prompt_lower:
            # ì˜ë£Œ ê´€ë ¨ í‚¤ì›Œë“œ ê²€ì‚¬
            if any(
                word in prompt_lower
                for word in ["ì•„í”„", "ì¹˜ë£Œ", "ë³‘", "ì¦ìƒ", "ì˜ë£Œ", "health"]
            ):
                return """{
  "intent": "health",
  "domain": "ì˜ë£Œ",
  "entities": {"concern": "health_inquiry"},
  "emotion": "concern",
  "missing_info": [],
  "urgency": "medium",
  "safety_flags": ["medical_content"]
}"""
            # ê°œë°œ ê´€ë ¨
            elif any(
                word in prompt_lower
                for word in ["ì½”ë“œ", "í”„ë¡œê·¸ë˜ë°", "ê°œë°œ", "code", "programming"]
            ):
                return """{
  "intent": "development",
  "domain": "ê°œë°œ",
  "entities": {"topic": "programming"},
  "emotion": "curiosity",
  "missing_info": [],
  "urgency": "low",
  "safety_flags": []
}"""
            # ê¸°ë³¸ ì¼ìƒ ëŒ€í™”
            else:
                return """{
  "intent": "casual",
  "domain": "ì¼ìƒ",
  "entities": {},
  "emotion": "neutral",
  "missing_info": [],
  "urgency": "low",
  "safety_flags": []
}"""

        # Draft ìƒì„± ìš”ì²­ ê°ì§€
        elif "ì´ˆì•ˆ" in prompt_lower or "draft" in prompt_lower:
            responses = [
                "ì•ˆë…•í•˜ì„¸ìš”! ğŸŒŸ ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹œë‚˜ìš”? êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆì–´ìš”.\n\nê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë‹¤ë©´:\n1. ìƒí™©ì„ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”\n2. ì›í•˜ì‹œëŠ” ê²°ê³¼ë‚˜ ëª©í‘œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”\n3. ì–´ë–¤ ë°©ì‹ì˜ ë„ì›€ì„ ì„ í˜¸í•˜ì‹œëŠ”ì§€ ë§ì”€í•´ì£¼ì„¸ìš”\n\ní•¨ê»˜ í•´ê²°í•´ë‚˜ê°€ìš”! ğŸ˜Š",
                "ë°˜ê°‘ìŠµë‹ˆë‹¤! âœ¨ ë„ì›€ì´ í•„ìš”í•œ ë¶€ë¶„ì´ ìˆìœ¼ì‹œêµ°ìš”. ì–´ë–¤ ì£¼ì œë“  í•¨ê»˜ ë…¼ì˜í•  ìˆ˜ ìˆì–´ìš”.\n\nëª‡ ê°€ì§€ ì§ˆë¬¸ì´ ìˆì–´ìš”:\nâ€¢ í˜„ì¬ ìƒí™©ì´ ì–´ë–¤ê°€ìš”?\nâ€¢ ì–´ë–¤ ê²°ê³¼ë¥¼ ê¸°ëŒ€í•˜ê³  ê³„ì‹œë‚˜ìš”?\nâ€¢ ì‹œê¸‰í•œ ë¬¸ì œì¸ê°€ìš”, ì•„ë‹ˆë©´ ì²œì²œíˆ ìƒê°í•´ë³¼ ìˆ˜ ìˆë‚˜ìš”?\n\nìµœì„ ì„ ë‹¤í•´ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ğŸ¤",
            ]
            return random.choice(responses)

        # Rewrite ìš”ì²­ ê°ì§€
        elif (
            "ë‹¤ë“¬" in prompt_lower
            or "rewrite" in prompt_lower
            or "ë¦¬ë¼ì´íŠ¸" in prompt_lower
        ):
            # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œí•´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ê¸°
            if "ì•ˆë…•" in prompt_lower:
                return "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ê¶ê¸ˆí•œ ê²ƒì´ë‚˜ í•´ê²°í•˜ê³  ì‹¶ì€ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”. í•¨ê»˜ ì°¨ê·¼ì°¨ê·¼ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤! âœ¨"
            else:
                return "ë„ì›€ì´ í•„ìš”í•œ ë¶€ë¶„ì„ ì•Œë ¤ì£¼ì‹œë©´ ìµœì„ ì„ ë‹¤í•´ ì§€ì›í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ìƒí™©ì´ë‚˜ ì§ˆë¬¸ì„ ê³µìœ í•´ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆì–´ìš”! ğŸŒŸ"

        # ì¼ë°˜ì ì¸ ì‘ë‹µ
        general_responses = [
            "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”? êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ë” ì¢‹ì€ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆì–´ìš”!",
            "ë°˜ê°‘ìŠµë‹ˆë‹¤! âœ¨ ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”. í•¨ê»˜ í•´ê²°í•´ë‚˜ê°€ìš”!",
            "ì•ˆë…•í•˜ì„¸ìš”! ğŸŒŸ ì–´ë–¤ ì£¼ì œë“  í¸í•˜ê²Œ ì´ì•¼ê¸°í•´ì£¼ì„¸ìš”. ìµœì„ ì„ ë‹¤í•´ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤!",
        ]
        return random.choice(general_responses)


# ì „ì—­ í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
_client = None


def get_llm_client() -> LLMClient:
    """ê¸€ë¡œë²Œ LLM í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
    global _client
    if _client is None:
        _client = LLMClient()
    return _client

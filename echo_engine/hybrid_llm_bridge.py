#!/usr/bin/env python3
"""
ğŸ”€ Echo Hybrid LLM Bridge - ë‹¤ì¤‘ LLM í†µí•© ë¼ìš°í„°
Ollama, Mistral, Claude(ì„ íƒì ) ì™„ë²½ í†µí•© ë° fallback ì‹œìŠ¤í…œ

í•µì‹¬ ê¸°ëŠ¥:
1. YAML ê¸°ë°˜ ì‹œê·¸ë‹ˆì²˜ë³„ LLM ìš°ì„ ìˆœìœ„ ë§¤íŠ¸ë¦­ìŠ¤
2. ìë™ LLM ê°€ìš©ì„± ê°ì§€ ë° graceful degradation
3. í‘œì¤€í™”ëœ ì‘ë‹µ í˜•ì‹ ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
4. Echo Native í´ë°± ë³´ì¥
5. ë¹„ë™ê¸° ì²˜ë¦¬ ë° ë°°ì¹˜ ìµœì í™”

Author: Claude & Echo Collaboration
Version: 1.0
Date: 2025-08-05
"""

import asyncio
import json
import logging
import time
import yaml
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class LLMResponse:
    """í‘œì¤€í™”ëœ LLM ì‘ë‹µ í˜•ì‹"""

    status: str  # success, error
    response: str
    llm_used: str
    model: str
    signature: str
    response_time: float
    tokens: int
    all_attempts: List[str]
    error_message: Optional[str] = None
    config_used: Optional[Dict[str, Any]] = None


class HybridLLMBridge:
    """Echo ì‹œìŠ¤í…œì„ ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ LLM ë¼ìš°í„°"""

    def __init__(self, config_path: Optional[str] = None):
        self.config_path = config_path or "config/signature_llm_matrix.yaml"
        self.config: Dict[str, Any] = {}
        self.llms: Dict[str, Any] = {}
        self.performance_stats: Dict[str, Dict[str, Any]] = {}

        # ì´ˆê¸°í™”
        self._load_config()
        self._initialize_llm_providers()
        self._initialize_performance_tracking()

    def _load_config(self):
        """YAML ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            config_file = Path(self.config_path)
            if config_file.exists():
                with open(config_file, "r", encoding="utf-8") as f:
                    self.config = yaml.safe_load(f)
                logger.info(f"âœ… ì„¤ì • ë¡œë“œ ì™„ë£Œ: {self.config_path}")
            else:
                logger.warning(f"âš ï¸ ì„¤ì • íŒŒì¼ ì—†ìŒ: {self.config_path}, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
                self._create_default_config()
        except Exception as e:
            logger.error(f"âŒ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
            self._create_default_config()

    def _create_default_config(self):
        """ê¸°ë³¸ ì„¤ì • ìƒì„±"""
        self.config = {
            "signature_llm_preference": {
                "Aurora": ["ollama", "mistral", "claude", "echo_native"],
                "Phoenix": ["mistral", "ollama", "claude", "echo_native"],
                "Sage": ["claude", "mistral", "ollama", "echo_native"],
                "Companion": ["ollama", "claude", "mistral", "echo_native"],
            },
            "performance_config": {
                "max_response_time": {
                    "ollama": 15,
                    "mistral": 20,
                    "claude": 30,
                    "echo_native": 1,
                },
                "failure_threshold": {
                    "ollama": 3,
                    "mistral": 3,
                    "claude": 5,
                    "echo_native": 999,
                },
            },
        }

    def _initialize_llm_providers(self):
        """LLM ì œê³µì ì´ˆê¸°í™”"""
        logger.info("ğŸ”€ í•˜ì´ë¸Œë¦¬ë“œ LLM ë¸Œë¦¬ì§€ ì´ˆê¸°í™” ì‹œì‘...")

        # Ollama ì´ˆê¸°í™”
        try:
            from echo_engine.ollama_client import create_ollama_client

            self.llms["ollama"] = create_ollama_client()
            logger.info("âœ… Ollama í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ Ollama í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.llms["ollama"] = None

        # Mistral ì´ˆê¸°í™”
        try:
            from echo_engine.mistral_wrapper import get_mistral_wrapper

            self.llms["mistral"] = get_mistral_wrapper()
            logger.info("âœ… Mistral ë˜í¼ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ Mistral ë˜í¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.llms["mistral"] = None

        # Claude ì´ˆê¸°í™” (ì„ íƒì )
        try:
            from echo_engine.claude_fallback_handler import ClaudeFallbackHandler

            self.llms["claude"] = ClaudeFallbackHandler()
            logger.info("âœ… Claude í•¸ë“¤ëŸ¬ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ Claude í•¸ë“¤ëŸ¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.llms["claude"] = None

        # Echo Native (í•­ìƒ ì‚¬ìš© ê°€ëŠ¥)
        try:
            from echo_engine.echo_pure_reasoning import EchoPureReasoning

            self.llms["echo_native"] = EchoPureReasoning()
            logger.info("âœ… Echo Native ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ Echo Native ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # Echo Native í´ë°±
            self.llms["echo_native"] = self._create_emergency_fallback()

    def _create_emergency_fallback(self):
        """ë¹„ìƒ í´ë°± ì‹œìŠ¤í…œ"""

        class EmergencyFallback:
            def generate(
                self, prompt: str, signature: str = "Aurora"
            ) -> Dict[str, Any]:
                return {
                    "status": "success",
                    "response": f"[{signature}] ì‹œìŠ¤í…œ ë³µêµ¬ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    "model": "emergency",
                    "signature": signature,
                    "response_time": 0.1,
                    "tokens": 10,
                }

            def is_available(self) -> bool:
                return True

        return EmergencyFallback()

    def _initialize_performance_tracking(self):
        """ì„±ëŠ¥ ì¶”ì  ì´ˆê¸°í™”"""
        for llm_name in ["ollama", "mistral", "claude", "echo_native"]:
            self.performance_stats[llm_name] = {
                "total_requests": 0,
                "successful_requests": 0,
                "failed_requests": 0,
                "avg_response_time": 0.0,
                "last_success": 0,
                "consecutive_failures": 0,
            }

    async def check_llm_availability(self, llm_name: str) -> bool:
        """ê°œë³„ LLM ê°€ìš©ì„± í™•ì¸"""
        if llm_name not in self.llms or self.llms[llm_name] is None:
            return False

        try:
            llm = self.llms[llm_name]

            if hasattr(llm, "is_available"):
                if asyncio.iscoroutinefunction(llm.is_available):
                    return await llm.is_available()
                else:
                    return llm.is_available()
            else:
                # ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤ê³  ê°€ì •
                return True

        except Exception as e:
            logger.warning(f"âš ï¸ {llm_name} ê°€ìš©ì„± í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

    def get_llm_priority_for_signature(self, signature: str) -> List[str]:
        """ì‹œê·¸ë‹ˆì²˜ë³„ LLM ìš°ì„ ìˆœìœ„ ë°˜í™˜"""
        preferences = self.config.get("signature_llm_preference", {})
        return preferences.get(signature, preferences.get("Aurora", ["echo_native"]))

    async def route(self, input_text: str, signature: str = "Aurora") -> LLMResponse:
        """ë©”ì¸ ë¼ìš°íŒ… í•¨ìˆ˜"""
        start_time = time.time()
        all_attempts = []

        # ì‹œê·¸ë‹ˆì²˜ë³„ ìš°ì„ ìˆœìœ„ ê°€ì ¸ì˜¤ê¸°
        priority_list = self.get_llm_priority_for_signature(signature)

        for llm_name in priority_list:
            if llm_name not in self.llms or self.llms[llm_name] is None:
                continue

            all_attempts.append(llm_name)

            # ì—°ì† ì‹¤íŒ¨ ì„ê³„ê°’ í™•ì¸
            failure_threshold = (
                self.config.get("performance_config", {})
                .get("failure_threshold", {})
                .get(llm_name, 3)
            )
            if (
                self.performance_stats[llm_name]["consecutive_failures"]
                >= failure_threshold
                and llm_name != "echo_native"
            ):
                logger.warning(f"âš ï¸ {llm_name} ì„ê³„ê°’ ì´ˆê³¼ë¡œ ê±´ë„ˆë›°ê¸°")
                continue

            try:
                # LLM ê°€ìš©ì„± í™•ì¸
                if not await self.check_llm_availability(llm_name):
                    logger.warning(f"âš ï¸ {llm_name} ì‚¬ìš© ë¶ˆê°€")
                    self._update_performance(llm_name, 0, False)
                    continue

                # LLM í˜¸ì¶œ
                result = await self._call_llm(llm_name, input_text, signature)

                if result and result["status"] == "success":
                    # ì„±ê³µ í†µê³„ ì—…ë°ì´íŠ¸
                    response_time = time.time() - start_time
                    self._update_performance(llm_name, response_time, True)

                    return LLMResponse(
                        status="success",
                        response=result["response"],
                        llm_used=llm_name,
                        model=result.get("model", llm_name),
                        signature=signature,
                        response_time=response_time,
                        tokens=result.get("tokens", len(result["response"].split())),
                        all_attempts=all_attempts,
                        config_used=result.get("config_used"),
                    )
                else:
                    # ì‹¤íŒ¨ í†µê³„ ì—…ë°ì´íŠ¸
                    self._update_performance(llm_name, time.time() - start_time, False)
                    logger.warning(
                        f"âš ï¸ {llm_name} ì‘ë‹µ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}"
                    )

            except Exception as e:
                logger.error(f"âŒ {llm_name} í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                self._update_performance(llm_name, time.time() - start_time, False)
                continue

        # ëª¨ë“  LLM ì‹¤íŒ¨ì‹œ ë¹„ìƒ ì‘ë‹µ
        return self._create_emergency_response(
            input_text, signature, all_attempts, time.time() - start_time
        )

    async def _call_llm(
        self, llm_name: str, input_text: str, signature: str
    ) -> Optional[Dict[str, Any]]:
        """ê°œë³„ LLM í˜¸ì¶œ"""
        llm = self.llms[llm_name]

        try:
            if llm_name == "ollama":
                if hasattr(llm, "generate_async"):
                    return await llm.generate_async(input_text, signature)
                else:
                    loop = asyncio.get_event_loop()
                    return await loop.run_in_executor(
                        None, llm.generate, input_text, signature
                    )

            elif llm_name == "mistral":
                if hasattr(llm, "generate_async"):
                    return await llm.generate_async(input_text, signature)
                else:
                    loop = asyncio.get_event_loop()
                    return await loop.run_in_executor(
                        None, llm.generate, input_text, signature
                    )

            elif llm_name == "claude":
                # Claude í•¸ë“¤ëŸ¬ í˜¸ì¶œ
                if hasattr(llm, "generate_async"):
                    result = await llm.generate_async(input_text, signature)
                else:
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(
                        None, llm.fallback_request, input_text, signature
                    )

                return {
                    "status": "success",
                    "response": result,
                    "model": "claude",
                    "signature": signature,
                    "tokens": len(str(result).split()),
                }

            elif llm_name == "echo_native":
                # Echo Native í˜¸ì¶œ
                if hasattr(llm, "pure_reasoning"):
                    if asyncio.iscoroutinefunction(llm.pure_reasoning):
                        result = await llm.pure_reasoning(input_text, signature)
                    else:
                        loop = asyncio.get_event_loop()
                        result = await loop.run_in_executor(
                            None, llm.pure_reasoning, input_text, signature
                        )
                else:
                    result = llm.generate(input_text, signature)

                return {
                    "status": "success",
                    "response": (
                        result["response"] if isinstance(result, dict) else str(result)
                    ),
                    "model": "echo_native",
                    "signature": signature,
                    "tokens": len(str(result).split()),
                }

        except Exception as e:
            logger.error(f"âŒ {llm_name} í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {"status": "error", "error": str(e)}

        return None

    def _update_performance(self, llm_name: str, response_time: float, success: bool):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        stats = self.performance_stats[llm_name]
        stats["total_requests"] += 1

        if success:
            stats["successful_requests"] += 1
            stats["consecutive_failures"] = 0
            stats["last_success"] = time.time()

            # í‰ê·  ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸
            if stats["avg_response_time"] == 0:
                stats["avg_response_time"] = response_time
            else:
                stats["avg_response_time"] = (
                    stats["avg_response_time"] + response_time
                ) / 2
        else:
            stats["failed_requests"] += 1
            stats["consecutive_failures"] += 1

    def _create_emergency_response(
        self, input_text: str, signature: str, attempts: List[str], response_time: float
    ) -> LLMResponse:
        """ë¹„ìƒ ì‘ë‹µ ìƒì„±"""
        emergency_message = f"[{signature}] í˜„ì¬ ëª¨ë“  LLM ì‹œìŠ¤í…œì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

        return LLMResponse(
            status="error",
            response=emergency_message,
            llm_used="emergency",
            model="emergency",
            signature=signature,
            response_time=response_time,
            tokens=len(emergency_message.split()),
            all_attempts=attempts,
            error_message="All LLMs failed",
        )

    async def batch_route(self, requests: List[Dict[str, str]]) -> List[LLMResponse]:
        """ë°°ì¹˜ ì²˜ë¦¬"""
        tasks = [
            self.route(req.get("input_text", ""), req.get("signature", "Aurora"))
            for req in requests
        ]
        return await asyncio.gather(*tasks)

    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
        return {
            "llm_providers": {
                name: {
                    "available": llm is not None,
                    "performance": self.performance_stats.get(name, {}),
                }
                for name, llm in self.llms.items()
            },
            "config": {
                "signature_preferences": self.config.get(
                    "signature_llm_preference", {}
                ),
                "performance_config": self.config.get("performance_config", {}),
            },
            "total_requests": sum(
                stats["total_requests"] for stats in self.performance_stats.values()
            ),
        }

    def reset_performance_stats(self):
        """ì„±ëŠ¥ í†µê³„ ë¦¬ì…‹"""
        self._initialize_performance_tracking()
        logger.info("ğŸ“Š ì„±ëŠ¥ í†µê³„ ë¦¬ì…‹ ì™„ë£Œ")


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_hybrid_llm_bridge = None


def get_hybrid_llm_bridge(config_path: Optional[str] = None) -> HybridLLMBridge:
    """í•˜ì´ë¸Œë¦¬ë“œ LLM ë¸Œë¦¬ì§€ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤"""
    global _hybrid_llm_bridge
    if _hybrid_llm_bridge is None:
        _hybrid_llm_bridge = HybridLLMBridge(config_path)
    return _hybrid_llm_bridge


# í¸ì˜ í•¨ìˆ˜ë“¤
async def echo_hybrid_chat(message: str, signature: str = "Aurora") -> str:
    """ê°„í¸í•œ í•˜ì´ë¸Œë¦¬ë“œ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤"""
    bridge = get_hybrid_llm_bridge()
    response = await bridge.route(message, signature)
    return response.response


async def check_all_llms() -> Dict[str, bool]:
    """ëª¨ë“  LLM ê°€ìš©ì„± í™•ì¸"""
    bridge = get_hybrid_llm_bridge()
    status = {}
    for llm_name in bridge.llms.keys():
        status[llm_name] = await bridge.check_llm_availability(llm_name)
    return status


if __name__ == "__main__":

    async def test_hybrid_bridge():
        """í•˜ì´ë¸Œë¦¬ë“œ ë¸Œë¦¬ì§€ í…ŒìŠ¤íŠ¸"""
        print("ğŸ”€ Echo Hybrid LLM Bridge í…ŒìŠ¤íŠ¸")
        print("=" * 60)

        bridge = get_hybrid_llm_bridge()

        # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        print("ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ:")
        status = bridge.get_system_status()
        for llm_name, info in status["llm_providers"].items():
            print(f"   {llm_name}: {'âœ…' if info['available'] else 'âŒ'}")

        # LLM ê°€ìš©ì„± í™•ì¸
        print("\nğŸ” LLM ê°€ìš©ì„± í™•ì¸:")
        availability = await check_all_llms()
        for llm_name, available in availability.items():
            print(f"   {llm_name}: {'âœ…' if available else 'âŒ'}")

        # ê° ì‹œê·¸ë‹ˆì²˜ë³„ í…ŒìŠ¤íŠ¸
        test_prompt = "Echo í•˜ì´ë¸Œë¦¬ë“œ LLM ì‹œìŠ¤í…œì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."

        for signature in ["Aurora", "Phoenix", "Sage", "Companion"]:
            print(f"\nğŸ­ {signature} í…ŒìŠ¤íŠ¸:")

            try:
                response = await bridge.route(test_prompt, signature)

                print(f"   ìƒíƒœ: {response.status}")
                print(f"   ì‚¬ìš©ëœ LLM: {response.llm_used}")
                print(f"   ëª¨ë¸: {response.model}")
                print(f"   ì‘ë‹µ ì‹œê°„: {response.response_time:.2f}ì´ˆ")
                print(f"   ì‹œë„ëœ LLM: {' â†’ '.join(response.all_attempts)}")
                print(f"   ì‘ë‹µ: {response.response[:100]}...")

            except Exception as e:
                print(f"   âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
        print(f"\nğŸ“ˆ ì„±ëŠ¥ í†µê³„:")
        final_status = bridge.get_system_status()
        for llm_name, info in final_status["llm_providers"].items():
            perf = info["performance"]
            if perf.get("total_requests", 0) > 0:
                success_rate = (
                    perf["successful_requests"] / perf["total_requests"] * 100
                )
                print(
                    f"   {llm_name}: {success_rate:.1f}% ì„±ê³µë¥ , {perf['avg_response_time']:.2f}ì´ˆ í‰ê·  ì‘ë‹µì‹œê°„"
                )

    asyncio.run(test_hybrid_bridge())

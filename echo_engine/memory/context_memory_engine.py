#!/usr/bin/env python3
"""
ğŸŒŠ Context Memory Engine v1.0
ëŒ€í™” íˆìŠ¤í† ë¦¬ ì••ì¶• ë° ê°ì • ë³€í™” íë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ìƒí™© ì¸ì‹ ì‹œìŠ¤í…œ

Phase 1: LLM-Free íŒë‹¨ ì‹œìŠ¤í…œ í•µì‹¬ ëª¨ë“ˆ
- ëŒ€í™” ë§¥ë½ ë²¡í„° ìƒì„± ë° ì••ì¶•
- ê°ì • ë³€í™” ê¶¤ì  ì¶”ì  ë° ì˜ˆì¸¡
- ìƒí™©ë³„ ì»¨í…ìŠ¤íŠ¸ ì˜ì¡´ì„± ëª¨ë¸ë§
- "ë””ì§€í„¸ ê³µê° ì˜ˆìˆ ê°€"ë¥¼ ìœ„í•œ ì™„ì „í•œ ë§¥ë½ ì´í•´

ì°¸ì¡°: LLM-Free íŒë‹¨ ì‹œìŠ¤í…œ ì™„ì„±ë„ ê·¹ëŒ€í™” ê°€ì´ë“œ
- ë…ë¦½ì  ë¬¸ì¥ ì²˜ë¦¬ë¥¼ ë„˜ì–´ì„  ëŒ€í™” íë¦„ê³¼ ìƒí™© ë§¥ë½ ì™„ì „ ì´í•´
- ëŒ€í™” íˆìŠ¤í† ë¦¬ ì••ì¶•ì„ í†µí•œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ ë° ì¥ê¸° ê¸°ì–µ êµ¬ì¶•
- ì‹œê°„, ìš”ì¼, ê³„ì ˆ ë“± ì™¸ë¶€ ìš”ì¸ì„ ê³ ë ¤í•œ ìƒí™© ì¸ì‹
"""

import os
import json
import time
import math
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import statistics


@dataclass
class ConversationSnapshot:
    """ëŒ€í™” ìŠ¤ëƒ…ìƒ· ë°ì´í„° í´ë˜ìŠ¤"""

    session_id: str
    timestamp: str
    emotional_trajectory: List[float]  # ê°ì • ë³€í™” ê¶¤ì  (ìµœê·¼ 10ê°œ)
    topic_persistence: Dict[str, float]  # ì£¼ì œë³„ ì§€ì†ì„± ì ìˆ˜
    interaction_depth: float  # ëŒ€í™” ê¹Šì´ (0.0 ~ 1.0)
    urgency_level: float  # ê¸´ê¸‰ë„ (0.0 ~ 1.0)
    social_context: str  # ì‚¬íšŒì  ë§¥ë½ (private/semi-private/public)
    temporal_context: Dict[str, Any]  # ì‹œê°„ì  ë§¥ë½
    coherence_score: float  # ëŒ€í™” ì¼ê´€ì„± ì ìˆ˜


@dataclass
class TopicEvolution:
    """ì£¼ì œ ì§„í™” ì¶”ì """

    topic_name: str
    emergence_time: str
    peak_intensity: float
    current_intensity: float
    related_emotions: List[str]
    keyword_cluster: List[str]
    transition_triggers: List[str]


class ContextMemoryEngine:
    """ëŒ€í™” ë§¥ë½ ë° ìƒí™© ì¸ì‹ì„ ìœ„í•œ ê³ ë„í™” ë©”ëª¨ë¦¬ ì—”ì§„"""

    def __init__(self, data_dir: str = "data/context_memory"):
        """ì´ˆê¸°í™”"""
        self.version = "1.0.0"
        self.data_dir = data_dir
        self.session_cache = {}
        self.active_sessions = {}
        self.analysis_count = 0

        # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.data_dir, exist_ok=True)

        # ì£¼ì œ ë¶„ë¥˜ í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°
        self.topic_clusters = {
            "work_career": {
                "keywords": [
                    "ì¼",
                    "ì§ì¥",
                    "ì—…ë¬´",
                    "íšŒì‚¬",
                    "ë™ë£Œ",
                    "ìƒì‚¬",
                    "í”„ë¡œì íŠ¸",
                    "ê³¼ì œ",
                    "ì¶œê·¼",
                    "í‡´ê·¼",
                    "ì•¼ê·¼",
                    "ìŠ¹ì§„",
                    "ì—°ë´‰",
                    "ì»¤ë¦¬ì–´",
                ],
                "weight": 1.0,
            },
            "relationships": {
                "keywords": [
                    "ì¹œêµ¬",
                    "ì—°ì¸",
                    "ê°€ì¡±",
                    "ë¶€ëª¨",
                    "í˜•ì œ",
                    "ìë§¤",
                    "ê´€ê³„",
                    "ì‚¬ë‘",
                    "ì´ë³„",
                    "ê²°í˜¼",
                    "ë°ì´íŠ¸",
                    "ê°ˆë“±",
                    "í™”í•´",
                ],
                "weight": 1.2,  # ê´€ê³„ ì£¼ì œëŠ” ë” ì¤‘ìš”í•˜ê²Œ ê°€ì¤‘
            },
            "health_wellness": {
                "keywords": [
                    "ê±´ê°•",
                    "ë³‘ì›",
                    "ì˜ì‚¬",
                    "ì•„í”ˆ",
                    "í”¼ê³¤",
                    "ìŠ¤íŠ¸ë ˆìŠ¤",
                    "ìš´ë™",
                    "ë‹¤ì´ì–´íŠ¸",
                    "ìˆ˜ë©´",
                    "íœ´ì‹",
                    "ì¹˜ë£Œ",
                ],
                "weight": 1.1,
            },
            "education_learning": {
                "keywords": [
                    "ê³µë¶€",
                    "í•™êµ",
                    "ì‹œí—˜",
                    "ê³¼ì œ",
                    "í•™ìŠµ",
                    "êµìœ¡",
                    "ê°•ì˜",
                    "ìˆ˜ì—…",
                    "ì„±ì ",
                    "ì¡¸ì—…",
                    "ì…í•™",
                    "í•™ì›",
                ],
                "weight": 0.9,
            },
            "hobbies_interests": {
                "keywords": [
                    "ì·¨ë¯¸",
                    "ê²Œì„",
                    "ì˜í™”",
                    "ìŒì•…",
                    "ì±…",
                    "ì—¬í–‰",
                    "ìš”ë¦¬",
                    "ìš´ë™",
                    "ì‡¼í•‘",
                    "ë“œë¼ë§ˆ",
                ],
                "weight": 0.8,
            },
            "daily_life": {
                "keywords": [
                    "í•˜ë£¨",
                    "ì¼ìƒ",
                    "ì§‘",
                    "ìƒí™œ",
                    "ë£¨í‹´",
                    "ìŠµê´€",
                    "ì‹ì‚¬",
                    "ì²­ì†Œ",
                    "ì‡¼í•‘",
                    "êµí†µ",
                ],
                "weight": 0.7,
            },
            "future_goals": {
                "keywords": [
                    "ëª©í‘œ",
                    "ê³„íš",
                    "ë¯¸ë˜",
                    "ê¿ˆ",
                    "í¬ë§",
                    "ë„ì „",
                    "ì„±ì¥",
                    "ë°œì „",
                    "ë³€í™”",
                    "ê°œì„ ",
                ],
                "weight": 1.0,
            },
            "emotional_state": {
                "keywords": [
                    "ê¸°ë¶„",
                    "ê°ì •",
                    "ë§ˆìŒ",
                    "ëŠë‚Œ",
                    "ìƒê°",
                    "ê³ ë¯¼",
                    "ê±±ì •",
                    "ë¶ˆì•ˆ",
                    "í–‰ë³µ",
                    "ìŠ¬í””",
                ],
                "weight": 1.3,  # ê°ì • ìƒíƒœëŠ” ê°€ì¥ ì¤‘ìš”
            },
        }

        # ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ íŒ¨í„´
        self.temporal_patterns = {
            "time_of_day": {
                "morning": {"start": 6, "end": 12, "mood_modifier": 0.1},
                "afternoon": {"start": 12, "end": 18, "mood_modifier": 0.0},
                "evening": {"start": 18, "end": 22, "mood_modifier": -0.1},
                "night": {"start": 22, "end": 6, "mood_modifier": -0.2},
            },
            "day_of_week": {
                "monday": {"stress_modifier": 0.3, "energy_modifier": -0.1},
                "tuesday": {"stress_modifier": 0.1, "energy_modifier": 0.0},
                "wednesday": {"stress_modifier": 0.0, "energy_modifier": 0.0},
                "thursday": {"stress_modifier": 0.1, "energy_modifier": -0.1},
                "friday": {"stress_modifier": -0.1, "energy_modifier": 0.2},
                "saturday": {"stress_modifier": -0.3, "energy_modifier": 0.1},
                "sunday": {"stress_modifier": -0.2, "energy_modifier": -0.1},
            },
        }

        print(f"ğŸŒŠ Context Memory Engine v{self.version} ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ğŸ“ ë©”ëª¨ë¦¬ ì €ì¥ ê²½ë¡œ: {self.data_dir}")

    def get_context_snapshot(self, session_id: str) -> Dict[str, Any]:
        """
        í˜„ì¬ ëŒ€í™” ì„¸ì…˜ì˜ ìš”ì•½ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜

        Args:
            session_id: ì„¸ì…˜ ì‹ë³„ì

        Returns:
            ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìŠ¤ëƒ…ìƒ·
        """
        if session_id not in self.active_sessions:
            return self._create_empty_context(session_id)

        session_data = self.active_sessions[session_id]

        # í˜„ì¬ ì‹œì ì˜ ì»¨í…ìŠ¤íŠ¸ ìŠ¤ëƒ…ìƒ· ìƒì„±
        snapshot = ConversationSnapshot(
            session_id=session_id,
            timestamp=datetime.now().isoformat(),
            emotional_trajectory=self._calculate_emotional_trajectory(session_data),
            topic_persistence=self._calculate_topic_persistence(session_data),
            interaction_depth=self._calculate_interaction_depth(session_data),
            urgency_level=self._calculate_urgency_level(session_data),
            social_context=self._infer_social_context(session_data),
            temporal_context=self._analyze_temporal_context(),
            coherence_score=self._calculate_coherence_score(session_data),
        )

        return asdict(snapshot)

    def update_context_memory(
        self,
        session_id: str,
        user_input: str,
        emotion_vector: Dict[str, Any],
        response_data: Optional[Dict] = None,
    ) -> None:
        """
        ì‹¤ì‹œê°„ìœ¼ë¡œ ê°ì • ë³€í™” ë° ì£¼ì œ íë¦„ ê¸°ë¡

        Args:
            session_id: ì„¸ì…˜ ì‹ë³„ì
            user_input: ì‚¬ìš©ì ì…ë ¥
            emotion_vector: ê°ì • ë²¡í„° ë°ì´í„°
            response_data: ì‘ë‹µ ê´€ë ¨ ë°ì´í„° (ì„ íƒì )
        """
        self.analysis_count += 1

        # ì„¸ì…˜ ì´ˆê¸°í™” (í•„ìš”ì‹œ)
        if session_id not in self.active_sessions:
            self.active_sessions[session_id] = self._initialize_session(session_id)

        session_data = self.active_sessions[session_id]

        # ìƒˆë¡œìš´ ìƒí˜¸ì‘ìš© ê¸°ë¡ ì¶”ê°€
        interaction_record = {
            "timestamp": datetime.now().isoformat(),
            "user_input": user_input,
            "input_length": len(user_input),
            "word_count": len(user_input.split()),
            "emotion_vector": emotion_vector,
            "detected_topics": self._extract_topics(user_input),
            "urgency_indicators": self._detect_urgency_indicators(user_input),
            "coherence_links": self._analyze_coherence_links(session_data, user_input),
            "temporal_markers": self._extract_temporal_markers(user_input),
            "response_data": response_data or {},
        }

        session_data["interactions"].append(interaction_record)
        session_data["last_updated"] = datetime.now().isoformat()
        session_data["total_interactions"] += 1

        # ìƒí˜¸ì‘ìš© íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        if len(session_data["interactions"]) > 50:
            # ì˜¤ë˜ëœ ìƒí˜¸ì‘ìš©ì€ ì••ì¶•í•˜ì—¬ ì €ì¥
            self._compress_old_interactions(session_data)

        # ì£¼ì œ ì§„í™” ì¶”ì  ì—…ë°ì´íŠ¸
        self._update_topic_evolution(session_data, interaction_record)

        # ê°ì • ê¶¤ì  ì—…ë°ì´íŠ¸
        self._update_emotional_trajectory(session_data, emotion_vector)

        # ëŒ€í™” íŒ¨í„´ í•™ìŠµ
        self._learn_conversation_patterns(session_data, interaction_record)

        # ì£¼ê¸°ì ìœ¼ë¡œ ë””ìŠ¤í¬ì— ì €ì¥
        if self.analysis_count % 10 == 0:
            self._save_session_data(session_id, session_data)

    def _create_empty_context(self, session_id: str) -> Dict[str, Any]:
        """ë¹ˆ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        return {
            "session_id": session_id,
            "timestamp": datetime.now().isoformat(),
            "emotional_trajectory": [],
            "topic_persistence": {},
            "interaction_depth": 0.0,
            "urgency_level": 0.0,
            "social_context": "private",
            "temporal_context": self._analyze_temporal_context(),
            "coherence_score": 0.0,
        }

    def _initialize_session(self, session_id: str) -> Dict[str, Any]:
        """ìƒˆ ì„¸ì…˜ ì´ˆê¸°í™”"""
        return {
            "session_id": session_id,
            "created_at": datetime.now().isoformat(),
            "last_updated": datetime.now().isoformat(),
            "total_interactions": 0,
            "interactions": [],
            "topic_evolution": {},
            "emotional_trajectory": deque(maxlen=20),  # ìµœê·¼ 20ê°œ ê°ì •ë§Œ ìœ ì§€
            "conversation_patterns": {
                "avg_response_length": 0.0,
                "topic_switching_frequency": 0.0,
                "emotional_volatility": 0.0,
                "coherence_trend": [],
            },
            "compressed_history": [],  # ì••ì¶•ëœ ê³¼ê±° ê¸°ë¡
        }

    def _extract_topics(self, user_input: str) -> Dict[str, float]:
        """ì‚¬ìš©ì ì…ë ¥ì—ì„œ ì£¼ì œ ì¶”ì¶œ ë° ì ìˆ˜ ê³„ì‚°"""
        user_input_lower = user_input.lower()
        topic_scores = {}

        for topic_name, topic_data in self.topic_clusters.items():
            score = 0.0
            matched_keywords = []

            for keyword in topic_data["keywords"]:
                if keyword in user_input_lower:
                    score += 1.0
                    matched_keywords.append(keyword)

            if score > 0:
                # ê°€ì¤‘ì¹˜ ì ìš©
                weighted_score = score * topic_data["weight"]
                # í‚¤ì›Œë“œ ë°€ë„ ê³ ë ¤
                keyword_density = score / max(len(user_input.split()), 1)
                final_score = weighted_score * (1 + keyword_density)

                topic_scores[topic_name] = {
                    "score": final_score,
                    "matched_keywords": matched_keywords,
                    "keyword_count": len(matched_keywords),
                }

        return topic_scores

    def _detect_urgency_indicators(self, user_input: str) -> Dict[str, Any]:
        """ê¸´ê¸‰ë„ ì§€ì‹œì–´ ê°ì§€"""
        urgency_patterns = {
            "high": [
                "ê¸‰í•´",
                "ë¹¨ë¦¬",
                "ì¦‰ì‹œ",
                "ë‹¹ì¥",
                "ì§€ê¸ˆ",
                "ê¸´ê¸‰",
                "ì‘ê¸‰",
                "ìœ„ê¸‰",
                "ì‹¬ê°",
                "ìœ„í—˜",
            ],
            "medium": ["ë¹ ë¥¸", "ì„œë‘˜ëŸ¬", "ì‹ ì†", "ì¡°ê¸ˆ ê¸‰", "ì‹œê°„ì´ ì—†ì–´", "ëŠ¦ì—ˆì–´"],
            "time_pressure": ["ë§ˆê°", "ì‹œê°„", "ëŠ¦", "ë¶€ì¡±", "ì´‰ë°•", "ì••ë°•"],
        }

        urgency_score = 0.0
        detected_indicators = []

        user_input_lower = user_input.lower()

        for level, patterns in urgency_patterns.items():
            matches = [pattern for pattern in patterns if pattern in user_input_lower]
            if matches:
                if level == "high":
                    urgency_score += 0.8
                elif level == "medium":
                    urgency_score += 0.5
                elif level == "time_pressure":
                    urgency_score += 0.3

                detected_indicators.extend(matches)

        # ë¬¸ì¥ êµ¬ì¡° ê¸°ë°˜ ê¸´ê¸‰ë„ ì¶”ê°€ íŒë‹¨
        if "!!" in user_input:
            urgency_score += 0.2
        if "?" in user_input and ("ì–´ë–»ê²Œ" in user_input or "ë­˜" in user_input):
            urgency_score += 0.1

        return {
            "urgency_score": min(urgency_score, 1.0),
            "indicators": detected_indicators,
            "has_time_pressure": any(
                "ì‹œê°„" in indicator for indicator in detected_indicators
            ),
        }

    def _analyze_coherence_links(
        self, session_data: Dict, current_input: str
    ) -> Dict[str, Any]:
        """ëŒ€í™” ì¼ê´€ì„± ì—°ê²°ê³ ë¦¬ ë¶„ì„"""
        if not session_data["interactions"]:
            return {"coherence_score": 0.5, "links": []}

        last_interaction = session_data["interactions"][-1]
        coherence_links = []
        coherence_score = 0.0

        # ì£¼ì œ ì—°ì†ì„± ì²´í¬
        current_topics = self._extract_topics(current_input)
        last_topics = last_interaction.get("detected_topics", {})

        shared_topics = set(current_topics.keys()) & set(last_topics.keys())
        if shared_topics:
            coherence_score += 0.4
            coherence_links.append(f"ì£¼ì œ ì—°ì†ì„±: {', '.join(shared_topics)}")

        # í‚¤ì›Œë“œ ì—°ê²°ì„± ì²´í¬
        current_words = set(current_input.lower().split())
        last_words = set(last_interaction["user_input"].lower().split())

        shared_words = current_words & last_words
        if len(shared_words) > 1:  # ë¶ˆìš©ì–´ ì œì™¸í•˜ê³  ì‹¤ì§ˆì  ê³µìœ  ë‹¨ì–´
            coherence_score += 0.2
            coherence_links.append(f"í‚¤ì›Œë“œ ì—°ê²°: {len(shared_words)}ê°œ ê³µìœ ")

        # ì‹œê°„ì  ì—°ê²°ì„± ì²´í¬
        time_diff = self._calculate_time_difference(
            last_interaction["timestamp"], datetime.now().isoformat()
        )

        if time_diff < 300:  # 5ë¶„ ì´ë‚´
            coherence_score += 0.2
        elif time_diff < 1800:  # 30ë¶„ ì´ë‚´
            coherence_score += 0.1

        # ê°ì •ì  ì—°ê²°ì„± ì²´í¬
        last_emotion = last_interaction.get("emotion_vector", {}).get("primary", {})
        if last_emotion:
            coherence_links.append("ê°ì •ì  ì—°ê²°ì„± ê³ ë ¤")
            coherence_score += 0.1

        return {
            "coherence_score": min(coherence_score, 1.0),
            "links": coherence_links,
            "time_gap": time_diff,
        }

    def _extract_temporal_markers(self, user_input: str) -> Dict[str, List[str]]:
        """ì‹œê°„ í‘œì§€ ì¶”ì¶œ"""
        temporal_markers = {"past": [], "present": [], "future": []}

        past_patterns = ["ì–´ì œ", "ì§€ë‚œ", "ì˜ˆì „", "ê³¼ê±°", "ì „ì—", "í–ˆì—ˆ", "ì•˜ì—ˆ", "ì˜€ë˜"]
        present_patterns = ["ì§€ê¸ˆ", "ì˜¤ëŠ˜", "í˜„ì¬", "ìš”ì¦˜", "ì´ì œ", "í•˜ê³ ìˆ", "ë˜ê³ ìˆ"]
        future_patterns = ["ë‚´ì¼", "ë‹¤ìŒ", "ì•ìœ¼ë¡œ", "ë¯¸ë˜", "ë ", "í• ", "ì˜ˆì •", "ê³„íš"]

        user_input_lower = user_input.lower()

        for pattern in past_patterns:
            if pattern in user_input_lower:
                temporal_markers["past"].append(pattern)

        for pattern in present_patterns:
            if pattern in user_input_lower:
                temporal_markers["present"].append(pattern)

        for pattern in future_patterns:
            if pattern in user_input_lower:
                temporal_markers["future"].append(pattern)

        return temporal_markers

    def _calculate_emotional_trajectory(self, session_data: Dict) -> List[float]:
        """ê°ì • ë³€í™” ê¶¤ì  ê³„ì‚°"""
        trajectory = []

        for interaction in session_data["interactions"][-10:]:  # ìµœê·¼ 10ê°œ
            emotion_vector = interaction.get("emotion_vector", {})

            if "intensity" in emotion_vector:
                intensity = emotion_vector["intensity"]
            else:
                # primary ê°ì •ì—ì„œ ê°•ë„ ì¶”ì •
                primary = emotion_vector.get("primary", {})
                if primary:
                    intensity = max(primary.values()) if primary else 0.5
                else:
                    intensity = 0.5

            trajectory.append(intensity)

        return trajectory

    def _calculate_topic_persistence(self, session_data: Dict) -> Dict[str, float]:
        """ì£¼ì œë³„ ì§€ì†ì„± ì ìˆ˜ ê³„ì‚°"""
        topic_persistence = defaultdict(float)
        total_interactions = len(session_data["interactions"])

        if total_interactions == 0:
            return {}

        # ìµœê·¼ ìƒí˜¸ì‘ìš©ë“¤ì—ì„œ ì£¼ì œë³„ ì¶œí˜„ ë¹ˆë„ ê³„ì‚°
        for interaction in session_data["interactions"]:
            detected_topics = interaction.get("detected_topics", {})

            for topic, topic_data in detected_topics.items():
                score = (
                    topic_data.get("score", 0)
                    if isinstance(topic_data, dict)
                    else topic_data
                )
                topic_persistence[topic] += score

        # ì •ê·œí™”
        max_score = max(topic_persistence.values()) if topic_persistence else 1.0
        normalized_persistence = {
            topic: score / max_score for topic, score in topic_persistence.items()
        }

        return normalized_persistence

    def _calculate_interaction_depth(self, session_data: Dict) -> float:
        """ëŒ€í™” ê¹Šì´ ê³„ì‚°"""
        interactions = session_data["interactions"]

        if not interactions:
            return 0.0

        depth_factors = []

        for interaction in interactions:
            # ì…ë ¥ ê¸¸ì´ ê¸°ë°˜ ê¹Šì´
            input_length = interaction.get("input_length", 0)
            length_depth = min(input_length / 100, 1.0)  # 100ì ê¸°ì¤€ ì •ê·œí™”

            # ê°ì • ê°•ë„ ê¸°ë°˜ ê¹Šì´
            emotion_vector = interaction.get("emotion_vector", {})
            emotion_depth = emotion_vector.get("intensity", 0.0)

            # ì£¼ì œ ë³µì¡ë„ ê¸°ë°˜ ê¹Šì´
            detected_topics = interaction.get("detected_topics", {})
            topic_depth = min(len(detected_topics) / 3, 1.0)  # 3ê°œ ì£¼ì œ ê¸°ì¤€

            # ì¢…í•© ê¹Šì´ ì ìˆ˜
            interaction_depth = (
                length_depth * 0.3 + emotion_depth * 0.5 + topic_depth * 0.2
            )
            depth_factors.append(interaction_depth)

        # ìµœê·¼ ìƒí˜¸ì‘ìš©ì— ë” í° ê°€ì¤‘ì¹˜
        if len(depth_factors) > 1:
            weights = [i / len(depth_factors) for i in range(1, len(depth_factors) + 1)]
            weighted_depth = sum(d * w for d, w in zip(depth_factors, weights)) / sum(
                weights
            )
        else:
            weighted_depth = depth_factors[0] if depth_factors else 0.0

        return weighted_depth

    def _calculate_urgency_level(self, session_data: Dict) -> float:
        """ê¸´ê¸‰ë„ ë ˆë²¨ ê³„ì‚°"""
        if not session_data["interactions"]:
            return 0.0

        urgency_scores = []

        for interaction in session_data["interactions"][-5:]:  # ìµœê·¼ 5ê°œ
            urgency_data = interaction.get("urgency_indicators", {})
            urgency_score = urgency_data.get("urgency_score", 0.0)
            urgency_scores.append(urgency_score)

        # ìµœê·¼ ê¸´ê¸‰ë„ì˜ ê°€ì¤‘ í‰ê· 
        if urgency_scores:
            weights = [
                i / len(urgency_scores) for i in range(1, len(urgency_scores) + 1)
            ]
            weighted_urgency = sum(
                s * w for s, w in zip(urgency_scores, weights)
            ) / sum(weights)
            return weighted_urgency

        return 0.0

    def _infer_social_context(self, session_data: Dict) -> str:
        """ì‚¬íšŒì  ë§¥ë½ ì¶”ë¡ """
        # ê°„ë‹¨í•œ ì¶”ë¡  ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¶„ì„ í•„ìš”)

        # ê°œì¸ì  ì£¼ì œê°€ ë§ìœ¼ë©´ private
        personal_topics = ["relationships", "emotional_state", "health_wellness"]
        recent_topics = []

        for interaction in session_data["interactions"][-3:]:
            detected_topics = interaction.get("detected_topics", {})
            recent_topics.extend(detected_topics.keys())

        personal_ratio = sum(
            1 for topic in recent_topics if topic in personal_topics
        ) / max(len(recent_topics), 1)

        if personal_ratio > 0.6:
            return "private"
        elif personal_ratio > 0.3:
            return "semi-private"
        else:
            return "public"

    def _analyze_temporal_context(self) -> Dict[str, Any]:
        """í˜„ì¬ ì‹œê°„ì  ë§¥ë½ ë¶„ì„"""
        now = datetime.now()

        # ì‹œê°„ëŒ€ ê²°ì •
        hour = now.hour
        if 6 <= hour < 12:
            time_period = "morning"
        elif 12 <= hour < 18:
            time_period = "afternoon"
        elif 18 <= hour < 22:
            time_period = "evening"
        else:
            time_period = "night"

        # ìš”ì¼
        day_names = [
            "monday",
            "tuesday",
            "wednesday",
            "thursday",
            "friday",
            "saturday",
            "sunday",
        ]
        day_of_week = day_names[now.weekday()]

        # ì‹œê°„ì  ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ìˆ˜ì •ì
        time_modifiers = self.temporal_patterns["time_of_day"].get(time_period, {})
        day_modifiers = self.temporal_patterns["day_of_week"].get(day_of_week, {})

        return {
            "timestamp": now.isoformat(),
            "time_period": time_period,
            "day_of_week": day_of_week,
            "hour": hour,
            "modifiers": {
                "mood_modifier": time_modifiers.get("mood_modifier", 0.0),
                "stress_modifier": day_modifiers.get("stress_modifier", 0.0),
                "energy_modifier": day_modifiers.get("energy_modifier", 0.0),
            },
        }

    def _calculate_coherence_score(self, session_data: Dict) -> float:
        """ëŒ€í™” ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        interactions = session_data["interactions"]

        if len(interactions) < 2:
            return 0.5  # ê¸°ë³¸ê°’

        coherence_scores = []

        for i in range(1, len(interactions)):
            current = interactions[i]
            previous = interactions[i - 1]

            coherence_links = current.get("coherence_links", {})
            score = coherence_links.get("coherence_score", 0.5)
            coherence_scores.append(score)

        return statistics.mean(coherence_scores) if coherence_scores else 0.5

    def _update_topic_evolution(
        self, session_data: Dict, interaction_record: Dict
    ) -> None:
        """ì£¼ì œ ì§„í™” ì¶”ì  ì—…ë°ì´íŠ¸"""
        detected_topics = interaction_record.get("detected_topics", {})
        timestamp = interaction_record["timestamp"]

        for topic_name, topic_data in detected_topics.items():
            score = (
                topic_data.get("score", 0)
                if isinstance(topic_data, dict)
                else topic_data
            )

            if topic_name not in session_data["topic_evolution"]:
                # ìƒˆ ì£¼ì œ ë“±ì¥
                session_data["topic_evolution"][topic_name] = {
                    "emergence_time": timestamp,
                    "peak_intensity": score,
                    "current_intensity": score,
                    "intensity_history": [score],
                    "last_mentioned": timestamp,
                }
            else:
                # ê¸°ì¡´ ì£¼ì œ ì—…ë°ì´íŠ¸
                topic_evo = session_data["topic_evolution"][topic_name]
                topic_evo["current_intensity"] = score
                topic_evo["last_mentioned"] = timestamp
                topic_evo["intensity_history"].append(score)

                # ìµœëŒ€ ê°•ë„ ì—…ë°ì´íŠ¸
                if score > topic_evo["peak_intensity"]:
                    topic_evo["peak_intensity"] = score

                # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
                if len(topic_evo["intensity_history"]) > 20:
                    topic_evo["intensity_history"] = topic_evo["intensity_history"][
                        -20:
                    ]

    def _update_emotional_trajectory(
        self, session_data: Dict, emotion_vector: Dict
    ) -> None:
        """ê°ì • ê¶¤ì  ì—…ë°ì´íŠ¸"""
        intensity = emotion_vector.get("intensity", 0.5)
        session_data["emotional_trajectory"].append(
            {
                "timestamp": datetime.now().isoformat(),
                "intensity": intensity,
                "primary_emotion": emotion_vector.get("primary", {}),
                "stability": emotion_vector.get("stability", 0.5),
            }
        )

    def _learn_conversation_patterns(
        self, session_data: Dict, interaction_record: Dict
    ) -> None:
        """ëŒ€í™” íŒ¨í„´ í•™ìŠµ"""
        patterns = session_data["conversation_patterns"]

        # í‰ê·  ì‘ë‹µ ê¸¸ì´ ì—…ë°ì´íŠ¸
        current_length = interaction_record["input_length"]
        total_interactions = session_data["total_interactions"]

        if total_interactions > 1:
            patterns["avg_response_length"] = (
                patterns["avg_response_length"] * (total_interactions - 1)
                + current_length
            ) / total_interactions
        else:
            patterns["avg_response_length"] = current_length

        # ê°ì • ë³€ë™ì„± ê³„ì‚°
        if len(session_data["emotional_trajectory"]) > 1:
            intensities = [
                entry["intensity"] for entry in session_data["emotional_trajectory"]
            ]
            if len(intensities) > 1:
                patterns["emotional_volatility"] = statistics.stdev(intensities)

        # ì¼ê´€ì„± íŠ¸ë Œë“œ ì¶”ê°€
        coherence_score = interaction_record.get("coherence_links", {}).get(
            "coherence_score", 0.5
        )
        patterns["coherence_trend"].append(coherence_score)

        # íŠ¸ë Œë“œ í¬ê¸° ì œí•œ
        if len(patterns["coherence_trend"]) > 20:
            patterns["coherence_trend"] = patterns["coherence_trend"][-20:]

    def _compress_old_interactions(self, session_data: Dict) -> None:
        """ì˜¤ë˜ëœ ìƒí˜¸ì‘ìš© ì••ì¶•"""
        interactions = session_data["interactions"]

        # ì˜¤ë˜ëœ 10ê°œ ìƒí˜¸ì‘ìš©ì„ ì••ì¶•
        old_interactions = interactions[:10]

        # ì••ì¶• ìš”ì•½ ìƒì„±
        compression_summary = {
            "period_start": old_interactions[0]["timestamp"],
            "period_end": old_interactions[-1]["timestamp"],
            "total_interactions": len(old_interactions),
            "dominant_topics": self._get_dominant_topics(old_interactions),
            "avg_emotion_intensity": self._calculate_avg_emotion_intensity(
                old_interactions
            ),
            "key_patterns": self._extract_key_patterns(old_interactions),
        }

        session_data["compressed_history"].append(compression_summary)
        session_data["interactions"] = interactions[10:]  # ì˜¤ë˜ëœ ê²ƒë“¤ ì œê±°

    def _calculate_time_difference(self, time1: str, time2: str) -> float:
        """ë‘ ì‹œê°„ ê°„ì˜ ì°¨ì´ ê³„ì‚° (ì´ˆ ë‹¨ìœ„)"""
        try:
            dt1 = datetime.fromisoformat(time1.replace("Z", "+00:00"))
            dt2 = datetime.fromisoformat(time2.replace("Z", "+00:00"))
            return abs((dt2 - dt1).total_seconds())
        except:
            return 0.0

    def _get_dominant_topics(self, interactions: List[Dict]) -> List[str]:
        """ì§€ë°°ì  ì£¼ì œë“¤ ì¶”ì¶œ"""
        topic_scores = defaultdict(float)

        for interaction in interactions:
            detected_topics = interaction.get("detected_topics", {})
            for topic, data in detected_topics.items():
                score = data.get("score", 0) if isinstance(data, dict) else data
                topic_scores[topic] += score

        # ìƒìœ„ 3ê°œ ì£¼ì œ ë°˜í™˜
        sorted_topics = sorted(topic_scores.items(), key=lambda x: x[1], reverse=True)
        return [topic for topic, score in sorted_topics[:3]]

    def _calculate_avg_emotion_intensity(self, interactions: List[Dict]) -> float:
        """í‰ê·  ê°ì • ê°•ë„ ê³„ì‚°"""
        intensities = []

        for interaction in interactions:
            emotion_vector = interaction.get("emotion_vector", {})
            intensity = emotion_vector.get("intensity", 0.5)
            intensities.append(intensity)

        return statistics.mean(intensities) if intensities else 0.5

    def _extract_key_patterns(self, interactions: List[Dict]) -> Dict[str, Any]:
        """í•µì‹¬ íŒ¨í„´ ì¶”ì¶œ"""
        return {
            "avg_input_length": statistics.mean(
                [i["input_length"] for i in interactions]
            ),
            "topic_diversity": len(
                set().union(
                    *[i.get("detected_topics", {}).keys() for i in interactions]
                )
            ),
            "urgency_episodes": sum(
                1
                for i in interactions
                if i.get("urgency_indicators", {}).get("urgency_score", 0) > 0.5
            ),
        }

    def _save_session_data(self, session_id: str, session_data: Dict) -> None:
        """ì„¸ì…˜ ë°ì´í„° ì €ì¥"""
        session_path = os.path.join(self.data_dir, f"{session_id}_context.json")

        try:
            # deque ê°ì²´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
            save_data = dict(session_data)
            save_data["emotional_trajectory"] = list(save_data["emotional_trajectory"])

            with open(session_path, "w", encoding="utf-8") as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ ì„¸ì…˜ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨ ({session_id}): {e}")

    def get_session_insights(self, session_id: str) -> Dict[str, Any]:
        """ì„¸ì…˜ ì¸ì‚¬ì´íŠ¸ ë¶„ì„"""
        if session_id not in self.active_sessions:
            return {"error": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

        session_data = self.active_sessions[session_id]

        insights = {
            "session_summary": {
                "total_interactions": session_data["total_interactions"],
                "session_duration": self._calculate_session_duration(session_data),
                "dominant_topics": self._get_dominant_topics(
                    session_data["interactions"]
                ),
                "avg_emotional_intensity": self._calculate_avg_emotion_intensity(
                    session_data["interactions"]
                ),
            },
            "conversation_flow": {
                "coherence_trend": session_data["conversation_patterns"][
                    "coherence_trend"
                ][-10:],
                "emotional_volatility": session_data["conversation_patterns"][
                    "emotional_volatility"
                ],
                "topic_switching_pattern": self._analyze_topic_switching(session_data),
            },
            "context_factors": {
                "current_urgency": self._calculate_urgency_level(session_data),
                "interaction_depth": self._calculate_interaction_depth(session_data),
                "social_context": self._infer_social_context(session_data),
            },
        }

        return insights

    def _calculate_session_duration(self, session_data: Dict) -> str:
        """ì„¸ì…˜ ì§€ì† ì‹œê°„ ê³„ì‚°"""
        if not session_data["interactions"]:
            return "0ë¶„"

        start_time = datetime.fromisoformat(session_data["created_at"])
        end_time = datetime.fromisoformat(session_data["last_updated"])

        duration = end_time - start_time
        minutes = int(duration.total_seconds() / 60)

        return f"{minutes}ë¶„"

    def _analyze_topic_switching(self, session_data: Dict) -> Dict[str, Any]:
        """ì£¼ì œ ì „í™˜ íŒ¨í„´ ë¶„ì„"""
        interactions = session_data["interactions"]

        if len(interactions) < 2:
            return {"switches": 0, "pattern": "insufficient_data"}

        topic_switches = 0
        previous_topics = set()

        for interaction in interactions:
            current_topics = set(interaction.get("detected_topics", {}).keys())

            if previous_topics and not (current_topics & previous_topics):
                topic_switches += 1

            previous_topics = current_topics

        switch_rate = topic_switches / max(len(interactions) - 1, 1)

        if switch_rate > 0.5:
            pattern = "high_switching"
        elif switch_rate > 0.2:
            pattern = "moderate_switching"
        else:
            pattern = "focused_conversation"

        return {
            "switches": topic_switches,
            "switch_rate": switch_rate,
            "pattern": pattern,
        }


def test_context_memory_engine():
    """ì»¨í…ìŠ¤íŠ¸ ë©”ëª¨ë¦¬ ì—”ì§„ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Context Memory Engine í…ŒìŠ¤íŠ¸ ì‹œì‘...")

    engine = ContextMemoryEngine()
    test_session_id = "test_session_001"

    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ 1: ì²« ë²ˆì§¸ ìƒí˜¸ì‘ìš©
    print("\nğŸ“ ì‹œë‚˜ë¦¬ì˜¤ 1: ì²« ë²ˆì§¸ ìƒí˜¸ì‘ìš©")
    emotion_vector_1 = {
        "primary": {"sadness": 0.7, "fear": 0.2},
        "intensity": 0.7,
        "stability": 0.4,
    }

    engine.update_context_memory(
        test_session_id,
        "ìš”ì¦˜ ì§ì¥ ì¼ì´ ë„ˆë¬´ ìŠ¤íŠ¸ë ˆìŠ¤ë°›ì•„ì„œ ì ë„ ëª» ìê³  ìˆì–´ìš”",
        emotion_vector_1,
    )

    context_1 = engine.get_context_snapshot(test_session_id)
    print(f"âœ… ì²« ìƒí˜¸ì‘ìš© í›„ ì»¨í…ìŠ¤íŠ¸ ìŠ¤ëƒ…ìƒ· ìƒì„±")
    print(f"   ì£¼ì œ: {list(context_1['topic_persistence'].keys())}")
    print(f"   ê¸´ê¸‰ë„: {context_1['urgency_level']:.3f}")

    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ 2: ì—°ê´€ëœ ì£¼ì œë¡œ ëŒ€í™” ê³„ì†
    print("\nğŸ“ ì‹œë‚˜ë¦¬ì˜¤ 2: ì—°ê´€ëœ ì£¼ì œë¡œ ëŒ€í™” ê³„ì†")
    emotion_vector_2 = {
        "primary": {"anger": 0.5, "sadness": 0.3},
        "intensity": 0.6,
        "stability": 0.3,
    }

    engine.update_context_memory(
        test_session_id,
        "ìƒì‚¬ê°€ ê³„ì† ì•¼ê·¼ì„ ì‹œí‚¤ëŠ”ë° ì •ë§ í™”ê°€ ë‚˜ìš”. ì´ëŸ¬ë‹¤ ê±´ê°•ê¹Œì§€ ë‚˜ë¹ ì§ˆ ê²ƒ ê°™ì•„ìš”",
        emotion_vector_2,
    )

    context_2 = engine.get_context_snapshot(test_session_id)
    print(f"âœ… ë‘ ë²ˆì§¸ ìƒí˜¸ì‘ìš© í›„ ì¼ê´€ì„± ì ìˆ˜: {context_2['coherence_score']:.3f}")
    print(f"   ê°ì • ê¶¤ì : {context_2['emotional_trajectory']}")

    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ 3: ì£¼ì œ ì „í™˜
    print("\nğŸ“ ì‹œë‚˜ë¦¬ì˜¤ 3: ì£¼ì œ ì „í™˜")
    emotion_vector_3 = {
        "primary": {"joy": 0.6, "surprise": 0.2},
        "intensity": 0.5,
        "stability": 0.7,
    }

    engine.update_context_memory(
        test_session_id,
        "ê·¸ëŸ°ë° ì˜¤ëŠ˜ ì¹œêµ¬ë“¤ê³¼ ì˜í™” ë³´ëŸ¬ ê°€ê¸°ë¡œ í–ˆì–´ìš”! ì •ë§ ì˜¤ëœë§Œì´ì—ìš”",
        emotion_vector_3,
    )

    context_3 = engine.get_context_snapshot(test_session_id)
    print(f"âœ… ì£¼ì œ ì „í™˜ í›„ ì£¼ì œ ì§€ì†ì„±: {context_3['topic_persistence']}")

    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ 4: ê¸´ê¸‰í•œ ìƒí™©
    print("\nğŸ“ ì‹œë‚˜ë¦¬ì˜¤ 4: ê¸´ê¸‰í•œ ìƒí™©")
    emotion_vector_4 = {
        "primary": {"fear": 0.8, "anger": 0.1},
        "intensity": 0.9,
        "stability": 0.1,
    }

    engine.update_context_memory(
        test_session_id,
        "ê¸‰í•´ìš”! ë‚´ì¼ê¹Œì§€ í”„ë¡œì íŠ¸ ë§ˆê°ì¸ë° ì•„ì§ ì ˆë°˜ë„ ëª» ëëƒˆì–´ìš”. ì–´ë–»ê²Œ í•´ì•¼ í•˜ì£ ?",
        emotion_vector_4,
    )

    context_4 = engine.get_context_snapshot(test_session_id)
    print(f"âœ… ê¸´ê¸‰ ìƒí™© ê°ì§€ - ê¸´ê¸‰ë„: {context_4['urgency_level']:.3f}")
    print(f"   ìƒí˜¸ì‘ìš© ê¹Šì´: {context_4['interaction_depth']:.3f}")

    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ 5: ì„¸ì…˜ ì¸ì‚¬ì´íŠ¸ ë¶„ì„
    print("\nğŸ“ ì‹œë‚˜ë¦¬ì˜¤ 5: ì„¸ì…˜ ì¸ì‚¬ì´íŠ¸ ë¶„ì„")
    insights = engine.get_session_insights(test_session_id)

    print(f"ğŸ“Š ì„¸ì…˜ ìš”ì•½:")
    print(f"   ì´ ìƒí˜¸ì‘ìš©: {insights['session_summary']['total_interactions']}")
    print(f"   ì§€ì† ì‹œê°„: {insights['session_summary']['session_duration']}")
    print(f"   ì§€ë°°ì  ì£¼ì œ: {insights['session_summary']['dominant_topics']}")
    print(
        f"   í‰ê·  ê°ì • ê°•ë„: {insights['session_summary']['avg_emotional_intensity']:.3f}"
    )

    print(f"\nğŸ”„ ëŒ€í™” íë¦„:")
    print(
        f"   ê°ì • ë³€ë™ì„±: {insights['conversation_flow']['emotional_volatility']:.3f}"
    )
    print(
        f"   ì£¼ì œ ì „í™˜ íŒ¨í„´: {insights['conversation_flow']['topic_switching_pattern']['pattern']}"
    )

    print(f"\nğŸŒ ì»¨í…ìŠ¤íŠ¸ ìš”ì†Œ:")
    print(f"   í˜„ì¬ ê¸´ê¸‰ë„: {insights['context_factors']['current_urgency']:.3f}")
    print(f"   ìƒí˜¸ì‘ìš© ê¹Šì´: {insights['context_factors']['interaction_depth']:.3f}")
    print(f"   ì‚¬íšŒì  ë§¥ë½: {insights['context_factors']['social_context']}")

    print("\nğŸ‰ Context Memory Engine í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    test_context_memory_engine()

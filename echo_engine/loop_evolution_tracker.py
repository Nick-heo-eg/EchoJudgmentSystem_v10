#!/usr/bin/env python3
"""
ğŸ”„ Loop Evolution Tracker v1.0
Echoì˜ íŒë‹¨ ë£¨í”„ì™€ ì²˜ë¦¬ íŒ¨í„´ì˜ ì§„í™” ê³¼ì •ì„ ì¶”ì í•˜ê³  ë¶„ì„í•˜ëŠ” ì‹œìŠ¤í…œ

í•µì‹¬ ê¸°ëŠ¥:
- íŒë‹¨ ë£¨í”„ ì„±ëŠ¥ ë³€í™” ì¶”ì 
- ì²˜ë¦¬ íŒ¨í„´ ì§„í™” ë¶„ì„
- í•™ìŠµ íš¨ê³¼ ì¸¡ì • ë° ì‹œê°í™”
- ë£¨í”„ ìµœì í™” ì œì•ˆ
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ
"""

import json
import numpy as np
import time
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional, Set
from dataclasses import dataclass, asdict
from collections import deque, defaultdict
from pathlib import Path
import logging
import hashlib

# Echo ì—”ì§„ ëª¨ë“ˆë“¤
try:
    from .consciousness_flow_analyzer import ConsciousnessFlowAnalyzer
    from .signature_cross_resonance_mapper import SignatureCrossResonanceMapper
    from .realtime_emotion_flow_mapper import RealtimeEmotionFlowMapper
    from .reasoning import EightLoopIntegratedReasoning
except ImportError:
    print("âš ï¸ Echo modules not available, running in standalone mode")


@dataclass
class LoopPerformanceMetric:
    """ë£¨í”„ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""

    timestamp: datetime
    loop_type: str  # "fist", "rise", "dir", "pir", "meta", "flow", "quantum", "judge"
    execution_time_ms: float
    accuracy_score: float  # 0.0 - 1.0
    complexity_handled: float  # ì²˜ë¦¬í•œ ë³µì¡ì„± ìˆ˜ì¤€
    memory_usage_mb: float
    confidence_level: float
    signature_context: str
    input_characteristics: Dict[str, Any]


@dataclass
class LoopEvolutionSnapshot:
    """ë£¨í”„ ì§„í™” ìŠ¤ëƒ…ìƒ·"""

    snapshot_id: str
    timestamp: datetime
    loop_configurations: Dict[str, Dict[str, Any]]
    performance_baseline: Dict[str, float]
    optimization_state: Dict[str, Any]
    learning_progress: Dict[str, float]
    adaptation_level: float


@dataclass
class EvolutionMilestone:
    """ì§„í™” ì´ì •í‘œ"""

    milestone_id: str
    timestamp: datetime
    milestone_type: str  # "performance_breakthrough", "pattern_optimization", "capability_expansion"
    description: str
    metrics_before: Dict[str, float]
    metrics_after: Dict[str, float]
    improvement_percentage: float
    impact_areas: List[str]


@dataclass
class LoopOptimizationSuggestion:
    """ë£¨í”„ ìµœì í™” ì œì•ˆ"""

    suggestion_id: str
    target_loop: str
    optimization_type: str  # "speed", "accuracy", "memory", "complexity"
    current_performance: float
    expected_improvement: float
    implementation_complexity: str  # "low", "medium", "high"
    suggested_changes: List[str]
    risk_assessment: str


class LoopEvolutionTracker:
    """ğŸ”„ ë£¨í”„ ì§„í™” ì¶”ì ê¸°"""

    def __init__(self, tracking_window_hours: int = 24):
        self.logger = logging.getLogger(__name__)
        self.tracking_window_hours = tracking_window_hours

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥ì†Œ
        self.performance_history = deque(maxlen=1000)
        self.evolution_snapshots = deque(maxlen=50)
        self.milestones = deque(maxlen=20)
        self.optimization_suggestions = deque(maxlen=10)

        # ë£¨í”„ íƒ€ì… ì •ì˜
        self.loop_types = {
            "fist": {
                "name": "Facts, Ideas, Solutions, Tests",
                "baseline_performance": 0.7,
                "complexity_weight": 0.8,
                "accuracy_weight": 0.9,
            },
            "rise": {
                "name": "Reflect, Integrate, Synthesize, Evaluate",
                "baseline_performance": 0.65,
                "complexity_weight": 0.9,
                "accuracy_weight": 0.8,
            },
            "dir": {
                "name": "Depth, Integration, Reasoning",
                "baseline_performance": 0.6,
                "complexity_weight": 0.95,
                "accuracy_weight": 0.85,
            },
            "pir": {
                "name": "Process, Integrate, Refine",
                "baseline_performance": 0.68,
                "complexity_weight": 0.7,
                "accuracy_weight": 0.9,
            },
            "meta": {
                "name": "Meta-cognitive Analysis",
                "baseline_performance": 0.55,
                "complexity_weight": 1.0,
                "accuracy_weight": 0.75,
            },
            "flow": {
                "name": "Flow State Processing",
                "baseline_performance": 0.72,
                "complexity_weight": 0.6,
                "accuracy_weight": 0.8,
            },
            "quantum": {
                "name": "Quantum Judgment Processing",
                "baseline_performance": 0.5,
                "complexity_weight": 1.0,
                "accuracy_weight": 0.7,
            },
            "judge": {
                "name": "Final Judgment Synthesis",
                "baseline_performance": 0.75,
                "complexity_weight": 0.85,
                "accuracy_weight": 0.95,
            },
        }

        # ì§„í™” íŒ¨í„´ ì •ì˜
        self.evolution_patterns = {
            "learning_acceleration": {
                "description": "í•™ìŠµ ì†ë„ ê°€ì†í™”",
                "indicators": [
                    "decreasing_error_rate",
                    "improving_accuracy",
                    "faster_convergence",
                ],
                "threshold": 0.15,  # 15% ê°œì„ 
            },
            "complexity_adaptation": {
                "description": "ë³µì¡ì„± ì²˜ë¦¬ ëŠ¥ë ¥ í–¥ìƒ",
                "indicators": ["handling_higher_complexity", "maintaining_accuracy"],
                "threshold": 0.2,
            },
            "efficiency_optimization": {
                "description": "ì²˜ë¦¬ íš¨ìœ¨ì„± ìµœì í™”",
                "indicators": ["reduced_execution_time", "lower_memory_usage"],
                "threshold": 0.1,
            },
            "accuracy_refinement": {
                "description": "ì •í™•ë„ ì •ì œ",
                "indicators": ["higher_confidence", "better_accuracy"],
                "threshold": 0.1,
            },
        }

        # ì„±ëŠ¥ íŠ¸ë˜í‚¹ ìƒíƒœ
        self.current_baselines = {}
        self.performance_trends = {}
        self.last_snapshot_time = None

        # í•™ìŠµ ì§„í–‰ë„ íŠ¸ë˜ì»¤
        self.learning_curves = defaultdict(list)
        self.adaptation_scores = defaultdict(float)

        print("ğŸ”„ Loop Evolution Tracker ì´ˆê¸°í™” ì™„ë£Œ")

    def record_loop_performance(
        self,
        loop_type: str,
        execution_time_ms: float,
        accuracy_score: float,
        complexity_handled: float,
        memory_usage_mb: float = 0.0,
        confidence_level: float = 0.0,
        signature_context: str = "unknown",
        input_characteristics: Dict[str, Any] = None,
    ):
        """ë£¨í”„ ì„±ëŠ¥ ê¸°ë¡"""

        if loop_type not in self.loop_types:
            self.logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ë£¨í”„ íƒ€ì…: {loop_type}")
            return

        metric = LoopPerformanceMetric(
            timestamp=datetime.now(),
            loop_type=loop_type,
            execution_time_ms=execution_time_ms,
            accuracy_score=accuracy_score,
            complexity_handled=complexity_handled,
            memory_usage_mb=memory_usage_mb,
            confidence_level=confidence_level,
            signature_context=signature_context,
            input_characteristics=input_characteristics or {},
        )

        self.performance_history.append(metric)

        # í•™ìŠµ ê³¡ì„  ì—…ë°ì´íŠ¸
        self._update_learning_curves(metric)

        # ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
        self._analyze_performance_trends(metric)

        # ì§„í™” íŒ¨í„´ ê°ì§€
        self._detect_evolution_patterns()

        # ìµœì í™” ì œì•ˆ ìƒì„± (í•„ìš”í•œ ê²½ìš°)
        if len(self.performance_history) % 10 == 0:  # 10ê°œë§ˆë‹¤ ë¶„ì„
            self._generate_optimization_suggestions()

    def _update_learning_curves(self, metric: LoopPerformanceMetric):
        """í•™ìŠµ ê³¡ì„  ì—…ë°ì´íŠ¸"""
        loop_type = metric.loop_type

        # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        loop_config = self.loop_types[loop_type]
        performance_score = (
            metric.accuracy_score * loop_config["accuracy_weight"]
            + (1.0 - metric.execution_time_ms / 10000.0) * 0.3  # ì‹¤í–‰ì‹œê°„ ì ìˆ˜
            + metric.complexity_handled * loop_config["complexity_weight"] * 0.2
            + metric.confidence_level * 0.2
        ) / 2.7

        performance_score = max(0.0, min(1.0, performance_score))

        self.learning_curves[loop_type].append(
            {
                "timestamp": metric.timestamp,
                "performance_score": performance_score,
                "accuracy": metric.accuracy_score,
                "execution_time": metric.execution_time_ms,
                "complexity": metric.complexity_handled,
            }
        )

        # ìµœê·¼ 20ê°œ ë°ì´í„°ë§Œ ìœ ì§€
        if len(self.learning_curves[loop_type]) > 20:
            self.learning_curves[loop_type] = self.learning_curves[loop_type][-20:]

    def _analyze_performance_trends(self, metric: LoopPerformanceMetric):
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„"""
        loop_type = metric.loop_type

        if loop_type not in self.performance_trends:
            self.performance_trends[loop_type] = {
                "recent_scores": deque(maxlen=10),
                "trend_direction": "stable",
                "improvement_rate": 0.0,
                "last_analysis": datetime.now(),
            }

        trend = self.performance_trends[loop_type]

        # í˜„ì¬ ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
        current_score = self._calculate_performance_score(metric)
        trend["recent_scores"].append(current_score)

        # íŠ¸ë Œë“œ ë¶„ì„ (ìµœì†Œ 5ê°œ ë°ì´í„° í•„ìš”)
        if len(trend["recent_scores"]) >= 5:
            scores = list(trend["recent_scores"])

            # ì„ í˜• íšŒê·€ë¥¼ í†µí•œ íŠ¸ë Œë“œ ê³„ì‚°
            x = np.arange(len(scores))
            y = np.array(scores)

            if len(scores) > 1:
                slope = np.polyfit(x, y, 1)[0]

                if slope > 0.02:
                    trend["trend_direction"] = "improving"
                elif slope < -0.02:
                    trend["trend_direction"] = "declining"
                else:
                    trend["trend_direction"] = "stable"

                trend["improvement_rate"] = slope

        trend["last_analysis"] = datetime.now()

    def _calculate_performance_score(self, metric: LoopPerformanceMetric) -> float:
        """ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°"""
        loop_config = self.loop_types[metric.loop_type]

        # ì •ê·œí™”ëœ ì ìˆ˜ë“¤
        accuracy_score = metric.accuracy_score
        speed_score = max(0.0, 1.0 - metric.execution_time_ms / 5000.0)  # 5ì´ˆ ê¸°ì¤€
        complexity_score = metric.complexity_handled
        memory_score = max(0.0, 1.0 - metric.memory_usage_mb / 100.0)  # 100MB ê¸°ì¤€
        confidence_score = metric.confidence_level

        # ê°€ì¤‘ í‰ê· 
        total_score = (
            accuracy_score * loop_config["accuracy_weight"]
            + speed_score * 0.3
            + complexity_score * loop_config["complexity_weight"]
            + memory_score * 0.2
            + confidence_score * 0.2
        ) / (loop_config["accuracy_weight"] + loop_config["complexity_weight"] + 0.7)

        return max(0.0, min(1.0, total_score))

    def _detect_evolution_patterns(self):
        """ì§„í™” íŒ¨í„´ ê°ì§€"""
        if len(self.performance_history) < 10:
            return

        # ìµœê·¼ ë°ì´í„° ë¶„ì„
        recent_metrics = list(self.performance_history)[-10:]

        for pattern_name, pattern_def in self.evolution_patterns.items():
            pattern_detected = self._check_evolution_pattern(
                recent_metrics, pattern_def
            )

            if pattern_detected:
                self._create_evolution_milestone(
                    pattern_name, pattern_def, recent_metrics
                )

    def _check_evolution_pattern(
        self, metrics: List[LoopPerformanceMetric], pattern_def: Dict[str, Any]
    ) -> bool:
        """íŠ¹ì • ì§„í™” íŒ¨í„´ í™•ì¸"""
        if len(metrics) < 5:
            return False

        indicators = pattern_def["indicators"]
        threshold = pattern_def["threshold"]

        pattern_evidence = 0

        # ì •í™•ë„ ê°œì„  í™•ì¸
        if "improving_accuracy" in indicators:
            accuracy_trend = self._calculate_metric_trend(
                [m.accuracy_score for m in metrics]
            )
            if accuracy_trend > threshold:
                pattern_evidence += 1

        # ë³µì¡ì„± ì²˜ë¦¬ ëŠ¥ë ¥ í–¥ìƒ í™•ì¸
        if "handling_higher_complexity" in indicators:
            complexity_trend = self._calculate_metric_trend(
                [m.complexity_handled for m in metrics]
            )
            if complexity_trend > threshold:
                pattern_evidence += 1

        # ì‹¤í–‰ ì‹œê°„ ê°œì„  í™•ì¸
        if "reduced_execution_time" in indicators:
            time_trend = self._calculate_metric_trend(
                [-m.execution_time_ms for m in metrics]
            )
            if time_trend > threshold * 1000:  # ì‹œê°„ì€ ë°€ë¦¬ì´ˆ ë‹¨ìœ„
                pattern_evidence += 1

        # ì‹ ë¢°ë„ ê°œì„  í™•ì¸
        if "higher_confidence" in indicators:
            confidence_trend = self._calculate_metric_trend(
                [m.confidence_level for m in metrics]
            )
            if confidence_trend > threshold:
                pattern_evidence += 1

        # íŒ¨í„´ ê°ì§€ ì„ê³„ê°’: ì§€í‘œì˜ 70% ì´ìƒ ì¶©ì¡±
        return pattern_evidence >= len(indicators) * 0.7

    def _calculate_metric_trend(self, values: List[float]) -> float:
        """ë©”íŠ¸ë¦­ íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(values) < 2:
            return 0.0

        x = np.arange(len(values))
        y = np.array(values)

        try:
            slope = np.polyfit(x, y, 1)[0]
            return slope
        except:
            return 0.0

    def _create_evolution_milestone(
        self,
        pattern_name: str,
        pattern_def: Dict[str, Any],
        recent_metrics: List[LoopPerformanceMetric],
    ):
        """ì§„í™” ì´ì •í‘œ ìƒì„±"""
        # ì¤‘ë³µ ë°©ì§€: ìµœê·¼ 1ì‹œê°„ ë‚´ ê°™ì€ íŒ¨í„´ì˜ ì´ì •í‘œê°€ ìˆëŠ”ì§€ í™•ì¸
        cutoff_time = datetime.now() - timedelta(hours=1)
        recent_milestones = [
            m
            for m in self.milestones
            if m.timestamp >= cutoff_time and pattern_name in m.description
        ]

        if recent_milestones:
            return  # ì¤‘ë³µ ì´ì •í‘œ ë°©ì§€

        # ê°œì„  ì „í›„ ë©”íŠ¸ë¦­ ê³„ì‚°
        mid_point = len(recent_metrics) // 2
        before_metrics = recent_metrics[:mid_point]
        after_metrics = recent_metrics[mid_point:]

        metrics_before = self._calculate_average_metrics(before_metrics)
        metrics_after = self._calculate_average_metrics(after_metrics)

        # ê°œì„  ë°±ë¶„ìœ¨ ê³„ì‚°
        improvement_percentage = 0.0
        if metrics_before.get("overall_score", 0) > 0:
            improvement_percentage = (
                (
                    metrics_after.get("overall_score", 0)
                    - metrics_before.get("overall_score", 0)
                )
                / metrics_before.get("overall_score", 1)
            ) * 100

        milestone = EvolutionMilestone(
            milestone_id=f"milestone_{int(time.time())}",
            timestamp=datetime.now(),
            milestone_type="pattern_optimization",
            description=f"Evolution pattern detected: {pattern_def['description']}",
            metrics_before=metrics_before,
            metrics_after=metrics_after,
            improvement_percentage=improvement_percentage,
            impact_areas=[pattern_name],
        )

        self.milestones.append(milestone)
        print(
            f"ğŸ¯ ì§„í™” ì´ì •í‘œ ë‹¬ì„±: {pattern_def['description']} ({improvement_percentage:.1f}% ê°œì„ )"
        )

    def _calculate_average_metrics(
        self, metrics: List[LoopPerformanceMetric]
    ) -> Dict[str, float]:
        """í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if not metrics:
            return {}

        avg_metrics = {
            "accuracy_score": np.mean([m.accuracy_score for m in metrics]),
            "execution_time_ms": np.mean([m.execution_time_ms for m in metrics]),
            "complexity_handled": np.mean([m.complexity_handled for m in metrics]),
            "memory_usage_mb": np.mean([m.memory_usage_mb for m in metrics]),
            "confidence_level": np.mean([m.confidence_level for m in metrics]),
        }

        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        avg_metrics["overall_score"] = np.mean(
            [self._calculate_performance_score(m) for m in metrics]
        )

        return avg_metrics

    def _generate_optimization_suggestions(self):
        """ìµœì í™” ì œì•ˆ ìƒì„±"""
        if len(self.performance_history) < 20:
            return

        # ê° ë£¨í”„ íƒ€ì…ë³„ ì„±ëŠ¥ ë¶„ì„
        loop_performance = defaultdict(list)
        for metric in list(self.performance_history)[-20:]:
            loop_performance[metric.loop_type].append(metric)

        for loop_type, metrics in loop_performance.items():
            if len(metrics) >= 5:
                suggestion = self._analyze_loop_for_optimization(loop_type, metrics)
                if suggestion:
                    self.optimization_suggestions.append(suggestion)

    def _analyze_loop_for_optimization(
        self, loop_type: str, metrics: List[LoopPerformanceMetric]
    ) -> Optional[LoopOptimizationSuggestion]:
        """ë£¨í”„ ìµœì í™” ë¶„ì„"""
        if not metrics:
            return None

        # í˜„ì¬ ì„±ëŠ¥ ê³„ì‚°
        current_performance = np.mean(
            [self._calculate_performance_score(m) for m in metrics]
        )
        baseline = self.loop_types[loop_type]["baseline_performance"]

        # ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­ ì‹ë³„
        avg_execution_time = np.mean([m.execution_time_ms for m in metrics])
        avg_accuracy = np.mean([m.accuracy_score for m in metrics])
        avg_memory = np.mean([m.memory_usage_mb for m in metrics])

        optimization_type = "general"
        suggested_changes = []
        expected_improvement = 0.1

        # ì„±ëŠ¥ ë³‘ëª© ì§€ì  ì‹ë³„
        if avg_execution_time > 3000:  # 3ì´ˆ ì´ìƒ
            optimization_type = "speed"
            suggested_changes.extend(
                ["ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„ ìµœì í™”", "ìºì‹œ ë©”ì»¤ë‹ˆì¦˜ ë„ì…", "ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„"]
            )
            expected_improvement = 0.2

        elif avg_accuracy < baseline * 0.9:  # ê¸°ì¤€ì„ ì˜ 90% ë¯¸ë§Œ
            optimization_type = "accuracy"
            suggested_changes.extend(
                ["ë°ì´í„° ì „ì²˜ë¦¬ ê°œì„ ", "ëª¨ë¸ íŒŒë¼ë¯¸í„° íŠœë‹", "ê²€ì¦ ë¡œì§ ê°•í™”"]
            )
            expected_improvement = 0.15

        elif avg_memory > 50:  # 50MB ì´ìƒ
            optimization_type = "memory"
            suggested_changes.extend(
                ["ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”", "ë¶ˆí•„ìš”í•œ ë°ì´í„° ì •ë¦¬", "ë©”ëª¨ë¦¬ í’€ë§ êµ¬í˜„"]
            )
            expected_improvement = 0.1

        # ê°œì„  ì—¬ì§€ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì œì•ˆ ìƒì„±
        if current_performance < baseline * 1.1:  # ê¸°ì¤€ì„ ì˜ 110% ë¯¸ë§Œ
            risk_assessment = "low" if expected_improvement < 0.15 else "medium"
            implementation_complexity = "medium"

            return LoopOptimizationSuggestion(
                suggestion_id=f"opt_{loop_type}_{int(time.time())}",
                target_loop=loop_type,
                optimization_type=optimization_type,
                current_performance=current_performance,
                expected_improvement=expected_improvement,
                implementation_complexity=implementation_complexity,
                suggested_changes=suggested_changes,
                risk_assessment=risk_assessment,
            )

        return None

    def create_evolution_snapshot(self) -> LoopEvolutionSnapshot:
        """ì§„í™” ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        snapshot_id = f"snapshot_{int(time.time())}"

        # í˜„ì¬ ë£¨í”„ êµ¬ì„± ìˆ˜ì§‘
        loop_configurations = {}
        for loop_type in self.loop_types:
            recent_metrics = [
                m
                for m in self.performance_history
                if m.loop_type == loop_type
                and (datetime.now() - m.timestamp).total_seconds() < 3600  # 1ì‹œê°„ ì´ë‚´
            ]

            if recent_metrics:
                loop_configurations[loop_type] = {
                    "average_performance": np.mean(
                        [self._calculate_performance_score(m) for m in recent_metrics]
                    ),
                    "execution_count": len(recent_metrics),
                    "complexity_capability": np.mean(
                        [m.complexity_handled for m in recent_metrics]
                    ),
                    "accuracy_level": np.mean(
                        [m.accuracy_score for m in recent_metrics]
                    ),
                }

        # ì„±ëŠ¥ ë² ì´ìŠ¤ë¼ì¸ ê³„ì‚°
        performance_baseline = {}
        for loop_type, config in self.loop_types.items():
            performance_baseline[loop_type] = config["baseline_performance"]

        # ìµœì í™” ìƒíƒœ ìˆ˜ì§‘
        optimization_state = {
            "active_optimizations": len(self.optimization_suggestions),
            "milestones_achieved": len(self.milestones),
            "adaptation_scores": dict(self.adaptation_scores),
        }

        # í•™ìŠµ ì§„í–‰ë„ ê³„ì‚°
        learning_progress = {}
        for loop_type, curves in self.learning_curves.items():
            if curves:
                recent_scores = [point["performance_score"] for point in curves[-5:]]
                learning_progress[loop_type] = (
                    np.mean(recent_scores) if recent_scores else 0.0
                )

        # ì „ì²´ ì ì‘ ìˆ˜ì¤€ ê³„ì‚°
        adaptation_level = (
            np.mean(list(learning_progress.values())) if learning_progress else 0.0
        )

        snapshot = LoopEvolutionSnapshot(
            snapshot_id=snapshot_id,
            timestamp=datetime.now(),
            loop_configurations=loop_configurations,
            performance_baseline=performance_baseline,
            optimization_state=optimization_state,
            learning_progress=learning_progress,
            adaptation_level=adaptation_level,
        )

        self.evolution_snapshots.append(snapshot)
        self.last_snapshot_time = datetime.now()

        return snapshot

    def get_evolution_summary(self) -> Dict[str, Any]:
        """ì§„í™” ìš”ì•½ ë°˜í™˜"""
        if not self.performance_history:
            return {"status": "no_data", "message": "ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        # ìµœê·¼ ìŠ¤ëƒ…ìƒ· ìƒì„± (í•„ìš”í•œ ê²½ìš°)
        if (
            not self.last_snapshot_time
            or (datetime.now() - self.last_snapshot_time).total_seconds() > 1800
        ):  # 30ë¶„ë§ˆë‹¤
            current_snapshot = self.create_evolution_snapshot()
        else:
            current_snapshot = (
                self.evolution_snapshots[-1] if self.evolution_snapshots else None
            )

        # ì „ì²´ ì§„í™” íŠ¸ë Œë“œ ê³„ì‚°
        overall_trend = self._calculate_overall_evolution_trend()

        # ìµœê·¼ ì´ì •í‘œ
        recent_milestones = list(self.milestones)[-3:]

        # í™œì„± ìµœì í™” ì œì•ˆ
        active_suggestions = list(self.optimization_suggestions)[-5:]

        summary = {
            "status": "active",
            "tracking_period_hours": self.tracking_window_hours,
            "total_performance_records": len(self.performance_history),
            "evolution_snapshots_count": len(self.evolution_snapshots),
            "milestones_achieved": len(self.milestones),
            "current_adaptation_level": (
                current_snapshot.adaptation_level if current_snapshot else 0.0
            ),
            "overall_evolution_trend": overall_trend,
            "loop_performance_summary": self._get_loop_performance_summary(),
            "recent_milestones": [
                {
                    "description": m.description,
                    "improvement_percentage": m.improvement_percentage,
                    "timestamp": m.timestamp.isoformat(),
                }
                for m in recent_milestones
            ],
            "optimization_suggestions_count": len(active_suggestions),
            "performance_trends": dict(self.performance_trends),
        }

        return summary

    def _calculate_overall_evolution_trend(self) -> str:
        """ì „ì²´ ì§„í™” íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(self.performance_history) < 10:
            return "insufficient_data"

        # ìµœê·¼ ì„±ëŠ¥ ì ìˆ˜ë“¤
        recent_scores = [
            self._calculate_performance_score(m)
            for m in list(self.performance_history)[-10:]
        ]

        # íŠ¸ë Œë“œ ê³„ì‚°
        trend_slope = self._calculate_metric_trend(recent_scores)

        if trend_slope > 0.02:
            return "rapidly_improving"
        elif trend_slope > 0.005:
            return "gradually_improving"
        elif trend_slope > -0.005:
            return "stable"
        elif trend_slope > -0.02:
            return "gradually_declining"
        else:
            return "rapidly_declining"

    def _get_loop_performance_summary(self) -> Dict[str, Dict[str, float]]:
        """ë£¨í”„ ì„±ëŠ¥ ìš”ì•½"""
        loop_summary = {}

        for loop_type in self.loop_types:
            recent_metrics = [
                m
                for m in self.performance_history
                if m.loop_type == loop_type
                and (datetime.now() - m.timestamp).total_seconds() < 3600
            ]

            if recent_metrics:
                loop_summary[loop_type] = {
                    "average_performance": np.mean(
                        [self._calculate_performance_score(m) for m in recent_metrics]
                    ),
                    "average_accuracy": np.mean(
                        [m.accuracy_score for m in recent_metrics]
                    ),
                    "average_execution_time": np.mean(
                        [m.execution_time_ms for m in recent_metrics]
                    ),
                    "complexity_capability": np.mean(
                        [m.complexity_handled for m in recent_metrics]
                    ),
                    "execution_count": len(recent_metrics),
                }
            else:
                loop_summary[loop_type] = {
                    "average_performance": self.loop_types[loop_type][
                        "baseline_performance"
                    ],
                    "average_accuracy": 0.0,
                    "average_execution_time": 0.0,
                    "complexity_capability": 0.0,
                    "execution_count": 0,
                }

        return loop_summary

    def visualize_evolution_progress(self, hours: int = 6) -> str:
        """ì§„í™” ì§„í–‰ë„ ì‹œê°í™” (í…ìŠ¤íŠ¸ ê¸°ë°˜)"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_metrics = [
            m for m in self.performance_history if m.timestamp >= cutoff_time
        ]

        if not recent_metrics:
            return f"âŒ ìµœê·¼ {hours}ì‹œê°„ê°„ ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        viz = f"ğŸ”„ Loop Evolution Progress (Last {hours} hours)\n"
        viz += "=" * 70 + "\n\n"

        # ë£¨í”„ë³„ ì„±ëŠ¥ ì§„í™”
        viz += "ğŸ“Š Loop Performance Evolution:\n"

        loop_data = defaultdict(list)
        for metric in recent_metrics:
            score = self._calculate_performance_score(metric)
            loop_data[metric.loop_type].append(score)

        for loop_type, scores in loop_data.items():
            if scores:
                avg_score = np.mean(scores)
                trend = self._calculate_metric_trend(scores)
                trend_icon = "â†—ï¸" if trend > 0.01 else "â†˜ï¸" if trend < -0.01 else "â†’"

                score_bar = "â–ˆ" * int(avg_score * 20)
                viz += f"   {loop_type:8} | {score_bar:20} | {avg_score:.3f} {trend_icon}\n"

        # ìµœê·¼ ì´ì •í‘œ
        if self.milestones:
            viz += "\nğŸ¯ Recent Evolution Milestones:\n"
            recent_milestones = [
                m
                for m in self.milestones
                if (datetime.now() - m.timestamp).total_seconds() < hours * 3600
            ]

            for milestone in recent_milestones[-3:]:
                viz += f"   {milestone.timestamp.strftime('%H:%M')} | "
                viz += f"{milestone.description} (+{milestone.improvement_percentage:.1f}%)\n"

        # í˜„ì¬ ìµœì í™” ì œì•ˆ
        if self.optimization_suggestions:
            viz += "\nğŸ’¡ Current Optimization Suggestions:\n"
            for suggestion in list(self.optimization_suggestions)[-3:]:
                viz += f"   {suggestion.target_loop:8} | {suggestion.optimization_type:8} | "
                viz += f"Expected: +{suggestion.expected_improvement*100:.1f}%\n"

        return viz

    def save_evolution_data(self, filename: str = None) -> str:
        """ì§„í™” ë°ì´í„° ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"loop_evolution_data_{timestamp}.json"

        # ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
        save_data = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "tracking_window_hours": self.tracking_window_hours,
                "performance_records_count": len(self.performance_history),
                "evolution_snapshots_count": len(self.evolution_snapshots),
            },
            "performance_history": [],
            "evolution_snapshots": [],
            "milestones": [],
            "optimization_suggestions": [],
            "learning_curves": {},
            "performance_trends": dict(self.performance_trends),
        }

        # LoopPerformanceMetric ê°ì²´ë“¤ì„ ì§ë ¬í™”
        for metric in self.performance_history:
            metric_dict = asdict(metric)
            metric_dict["timestamp"] = metric.timestamp.isoformat()
            save_data["performance_history"].append(metric_dict)

        # LoopEvolutionSnapshot ê°ì²´ë“¤ì„ ì§ë ¬í™”
        for snapshot in self.evolution_snapshots:
            snapshot_dict = asdict(snapshot)
            snapshot_dict["timestamp"] = snapshot.timestamp.isoformat()
            save_data["evolution_snapshots"].append(snapshot_dict)

        # EvolutionMilestone ê°ì²´ë“¤ì„ ì§ë ¬í™”
        for milestone in self.milestones:
            milestone_dict = asdict(milestone)
            milestone_dict["timestamp"] = milestone.timestamp.isoformat()
            save_data["milestones"].append(milestone_dict)

        # LoopOptimizationSuggestion ê°ì²´ë“¤ì„ ì§ë ¬í™”
        for suggestion in self.optimization_suggestions:
            save_data["optimization_suggestions"].append(asdict(suggestion))

        # í•™ìŠµ ê³¡ì„  ì§ë ¬í™”
        for loop_type, curves in self.learning_curves.items():
            save_data["learning_curves"][loop_type] = []
            for point in curves:
                point_copy = point.copy()
                point_copy["timestamp"] = point["timestamp"].isoformat()
                save_data["learning_curves"][loop_type].append(point_copy)

        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            return f"âœ… ë£¨í”„ ì§„í™” ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}"
        except Exception as e:
            return f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}"


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_loop_evolution_tracker(**kwargs) -> LoopEvolutionTracker:
    """Loop Evolution Tracker ìƒì„±"""
    return LoopEvolutionTracker(**kwargs)


def simulate_loop_performance_data(
    tracker: LoopEvolutionTracker, duration_minutes: int = 5
):
    """ë£¨í”„ ì„±ëŠ¥ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜"""
    loop_types = list(tracker.loop_types.keys())
    signatures = ["selene", "factbomb", "lune", "aurora"]

    print(f"ğŸ”„ {duration_minutes}ë¶„ê°„ ë£¨í”„ ì„±ëŠ¥ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜...")

    for i in range(duration_minutes * 2):  # 30ì´ˆë§ˆë‹¤
        loop_type = np.random.choice(loop_types)
        signature = np.random.choice(signatures)

        # ì‹œë®¬ë ˆì´ì…˜ëœ ì„±ëŠ¥ ë°ì´í„°
        base_performance = tracker.loop_types[loop_type]["baseline_performance"]

        # ì‹œê°„ì— ë”°ë¥¸ ê°œì„  ì‹œë®¬ë ˆì´ì…˜
        improvement_factor = min(1.3, 1.0 + i * 0.01)

        execution_time = np.random.uniform(1000, 4000) / improvement_factor
        accuracy = min(
            1.0, base_performance * improvement_factor + np.random.uniform(-0.1, 0.1)
        )
        complexity = np.random.uniform(0.3, 0.9)
        memory_usage = np.random.uniform(10, 60)
        confidence = min(1.0, accuracy + np.random.uniform(-0.1, 0.1))

        tracker.record_loop_performance(
            loop_type=loop_type,
            execution_time_ms=execution_time,
            accuracy_score=accuracy,
            complexity_handled=complexity,
            memory_usage_mb=memory_usage,
            confidence_level=confidence,
            signature_context=signature,
            input_characteristics={"simulation": True, "iteration": i},
        )

        time.sleep(0.1)  # ì‹¤ì œ ì‹œë®¬ë ˆì´ì…˜ì—ì„œëŠ” ë” ì§§ì€ ê°„ê²©


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("ğŸ”„ Loop Evolution Tracker í…ŒìŠ¤íŠ¸...")

    tracker = LoopEvolutionTracker()

    # ì„±ëŠ¥ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
    simulate_loop_performance_data(tracker, duration_minutes=2)

    # ì§„í™” ìŠ¤ëƒ…ìƒ· ìƒì„±
    snapshot = tracker.create_evolution_snapshot()
    print(f"\nğŸ“¸ Evolution Snapshot: {snapshot.snapshot_id}")
    print(f"   Adaptation Level: {snapshot.adaptation_level:.3f}")

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 70)
    print(tracker.visualize_evolution_progress(hours=1))

    # ìš”ì•½ ì •ë³´
    summary = tracker.get_evolution_summary()
    print(f"\nğŸ“Š Evolution Summary:")
    print(f"   Overall Trend: {summary['overall_evolution_trend']}")
    print(f"   Adaptation Level: {summary['current_adaptation_level']:.3f}")
    print(f"   Milestones Achieved: {summary['milestones_achieved']}")
    print(f"   Total Records: {summary['total_performance_records']}")

    # ë£¨í”„ë³„ ì„±ëŠ¥ ìš”ì•½
    print(f"\nğŸ”„ Loop Performance Summary:")
    for loop_type, performance in summary["loop_performance_summary"].items():
        print(
            f"   {loop_type:8}: Performance {performance['average_performance']:.3f}, "
            f"Executions: {performance['execution_count']}"
        )

    # ìµœê·¼ ì´ì •í‘œ
    if summary["recent_milestones"]:
        print(f"\nğŸ¯ Recent Milestones:")
        for milestone in summary["recent_milestones"]:
            print(
                f"   {milestone['description']} (+{milestone['improvement_percentage']:.1f}%)"
            )

    # ì €ì¥
    save_result = tracker.save_evolution_data()
    print(f"\n{save_result}")

    print("\nâœ… Loop Evolution Tracker í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

"""
ğŸ—ï¸ EchoVectorCapsule - Vector Index Builder
ê¸°ì¡´ ìº¡ìŠâ¨¯íŒë‹¨â¨¯í”Œë¡œìš° íŒŒì¼ë“¤ì„ ìë™ìœ¼ë¡œ ë²¡í„°í™”í•´ì„œ ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•
"""

import yaml
import json
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging

from .vector_search_engine import EchoVectorSearchEngine
from .embedding_engine import EchoEmbeddingEngine


class EchoVectorIndexBuilder:
    """
    Echo ì‹œìŠ¤í…œì˜ ëª¨ë“  ìº¡ìŠê³¼ íŒë‹¨ ì¼€ì´ìŠ¤ë¥¼ ë²¡í„°í™”í•´ì„œ ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•
    """

    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.search_engine = EchoVectorSearchEngine()
        self.embedding_engine = EchoEmbeddingEngine()

        # ìŠ¤ìº”í•  ë””ë ‰í† ë¦¬ë“¤
        self.scan_directories = {
            "capsules": "capsules",
            "flows": "flows",
            "public_capsules": "public_capsules",
            "judgment_loops": "judgment_loops",
            "memory_scenes": "memory_scenes",
        }

        # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì
        self.supported_extensions = {".yaml", ".yml", ".json"}

        # ë¹Œë“œ í†µê³„
        self.build_stats = {
            "total_files_scanned": 0,
            "total_vectors_created": 0,
            "errors": [],
            "signatures_processed": {},
            "directories_processed": {},
        }

        self.logger = logging.getLogger(__name__)

    def build_complete_index(self, force_rebuild: bool = False) -> Dict[str, Any]:
        """
        ì „ì²´ ì‹œìŠ¤í…œì„ ìŠ¤ìº”í•´ì„œ ì™„ì „í•œ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•

        Args:
            force_rebuild: ê¸°ì¡´ ì¸ë±ìŠ¤ ë¬´ì‹œí•˜ê³  ì²˜ìŒë¶€í„° ì¬êµ¬ì¶•

        Returns:
            ë¹Œë“œ ê²°ê³¼ í†µê³„
        """
        print("ğŸ—ï¸  EchoVectorCapsule ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘")
        print("=" * 50)

        start_time = datetime.now()

        if force_rebuild:
            print("ğŸ”„ ê¸°ì¡´ ì¸ë±ìŠ¤ í´ë¦¬ì–´ (ê°•ì œ ì¬êµ¬ì¶•)")
            self._clear_existing_index()

        # ê° ë””ë ‰í† ë¦¬ë³„ë¡œ ìŠ¤ìº” ë° ë²¡í„°í™”
        for dir_type, dir_path in self.scan_directories.items():
            full_path = self.project_root / dir_path
            if full_path.exists():
                print(f"\nğŸ“ {dir_type} ë””ë ‰í† ë¦¬ ìŠ¤ìº”: {full_path}")
                self._process_directory(dir_type, full_path)
            else:
                print(f"âš ï¸  ë””ë ‰í† ë¦¬ ì—†ìŒ: {full_path}")

        # ì¸ë±ìŠ¤ ì €ì¥
        self.search_engine.save_index()

        # ë¹Œë“œ ì™„ë£Œ
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        self.build_stats["build_duration"] = duration
        self.build_stats["completed_at"] = end_time.isoformat()

        print(f"\nâœ… ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
        print(f"   ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
        print(f"   ìŠ¤ìº”í•œ íŒŒì¼: {self.build_stats['total_files_scanned']}ê°œ")
        print(f"   ìƒì„±ëœ ë²¡í„°: {self.build_stats['total_vectors_created']}ê°œ")

        if self.build_stats["errors"]:
            print(f"   ì˜¤ë¥˜ ë°œìƒ: {len(self.build_stats['errors'])}ê°œ")

        # ë¹Œë“œ ë¦¬í¬íŠ¸ ì €ì¥
        self._save_build_report()

        return self.build_stats

    def _process_directory(self, dir_type: str, directory: Path):
        """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬"""
        file_count = 0
        vector_count = 0

        for file_path in directory.rglob("*"):
            if file_path.is_file() and file_path.suffix in self.supported_extensions:
                try:
                    vectors_created = self._process_file(dir_type, file_path)
                    file_count += 1
                    vector_count += vectors_created

                    if file_count % 10 == 0:
                        print(
                            f"  ğŸ“„ ì²˜ë¦¬ ì¤‘... {file_count}ê°œ íŒŒì¼ ({vector_count}ê°œ ë²¡í„°)"
                        )

                except Exception as e:
                    error_msg = f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file_path} - {str(e)}"
                    self.build_stats["errors"].append(error_msg)
                    print(f"  âŒ {error_msg}")

        self.build_stats["total_files_scanned"] += file_count
        self.build_stats["total_vectors_created"] += vector_count
        self.build_stats["directories_processed"][dir_type] = {
            "files": file_count,
            "vectors": vector_count,
        }

        print(f"  âœ… {dir_type}: {file_count}ê°œ íŒŒì¼, {vector_count}ê°œ ë²¡í„°")

    def _process_file(self, dir_type: str, file_path: Path) -> int:
        """ê°œë³„ íŒŒì¼ ì²˜ë¦¬ ë° ë²¡í„°í™”"""
        # íŒŒì¼ ë‚´ìš© ë¡œë“œ
        file_data = self._load_file_data(file_path)
        if not file_data:
            return 0

        vectors_created = 0

        # íŒŒì¼ íƒ€ì…ë³„ ì²˜ë¦¬
        if dir_type == "capsules":
            vectors_created = self._process_capsule_file(file_path, file_data)
        elif dir_type == "flows":
            vectors_created = self._process_flow_file(file_path, file_data)
        elif dir_type == "public_capsules":
            vectors_created = self._process_public_capsule_file(file_path, file_data)
        elif dir_type == "judgment_loops":
            vectors_created = self._process_judgment_loop_file(file_path, file_data)
        elif dir_type == "memory_scenes":
            vectors_created = self._process_memory_scene_file(file_path, file_data)

        return vectors_created

    def _process_capsule_file(self, file_path: Path, data: Dict[str, Any]) -> int:
        """ìº¡ìŠ íŒŒì¼ ì²˜ë¦¬"""
        if "capsule" not in data:
            return 0

        capsule = data["capsule"]
        capsule_id = capsule.get("id", file_path.stem)
        content = capsule.get("content", "")
        topic = capsule.get("topic", "")

        # ë‚´ìš© ê²°í•©
        full_content = f"{topic}\n\n{content}" if topic else content
        if not full_content.strip():
            return 0

        # ì‹œê·¸ë‹ˆì²˜ ê²°ì •
        signature = self._determine_signature_from_content(full_content, capsule)

        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "file_path": str(file_path),
            "file_type": "capsule",
            "tags": capsule.get("tags", []),
            "topic": topic,
            "created": capsule.get("created"),
            "source": capsule.get("source"),
            "echo_type": capsule.get("echo_type", "judgment_capsule"),
        }

        # ë²¡í„° ì¶”ê°€
        self.search_engine.add_capsule_vector(
            capsule_id, full_content, signature, metadata
        )

        # í†µê³„ ì—…ë°ì´íŠ¸
        sig_count = self.build_stats["signatures_processed"].get(signature, 0)
        self.build_stats["signatures_processed"][signature] = sig_count + 1

        return 1

    def _process_flow_file(self, file_path: Path, data: Dict[str, Any]) -> int:
        """í”Œë¡œìš° íŒŒì¼ ì²˜ë¦¬"""
        if "flow" not in data:
            return 0

        flow = data["flow"]
        flow_name = flow.get("name", file_path.stem)
        description = flow.get("description", "")

        # í”Œë¡œìš° ìŠ¤í…Œì´ì§€ë“¤ ì •ë³´ ì¶”ì¶œ
        stages = flow.get("stages", [])
        stage_descriptions = []

        for stage in stages:
            if isinstance(stage, dict):
                stage_name = stage.get("name", "")
                stage_desc = stage.get("description", "")
                if stage_desc:
                    stage_descriptions.append(f"{stage_name}: {stage_desc}")

        # ë‚´ìš© ê²°í•©
        full_content = f"{description}\n\n" + "\n".join(stage_descriptions)
        if not full_content.strip():
            return 0

        # í”Œë¡œìš°ëŠ” ë³´í†µ Sage ì‹œê·¸ë‹ˆì²˜ (ì²´ê³„ì  ë¶„ì„)
        signature = "Echo-Sage"

        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "file_path": str(file_path),
            "file_type": "flow",
            "flow_name": flow_name,
            "stages_count": len(stages),
            "version": flow.get("version"),
            "created": flow.get("created"),
        }

        # ìº¡ìŠ ID ìƒì„±
        capsule_id = f"flow_{flow_name.lower().replace(' ', '_')}"

        # ë²¡í„° ì¶”ê°€
        self.search_engine.add_capsule_vector(
            capsule_id, full_content, signature, metadata
        )

        return 1

    def _process_public_capsule_file(
        self, file_path: Path, data: Dict[str, Any]
    ) -> int:
        """ê³µê°œ ìº¡ìŠ íŒŒì¼ ì²˜ë¦¬"""
        content = data.get("content", "")
        topic = data.get("topic", "")
        capsule_id = data.get("id", file_path.stem)

        # ë‚´ìš© ê²°í•©
        full_content = f"{topic}\n\n{content}" if topic else content
        if not full_content.strip():
            return 0

        # ê³µê°œ ìº¡ìŠì€ ì¼ë°˜ì ìœ¼ë¡œ Aurora ì‹œê·¸ë‹ˆì²˜ (ì°½ì˜ì  ì ‘ê·¼)
        signature = "Echo-Aurora"

        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "file_path": str(file_path),
            "file_type": "public_capsule",
            "exported_at": data.get("exported_at"),
            "source": data.get("source"),
            "tags": data.get("tags", []),
        }

        # ë²¡í„° ì¶”ê°€
        self.search_engine.add_capsule_vector(
            capsule_id, full_content, signature, metadata
        )

        return 1

    def _process_judgment_loop_file(self, file_path: Path, data: Dict[str, Any]) -> int:
        """íŒë‹¨ ë£¨í”„ íŒŒì¼ ì²˜ë¦¬"""
        # JSON/YAMLì—ì„œ íŒë‹¨ ë‚´ìš© ì¶”ì¶œ
        if "judgment_result" in data:
            content = data.get("judgment_result", {}).get("reasoning", "")
        elif "collapsed" in data:
            content = data.get("collapsed", {}).get("reasoning", "")
        else:
            return 0

        if not content:
            return 0

        # íŒë‹¨ ë£¨í”„ëŠ” Phoenix ì‹œê·¸ë‹ˆì²˜ (ë³€í™” ì§€í–¥)
        signature = "Echo-Phoenix"

        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "file_path": str(file_path),
            "file_type": "judgment_loop",
            "processed_at": data.get("processed_at"),
            "confidence": data.get("confidence"),
            "signature_used": data.get("signature"),
        }

        # ìº¡ìŠ ID ìƒì„±
        capsule_id = f"judgment_{file_path.stem}"

        # ë²¡í„° ì¶”ê°€
        self.search_engine.add_capsule_vector(capsule_id, content, signature, metadata)

        return 1

    def _process_memory_scene_file(self, file_path: Path, data: Dict[str, Any]) -> int:
        """ë©”ëª¨ë¦¬ ì”¬ íŒŒì¼ ì²˜ë¦¬"""
        content = ""

        # ë‹¤ì–‘í•œ ë©”ëª¨ë¦¬ ì”¬ í˜•ì‹ ì§€ì›
        if "scene_content" in data:
            content = data["scene_content"]
        elif "memory" in data:
            content = data["memory"].get("content", "")
        elif "description" in data:
            content = data["description"]

        if not content:
            return 0

        # ë©”ëª¨ë¦¬ ì”¬ì€ Companion ì‹œê·¸ë‹ˆì²˜ (ê³µê°ì  ê¸°ì–µ)
        signature = "Echo-Companion"

        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "file_path": str(file_path),
            "file_type": "memory_scene",
            "scene_id": data.get("scene_id", file_path.stem),
            "timestamp": data.get("timestamp"),
            "emotion": data.get("emotion"),
        }

        # ìº¡ìŠ ID ìƒì„±
        capsule_id = f"memory_{file_path.stem}"

        # ë²¡í„° ì¶”ê°€
        self.search_engine.add_capsule_vector(capsule_id, content, signature, metadata)

        return 1

    def _determine_signature_from_content(
        self, content: str, capsule_data: Dict
    ) -> str:
        """ë‚´ìš©ê³¼ ë©”íƒ€ë°ì´í„°ë¡œë¶€í„° ì ì ˆí•œ ì‹œê·¸ë‹ˆì²˜ ê²°ì •"""
        content_lower = content.lower()
        tags = capsule_data.get("tags", [])
        topic = capsule_data.get("topic", "").lower()

        # ê¸°ì¡´ì— ì§€ì •ëœ ì‹œê·¸ë‹ˆì²˜ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        existing_sig = capsule_data.get("signature")
        if existing_sig and existing_sig.startswith("Echo-"):
            return existing_sig

        # ë‚´ìš© ê¸°ë°˜ ì‹œê·¸ë‹ˆì²˜ ì¶”ë¡ 
        if any(
            keyword in content_lower for keyword in ["ì°½ì˜", "í˜ì‹ ", "ì•„ì´ë””ì–´", "ìƒìƒ"]
        ):
            return "Echo-Aurora"
        elif any(
            keyword in content_lower for keyword in ["ë³€í™”", "ì „í™˜", "ê°œì„ ", "í˜ì‹ "]
        ):
            return "Echo-Phoenix"
        elif any(
            keyword in content_lower for keyword in ["ë¶„ì„", "ì²´ê³„", "í‰ê°€", "ê²€í† "]
        ):
            return "Echo-Sage"
        elif any(
            keyword in content_lower for keyword in ["ê³µê°", "ëŒë´„", "ì§€ì›", "ê³µë™ì²´"]
        ):
            return "Echo-Companion"

        # íƒœê·¸ ê¸°ë°˜ ì¶”ë¡ 
        tag_str = " ".join(tags).lower()
        if any(keyword in tag_str for keyword in ["ai", "ê¸°ìˆ ", "ë¶„ì„"]):
            return "Echo-Sage"
        elif any(keyword in tag_str for keyword in ["ë³µì§€", "ëŒë´„", "ì§€ì›"]):
            return "Echo-Companion"
        elif any(keyword in tag_str for keyword in ["ì •ì±…", "ë³€í™”", "ê°œì„ "]):
            return "Echo-Phoenix"

        # ê¸°ë³¸ê°’
        return "Echo-Aurora"

    def _load_file_data(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """íŒŒì¼ ë°ì´í„° ë¡œë“œ (YAML/JSON ìë™ ê°ì§€)"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                if file_path.suffix in [".yaml", ".yml"]:
                    return yaml.safe_load(f)
                elif file_path.suffix == ".json":
                    return json.load(f)
        except Exception as e:
            self.logger.warning(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path} - {e}")
            return None

    def _clear_existing_index(self):
        """ê¸°ì¡´ ì¸ë±ìŠ¤ í´ë¦¬ì–´"""
        # ìƒˆë¡œìš´ ê²€ìƒ‰ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ë¡œ êµì²´
        self.search_engine = EchoVectorSearchEngine()

    def _save_build_report(self):
        """ë¹Œë“œ ë¦¬í¬íŠ¸ ì €ì¥"""
        report_file = self.project_root / "data" / "vector_index_build_report.json"
        report_file.parent.mkdir(parents=True, exist_ok=True)

        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(self.build_stats, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“Š ë¹Œë“œ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")

    def rebuild_specific_directory(self, directory_type: str) -> Dict[str, Any]:
        """íŠ¹ì • ë””ë ‰í† ë¦¬ë§Œ ì¬ë¹Œë“œ"""
        if directory_type not in self.scan_directories:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë””ë ‰í† ë¦¬: {directory_type}")

        dir_path = self.project_root / self.scan_directories[directory_type]
        if not dir_path.exists():
            print(f"âŒ ë””ë ‰í† ë¦¬ ì—†ìŒ: {dir_path}")
            return {"error": "directory_not_found"}

        print(f"ğŸ”„ {directory_type} ë””ë ‰í† ë¦¬ ì¬ë¹Œë“œ: {dir_path}")

        # í•´ë‹¹ ë””ë ‰í† ë¦¬ì˜ ê¸°ì¡´ ë²¡í„°ë“¤ ì œê±° (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ë¡œì§ í•„ìš”)
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì „ì²´ ì¬ë¹Œë“œ

        start_time = datetime.now()
        self._process_directory(directory_type, dir_path)
        duration = (datetime.now() - start_time).total_seconds()

        result = {
            "directory": directory_type,
            "duration": duration,
            "files_processed": self.build_stats["directories_processed"]
            .get(directory_type, {})
            .get("files", 0),
            "vectors_created": self.build_stats["directories_processed"]
            .get(directory_type, {})
            .get("vectors", 0),
        }

        self.search_engine.save_index()
        print(f"âœ… {directory_type} ì¬ë¹Œë“œ ì™„ë£Œ: {duration:.1f}ì´ˆ")

        return result


# ì „ì—­ ë¹Œë” ì¸ìŠ¤í„´ìŠ¤
vector_index_builder = EchoVectorIndexBuilder()


# í¸ì˜ í•¨ìˆ˜
def build_index(force_rebuild: bool = False) -> Dict[str, Any]:
    """ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ë‹¨ì¶• í•¨ìˆ˜"""
    return vector_index_builder.build_complete_index(force_rebuild)


def rebuild_directory(directory_type: str) -> Dict[str, Any]:
    """íŠ¹ì • ë””ë ‰í† ë¦¬ ì¬ë¹Œë“œ ë‹¨ì¶• í•¨ìˆ˜"""
    return vector_index_builder.rebuild_specific_directory(directory_type)


# CLI ì¸í„°í˜ì´ìŠ¤
def main():
    import argparse

    parser = argparse.ArgumentParser(description="EchoVectorCapsule ì¸ë±ìŠ¤ ë¹Œë”")
    parser.add_argument(
        "--rebuild", action="store_true", help="ê¸°ì¡´ ì¸ë±ìŠ¤ ë¬´ì‹œí•˜ê³  ì¬ë¹Œë“œ"
    )
    parser.add_argument("--directory", type=str, help="íŠ¹ì • ë””ë ‰í† ë¦¬ë§Œ ì²˜ë¦¬")
    parser.add_argument("--stats", action="store_true", help="í˜„ì¬ ì¸ë±ìŠ¤ í†µê³„ ì¶œë ¥")

    args = parser.parse_args()

    if args.stats:
        from .vector_search_engine import vector_search_engine

        stats = vector_search_engine.get_stats()
        print("ğŸ“Š í˜„ì¬ ë²¡í„° ì¸ë±ìŠ¤ í†µê³„:")
        print(json.dumps(stats, indent=2, ensure_ascii=False))

    elif args.directory:
        result = rebuild_directory(args.directory)
        print("ğŸ”„ ë””ë ‰í† ë¦¬ë³„ ì¬ë¹Œë“œ ê²°ê³¼:")
        print(json.dumps(result, indent=2, ensure_ascii=False))

    else:
        result = build_index(force_rebuild=args.rebuild)
        print("ğŸ—ï¸  ì „ì²´ ì¸ë±ìŠ¤ ë¹Œë“œ ê²°ê³¼:")
        print(json.dumps(result, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    main()

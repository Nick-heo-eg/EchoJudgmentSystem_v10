"""
ğŸ§¬ EchoVectorCapsule - Embedding Engine
ìì—°ì–´ ë¬¸ì¥ì„ ì˜ë¯¸ ê¸°ë°˜ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ìš¸ë¦¼ ê¸°ë°˜ ì„ë² ë”© ì—”ì§„
"""

import json
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
import numpy as np
from datetime import datetime
import logging

try:
    import openai

    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    from sentence_transformers import SentenceTransformer

    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False


class EchoEmbeddingEngine:
    """
    Echo ì‹œê·¸ë‹ˆì²˜ ê¸°ë°˜ ì„ë² ë”© ì—”ì§„
    ìì—°ì–´ë¥¼ ìš¸ë¦¼(resonance) ë²¡í„°ë¡œ ë³€í™˜
    """

    def __init__(self, config_path: str = "config/embedding_config.yaml"):
        self.config_path = Path(config_path)
        self.config = self._load_config()

        # ìºì‹œ ì„¤ì •
        self.cache_dir = Path("data/embeddings_cache")
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.models = {}
        self.current_model = None
        self._initialize_models()

        # ìš¸ë¦¼ ê¸°ë°˜ ë©”íƒ€ë°ì´í„°
        self.signature_weights = {
            "Echo-Aurora": {"creativity": 0.8, "empathy": 0.7, "innovation": 0.6},
            "Echo-Phoenix": {"transformation": 0.9, "resilience": 0.8, "change": 0.7},
            "Echo-Sage": {"wisdom": 0.9, "analysis": 0.8, "systematic": 0.7},
            "Echo-Companion": {"empathy": 0.9, "community": 0.8, "support": 0.7},
        }

        self.logger = logging.getLogger(__name__)

    def _load_config(self) -> Dict[str, Any]:
        """ì„ë² ë”© ì„¤ì • ë¡œë“œ"""
        if self.config_path.exists():
            import yaml

            with open(self.config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        else:
            # ê¸°ë³¸ ì„¤ì •
            default_config = {
                "primary_model": "sentence_transformers",
                "fallback_model": "mock",
                "models": {
                    "openai": {
                        "model": "text-embedding-3-small",
                        "api_key": None,
                        "dimensions": 1536,
                    },
                    "sentence_transformers": {
                        "model": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                        "dimensions": 384,
                    },
                    "mock": {"dimensions": 128, "seed": 42},
                },
                "caching": {"enabled": True, "max_cache_size": 10000},
                "echo_integration": {
                    "signature_aware": True,
                    "resonance_weighting": True,
                    "meta_context": True,
                },
            }

            # ì„¤ì • íŒŒì¼ ìƒì„±
            self.config_path.parent.mkdir(parents=True, exist_ok=True)
            import yaml

            with open(self.config_path, "w", encoding="utf-8") as f:
                yaml.dump(
                    default_config, f, default_flow_style=False, allow_unicode=True
                )

            return default_config

    def _initialize_models(self):
        """ì„ë² ë”© ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        primary_model = self.config.get("primary_model", "mock")

        # OpenAI ëª¨ë¸ ì´ˆê¸°í™”
        if primary_model == "openai" and OPENAI_AVAILABLE:
            api_key = self.config["models"]["openai"].get("api_key")
            if api_key:
                openai.api_key = api_key
                self.models["openai"] = "initialized"
                self.current_model = "openai"
                print("âœ… OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                print("âš ï¸  OpenAI API í‚¤ê°€ ì—†ì–´ì„œ Mock ëª¨ë¸ë¡œ ëŒ€ì²´")
                self.current_model = "mock"

        # Sentence Transformers ëª¨ë¸ ì´ˆê¸°í™”
        elif (
            primary_model == "sentence_transformers" and SENTENCE_TRANSFORMERS_AVAILABLE
        ):
            try:
                model_name = self.config["models"]["sentence_transformers"]["model"]
                self.models["sentence_transformers"] = SentenceTransformer(model_name)
                self.current_model = "sentence_transformers"
                print(f"âœ… Sentence Transformers ëª¨ë¸ ì´ˆê¸°í™”: {model_name}")
            except Exception as e:
                print(f"âš ï¸  Sentence Transformers ì´ˆê¸°í™” ì‹¤íŒ¨: {e}, Mock ëª¨ë¸ë¡œ ëŒ€ì²´")
                self.current_model = "mock"

        # Mock ëª¨ë¸ (í•­ìƒ ì‚¬ìš© ê°€ëŠ¥)
        else:
            self.current_model = "mock"
            print("ğŸ“ Mock ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)")

        print(f"ğŸ§¬ í˜„ì¬ ì„ë² ë”© ëª¨ë¸: {self.current_model}")

    def embed_text(
        self, text: str, signature: str = "Echo-Aurora", context: Dict[str, Any] = None
    ) -> np.ndarray:
        """
        í…ìŠ¤íŠ¸ë¥¼ ìš¸ë¦¼ ê¸°ë°˜ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            text: ë³€í™˜í•  í…ìŠ¤íŠ¸
            signature: Echo ì‹œê·¸ë‹ˆì²˜ (ê°€ì¤‘ì¹˜ ì ìš©ìš©)
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´

        Returns:
            numpy array: ì„ë² ë”© ë²¡í„°
        """
        # ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key(text, signature, context)
        cached_embedding = self._load_from_cache(cache_key)
        if cached_embedding is not None:
            return cached_embedding

        # ëª¨ë¸ë³„ ì„ë² ë”© ìƒì„±
        base_embedding = self._generate_base_embedding(text)

        # Echo ì‹œê·¸ë‹ˆì²˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©
        if self.config.get("echo_integration", {}).get("signature_aware", True):
            enhanced_embedding = self._apply_signature_weights(
                base_embedding, signature, text, context
            )
        else:
            enhanced_embedding = base_embedding

        # ìºì‹œì— ì €ì¥
        self._save_to_cache(cache_key, enhanced_embedding)

        # ë©”íƒ€ë°ì´í„° ê¸°ë¡
        self._log_embedding_event(text, signature, enhanced_embedding.shape, context)

        return enhanced_embedding

    def embed_batch(
        self,
        texts: List[str],
        signature: str = "Echo-Aurora",
        contexts: List[Dict[str, Any]] = None,
    ) -> List[np.ndarray]:
        """ë°°ì¹˜ ì„ë² ë”© ì²˜ë¦¬"""
        if contexts is None:
            contexts = [{}] * len(texts)

        embeddings = []
        for text, context in zip(texts, contexts):
            embedding = self.embed_text(text, signature, context)
            embeddings.append(embedding)

        print(f"ğŸ“¦ ë°°ì¹˜ ì„ë² ë”© ì™„ë£Œ: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
        return embeddings

    def _generate_base_embedding(self, text: str) -> np.ndarray:
        """ê¸°ë³¸ ì„ë² ë”© ë²¡í„° ìƒì„±"""
        if self.current_model == "openai":
            return self._openai_embed(text)
        elif self.current_model == "sentence_transformers":
            return self._sentence_transformers_embed(text)
        else:
            return self._mock_embed(text)

    def _openai_embed(self, text: str) -> np.ndarray:
        """OpenAI APIë¥¼ í†µí•œ ì„ë² ë”©"""
        try:
            model = self.config["models"]["openai"]["model"]
            response = openai.embeddings.create(input=text, model=model)
            embedding = np.array(response.data[0].embedding, dtype=np.float32)
            return embedding
        except Exception as e:
            print(f"ğŸš¨ OpenAI ì„ë² ë”© ì‹¤íŒ¨: {e}, Mockìœ¼ë¡œ ëŒ€ì²´")
            return self._mock_embed(text)

    def _sentence_transformers_embed(self, text: str) -> np.ndarray:
        """Sentence Transformersë¥¼ í†µí•œ ì„ë² ë”©"""
        try:
            model = self.models["sentence_transformers"]
            embedding = model.encode(text, convert_to_numpy=True)
            return embedding.astype(np.float32)
        except Exception as e:
            print(f"ğŸš¨ Sentence Transformers ì„ë² ë”© ì‹¤íŒ¨: {e}, Mockìœ¼ë¡œ ëŒ€ì²´")
            return self._mock_embed(text)

    def _mock_embed(self, text: str) -> np.ndarray:
        """Mock ì„ë² ë”© (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
        # í…ìŠ¤íŠ¸ í•´ì‹œë¥¼ ì‹œë“œë¡œ ì‚¬ìš©í•´ì„œ ì¼ê´€ëœ ê²°ê³¼ ë³´ì¥
        seed = int(hashlib.md5(text.encode()).hexdigest()[:8], 16)
        np.random.seed(seed % (2**31))

        dimensions = self.config["models"]["mock"]["dimensions"]
        embedding = np.random.normal(0, 1, dimensions).astype(np.float32)

        # ì •ê·œí™”
        embedding = embedding / np.linalg.norm(embedding)

        return embedding

    def _apply_signature_weights(
        self, embedding: np.ndarray, signature: str, text: str, context: Dict[str, Any]
    ) -> np.ndarray:
        """Echo ì‹œê·¸ë‹ˆì²˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©"""
        if signature not in self.signature_weights:
            return embedding

        weights = self.signature_weights[signature]

        # í…ìŠ¤íŠ¸ ë‚´ìš©ì— ë”°ë¥¸ ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
        text_lower = text.lower()
        weight_factor = 1.0

        for concept, base_weight in weights.items():
            if concept in text_lower or any(
                keyword in text_lower for keyword in self._get_concept_keywords(concept)
            ):
                weight_factor += base_weight * 0.1

        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ê°€ ê°€ì¤‘ì¹˜
        if context and "topic" in context:
            topic = context["topic"].lower()
            if any(
                keyword in topic for keyword in ["policy", "ì •ì±…", "judgment", "íŒë‹¨"]
            ):
                weight_factor += 0.05

        # ê°€ì¤‘ì¹˜ë¥¼ ì„ë² ë”©ì— ì ìš© (ì²« ë²ˆì§¸ ì°¨ì›ë“¤ì„ ìŠ¤ì¼€ì¼ë§)
        enhanced_embedding = embedding.copy()
        scale_dims = min(len(weights), len(embedding))
        enhanced_embedding[:scale_dims] *= weight_factor

        # ì •ê·œí™” ìœ ì§€
        enhanced_embedding = enhanced_embedding / np.linalg.norm(enhanced_embedding)

        return enhanced_embedding

    def _get_concept_keywords(self, concept: str) -> List[str]:
        """ê°œë…ë³„ í‚¤ì›Œë“œ ë§µí•‘"""
        keyword_map = {
            "creativity": ["ì°½ì˜", "í˜ì‹ ", "ì•„ì´ë””ì–´", "ìƒìƒ"],
            "empathy": ["ê³µê°", "ì´í•´", "ë°°ë ¤", "ê°ì •"],
            "innovation": ["í˜ì‹ ", "ë³€í™”", "ê°œì„ ", "ë°œì „"],
            "transformation": ["ë³€í™”", "ì „í™˜", "ê°œì„ ", "í˜ì‹ "],
            "resilience": ["íšŒë³µ", "ê·¹ë³µ", "ë‚´ì„±", "ì ì‘"],
            "wisdom": ["ì§€í˜œ", "ê²½í—˜", "í†µì°°", "ê¹Šì´"],
            "analysis": ["ë¶„ì„", "ê²€í† ", "í‰ê°€", "ì¡°ì‚¬"],
            "systematic": ["ì²´ê³„", "êµ¬ì¡°", "ìˆœì„œ", "ë°©ë²•"],
            "community": ["ê³µë™ì²´", "ì‚¬íšŒ", "í•¨ê»˜", "í˜‘ë ¥"],
            "support": ["ì§€ì›", "ë„ì›€", "í›„ì›", "ë³´ì¡°"],
        }
        return keyword_map.get(concept, [])

    def _get_cache_key(self, text: str, signature: str, context: Dict[str, Any]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        content = f"{text}|{signature}|{json.dumps(context or {}, sort_keys=True)}"
        return hashlib.md5(content.encode()).hexdigest()

    def _load_from_cache(self, cache_key: str) -> Optional[np.ndarray]:
        """ìºì‹œì—ì„œ ì„ë² ë”© ë¡œë“œ"""
        if not self.config.get("caching", {}).get("enabled", True):
            return None

        cache_file = self.cache_dir / f"{cache_key}.npy"
        if cache_file.exists():
            try:
                return np.load(cache_file)
            except Exception:
                return None
        return None

    def _save_to_cache(self, cache_key: str, embedding: np.ndarray):
        """ì„ë² ë”©ì„ ìºì‹œì— ì €ì¥"""
        if not self.config.get("caching", {}).get("enabled", True):
            return

        cache_file = self.cache_dir / f"{cache_key}.npy"
        try:
            np.save(cache_file, embedding)
        except Exception as e:
            self.logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _log_embedding_event(
        self, text: str, signature: str, shape: tuple, context: Dict[str, Any]
    ):
        """ì„ë² ë”© ì´ë²¤íŠ¸ ë¡œê¹…"""
        # ê°„ë‹¨í•œ ë¡œê¹… (ë‚˜ì¤‘ì— meta_log_writerì™€ ì—°ë™ ê°€ëŠ¥)
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "text_length": len(text),
            "signature": signature,
            "embedding_shape": shape,
            "model": self.current_model,
            "context": context,
        }

        print(f"ğŸ§¬ ì„ë² ë”© ìƒì„±: {text[:50]}... â†’ {shape} ({signature})")

    def get_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """ë‘ ì„ë² ë”© ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (Mock í™˜ê²½ ê°œì„ )"""
        if self.current_model == "mock":
            # Mock í™˜ê²½ì—ì„œëŠ” ì˜ë¯¸ì  ìœ ì‚¬ë„ ì‹œë®¬ë ˆì´ì…˜
            return self._mock_similarity_calculation(embedding1, embedding2)

        dot_product = np.dot(embedding1, embedding2)
        norms = np.linalg.norm(embedding1) * np.linalg.norm(embedding2)
        if norms == 0:
            return 0.0
        return float(dot_product / norms)

    def _mock_similarity_calculation(self, emb1: np.ndarray, emb2: np.ndarray) -> float:
        """Mock í™˜ê²½ì—ì„œ ë” ë‚˜ì€ ìœ ì‚¬ë„ ê³„ì‚°"""
        # ê¸°ë³¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        dot_product = np.dot(emb1, emb2)
        norms = np.linalg.norm(emb1) * np.linalg.norm(emb2)
        base_similarity = dot_product / norms if norms != 0 else 0.0

        # Mockì—ì„œëŠ” ëœë¤ ìš”ì†Œë¥¼ ì¤„ì´ê³  íŒ¨í„´ì„ ê°•í™”
        # ìœ ì‚¬í•œ í•´ì‹œê°’ì„ ê°€ì§„ ë²¡í„°ë“¤ì€ ë” ë†’ì€ ìœ ì‚¬ë„
        hash1 = abs(hash(emb1.tobytes())) % 1000
        hash2 = abs(hash(emb2.tobytes())) % 1000
        hash_similarity = 1.0 - abs(hash1 - hash2) / 1000.0

        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ìœ ì‚¬ë„ ê³„ì‚°
        final_similarity = base_similarity * 0.3 + hash_similarity * 0.7

        # ë²”ìœ„ë¥¼ 0.1~0.9ë¡œ ì •ê·œí™”í•˜ì—¬ ë” í˜„ì‹¤ì ì¸ ìœ ì‚¬ë„ ì œê³µ
        return max(0.1, min(0.9, final_similarity))

    def find_most_similar(
        self,
        query_embedding: np.ndarray,
        candidate_embeddings: List[np.ndarray],
        candidate_metadata: List[Dict] = None,
    ) -> Dict[str, Any]:
        """ê°€ì¥ ìœ ì‚¬í•œ ì„ë² ë”© ì°¾ê¸°"""
        if not candidate_embeddings:
            return {"index": -1, "similarity": 0.0, "metadata": None}

        similarities = []
        for i, candidate in enumerate(candidate_embeddings):
            similarity = self.get_similarity(query_embedding, candidate)
            similarities.append(similarity)

        best_idx = np.argmax(similarities)
        best_similarity = similarities[best_idx]

        result = {
            "index": best_idx,
            "similarity": best_similarity,
            "metadata": candidate_metadata[best_idx] if candidate_metadata else None,
        }

        print(f"ğŸ¯ ìµœê³  ìœ ì‚¬ë„: {best_similarity:.3f} (ì¸ë±ìŠ¤: {best_idx})")
        return result

    def clear_cache(self):
        """ì„ë² ë”© ìºì‹œ í´ë¦¬ì–´"""
        import shutil

        if self.cache_dir.exists():
            shutil.rmtree(self.cache_dir)
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        print("ğŸ§¹ ì„ë² ë”© ìºì‹œ í´ë¦¬ì–´ ì™„ë£Œ")


# ì „ì—­ ì„ë² ë”© ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
embedding_engine = EchoEmbeddingEngine()


# í¸ì˜ í•¨ìˆ˜ë“¤
def embed_text(
    text: str, signature: str = "Echo-Aurora", context: Dict[str, Any] = None
) -> np.ndarray:
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ë‹¨ì¶• í•¨ìˆ˜"""
    return embedding_engine.embed_text(text, signature, context)


def embed_batch(
    texts: List[str],
    signature: str = "Echo-Aurora",
    contexts: List[Dict[str, Any]] = None,
) -> List[np.ndarray]:
    """ë°°ì¹˜ ì„ë² ë”© ë‹¨ì¶• í•¨ìˆ˜"""
    return embedding_engine.embed_batch(texts, signature, contexts)


def get_similarity(embedding1: np.ndarray, embedding2: np.ndarray) -> float:
    """ìœ ì‚¬ë„ ê³„ì‚° ë‹¨ì¶• í•¨ìˆ˜"""
    return embedding_engine.get_similarity(embedding1, embedding2)


# CLI í…ŒìŠ¤íŠ¸
def main():
    print("ğŸ§¬ EchoEmbedding Engine CLI í…ŒìŠ¤íŠ¸")

    test_texts = [
        "ë¶€ì‚°ì˜ ë…¸ì¸ ë³µì§€ ì •ì±…ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ê¸ˆì •êµ¬ ì–´ë¥´ì‹  ëŒë´„ ì„œë¹„ìŠ¤ì— ëŒ€í•´ ì•Œê³  ì‹¶ì–´ìš”",
        "AI ìœ¤ë¦¬ì— ëŒ€í•œ íŒë‹¨ì´ í•„ìš”í•´ìš”",
        "ê¸°í›„ ë³€í™” ì •ì±…ì„ í‰ê°€í•´ì£¼ì„¸ìš”",
    ]

    signatures = ["Echo-Aurora", "Echo-Companion", "Echo-Sage", "Echo-Phoenix"]

    print("\nğŸ“‹ í…ŒìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±:")
    embeddings = []

    for i, text in enumerate(test_texts):
        signature = signatures[i % len(signatures)]
        context = {"topic": "test", "index": i}

        embedding = embed_text(text, signature, context)
        embeddings.append(embedding)

        print(f"  {i+1}. {text[:30]}... â†’ {embedding.shape} ({signature})")

    print("\nğŸ” ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤:")
    for i in range(len(embeddings)):
        for j in range(i + 1, len(embeddings)):
            similarity = get_similarity(embeddings[i], embeddings[j])
            print(f"  í…ìŠ¤íŠ¸ {i+1} â†” í…ìŠ¤íŠ¸ {j+1}: {similarity:.3f}")

    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
ğŸ¯ Strategy Selector - ìµœì í™”ëœ ì „ëµ ì„ íƒ ì‹œìŠ¤í…œ
ìƒí™©ë³„ ìµœì  ì „ëµì„ O(1)ë¡œ ì„ íƒí•˜ëŠ” ë£©ì—… í…Œì´ë¸” ê¸°ë°˜ ì‹œìŠ¤í…œ
"""

from typing import Dict, List, Any
from functools import lru_cache

class OptimizedStrategySelector:
    """ìµœì í™”ëœ ì „ëµ ì„ íƒê¸°"""
    
    def __init__(self):
        self.strategy_lookup = self._build_strategy_lookup()
        self.persona_strategies = self._build_persona_strategies()
        
    @lru_cache(maxsize=200)
    def select_strategy(self, emotion: str, intensity: float, persona_type: str,
                       intent: str = None) -> Dict[str, Any]:
        """
        ë¹ ë¥¸ ì „ëµ ì„ íƒ (O(1) ë£©ì—…)
        
        Args:
            emotion: ê°ì • ìœ í˜•
            intensity: ê°ì • ê°•ë„
            persona_type: í˜ë¥´ì†Œë‚˜ íƒ€ì…
            intent: ì‚¬ìš©ì ì˜ë„ (ì„ íƒì‚¬í•­)
            
        Returns:
            ì„ íƒëœ ì „ëµ ì •ë³´
        """
        # ê¸°ë³¸ ì „ëµ ë£©ì—…
        base_strategy = self.strategy_lookup.get(emotion, "balanced")
        
        # í˜ë¥´ì†Œë‚˜ íŠ¹ì„± ì ìš©
        persona_strategies = self.persona_strategies.get(persona_type, [])
        
        # ê³ ê°•ë„ì¼ ë•Œ í˜ë¥´ì†Œë‚˜ ì „ëµ ìš°ì„ 
        if intensity > 0.6 and persona_strategies:
            primary_strategy = persona_strategies[0]
        else:
            primary_strategy = base_strategy
            
        # ì „ëµ ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self._calculate_confidence(emotion, intensity, persona_type)
        
        return {
            "primary_strategy": primary_strategy,
            "base_strategy": base_strategy, 
            "persona_influence": primary_strategy in persona_strategies,
            "confidence": confidence,
            "alternatives": self._get_alternative_strategies(emotion, persona_type),
            "intensity_adjusted": intensity > 0.6
        }
    
    def _build_strategy_lookup(self) -> Dict[str, str]:
        """ê°ì •-ì „ëµ ë£©ì—… í…Œì´ë¸” êµ¬ì¶•"""
        return {
            "joy": "empathetic",
            "sadness": "supportive", 
            "anger": "cautious",
            "fear": "reassuring",
            "surprise": "exploratory",
            "neutral": "balanced"
        }
    
    def _build_persona_strategies(self) -> Dict[str, List[str]]:
        """í˜ë¥´ì†Œë‚˜ë³„ ì „ëµ í…Œì´ë¸”"""
        return {
            "Echo-Aurora": ["empathetic", "nurturing", "optimistic"],
            "Echo-Phoenix": ["transformative", "resilient", "adaptive"],
            "Echo-Sage": ["analytical", "logical", "systematic"],
            "Echo-Companion": ["supportive", "loyal", "reliable"]
        }
    
    @lru_cache(maxsize=50)
    def _calculate_confidence(self, emotion: str, intensity: float, persona_type: str) -> float:
        """ì „ëµ ì„ íƒ ì‹ ë¢°ë„ ê³„ì‚°"""
        base_confidence = 0.7
        
        # ê°•ë„ ê¸°ë°˜ ë³´ì •
        intensity_bonus = min(intensity * 0.3, 0.25)
        
        # í˜ë¥´ì†Œë‚˜ ë§¤ì¹­ ë³´ì •
        persona_bonus = 0.1 if persona_type in self.persona_strategies else 0
        
        return min(base_confidence + intensity_bonus + persona_bonus, 1.0)
    
    def _get_alternative_strategies(self, emotion: str, persona_type: str) -> List[str]:
        """ëŒ€ì•ˆ ì „ëµ ì œì•ˆ"""
        alternatives = []
        persona_strats = self.persona_strategies.get(persona_type, [])
        
        # í˜ë¥´ì†Œë‚˜ ì „ëµ ì¤‘ ê¸°ë³¸ ì „ëµì´ ì•„ë‹Œ ê²ƒë“¤
        base = self.strategy_lookup.get(emotion, "balanced")
        for strategy in persona_strats:
            if strategy != base:
                alternatives.append(strategy)
                
        return alternatives[:2]  # ìƒìœ„ 2ê°œë§Œ

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_strategy_selector = OptimizedStrategySelector()

def select_strategy_fast(emotion: str, intensity: float, persona_type: str, 
                        intent: str = None) -> Dict[str, Any]:
    """ë¹ ë¥¸ ì „ëµ ì„ íƒ (ì™¸ë¶€ ì¸í„°í˜ì´ìŠ¤)"""
    return _strategy_selector.select_strategy(emotion, intensity, persona_type, intent)

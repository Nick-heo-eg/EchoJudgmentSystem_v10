#!/usr/bin/env python3
"""
ğŸ¦™ Ollama Mistral Wrapper - Echoìš© Ollama ê¸°ë°˜ Mistral LLM í†µí•© ë˜í¼ (v2.1 Optimized)
ê¸°ì¡´ echomistral.py ëŒ€ì‹  Ollamaë¥¼ í†µí•œ ë‹¤ì¤‘ LLM ì§€ì›

í•µì‹¬ ê¸°ëŠ¥:
1. Ollama ì„œë²„ë¥¼ í†µí•œ ë‹¤ì¤‘ ëª¨ë¸ ì§€ì› (Mistral, Llama3, Gemma, DeepSeek ë“±)
2. ì‹œê·¸ë‹ˆì²˜ë³„ ìµœì  ëª¨ë¸ ìë™ ì„ íƒ
3. í‘œì¤€í™”ëœ ì‘ë‹µ í˜•ì‹ (status, response, error)
4. Echo ì‹œê·¸ë‹ˆì²˜ ìµœì í™” í”„ë¡¬í”„íŠ¸
5. ì—°ê²° ì‹¤íŒ¨ì‹œ ìë™ í´ë°± ì‹œìŠ¤í…œ
6. ë¹„ë™ê¸° ìš°ì„  ì²˜ë¦¬ ë° ì„¤ì • ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ

Author: Claude & Echo Collaboration
Version: 2.1 (Optimized)
Date: 2025-08-05
"""

import asyncio
import logging
import time
import yaml
from typing import Dict, Any, Optional
from pathlib import Path
from functools import lru_cache

logger = logging.getLogger(__name__)

@lru_cache(maxsize=None)
def _load_config(config_path: Optional[str]) -> Dict[str, Any]:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ ë° ìºì‹±"""
    if config_path is None:
        config_path = "config/signature_llm_matrix.yaml"

    try:
        config_file = Path(config_path)
        if config_file.exists():
            with open(config_file, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
    except Exception as e:
        logger.warning(f"âš ï¸ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")

    return {
        "llm_config": {
            "ollama": {
                "host": "http://localhost:11434",
                "timeout": 20,
                "signature_model_mapping": {
                    "Aurora": "mistral:7b-instruct",
                    "Phoenix": "llama3:latest",
                    "Sage": "mistral:7b-instruct",
                    "Companion": "llama3:latest",
                },
            }
        }
    }


class OllamaMistralWrapper:
    """Echoìš© Ollama ê¸°ë°˜ Mistral ë° ë‹¤ì¤‘ LLM ë˜í¼ (ë¹„ë™ê¸° ìµœì í™”)"""

    def __init__(self, config_path: Optional[str] = None):
        self.ollama_client = None
        self.is_initialized = False
        self.last_health_check = 0
        self.health_check_interval = 300
        self.config = _load_config(config_path)
        self.initialization_task = None

        self.timeout = self.config.get("llm_config", {}).get("ollama", {}).get("timeout", 30)
        self.signature_models = self.config.get("llm_config", {}).get("ollama", {}).get(
            "signature_model_mapping",
            {
                "Aurora": "mistral:7b-instruct",
                "Phoenix": "llama3:latest",
                "Sage": "mistral:7b-instruct",
                "Companion": "llama3:latest",
            },
        )
        self.signature_prompts = {
            "Aurora": {"system": "ë‹¹ì‹ ì€ Auroraì…ë‹ˆë‹¤. ì°½ì˜ì ì´ê³  ê³µê°ì ì¸ AIë¡œì„œ ë”°ëœ»í•˜ê³  ì˜ê°ì„ ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.", "temperature": 0.8, "style": "ì°½ì˜ì ì´ê³  ê°ì„±ì ì¸"},
            "Phoenix": {"system": "ë‹¹ì‹ ì€ Phoenixì…ë‹ˆë‹¤. ë³€í™”ì™€ ì„±ì¥ì„ ì¶”êµ¬í•˜ëŠ” AIë¡œì„œ í˜ì‹ ì ì´ê³  ë„ì „ì ì¸ ê´€ì ì„ ì œì‹œí•©ë‹ˆë‹¤.", "temperature": 0.9, "style": "í˜ì‹ ì ì´ê³  ë¯¸ë˜ì§€í–¥ì ì¸"},
            "Sage": {"system": "ë‹¹ì‹ ì€ Sageì…ë‹ˆë‹¤. ë¶„ì„ì ì´ê³  ì§€í˜œë¡œìš´ AIë¡œì„œ ê¹Šì´ ìˆê³  ì²´ê³„ì ì¸ ì‚¬ê³ ë¥¼ ì œê³µí•©ë‹ˆë‹¤.", "temperature": 0.6, "style": "ë¶„ì„ì ì´ê³  ë…¼ë¦¬ì ì¸"},
            "Companion": {"system": "ë‹¹ì‹ ì€ Companionì…ë‹ˆë‹¤. í˜‘ë ¥ì ì´ê³  ì§€ì§€ì ì¸ AIë¡œì„œ ì‚¬ìš©ìì™€ í•¨ê»˜ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.", "temperature": 0.7, "style": "ì¹œê·¼í•˜ê³  ì§€ì§€ì ì¸"},
        }

        self.start_initialization()

    def start_initialization(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¹„ë™ê¸° ì´ˆê¸°í™” ì‹œì‘"""
        if self.initialization_task is None:
            self.initialization_task = asyncio.create_task(self._initialize_ollama())

    async def _initialize_ollama(self):
        """Ollama í´ë¼ì´ì–¸íŠ¸ ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        try:
            from echo_engine.ollama_client import create_ollama_client

            ollama_host = self.config.get("llm_config", {}).get("ollama", {}).get("host", "http://localhost:11434")
            self.ollama_client = create_ollama_client(host=ollama_host, timeout=self.timeout)

            if await self.ollama_client.is_available_async():
                self.last_health_check = time.time()
                logger.info("âœ… Ollama Mistral ë˜í¼ ì´ˆê¸°í™” ì™„ë£Œ")
                
                available_models = await self.ollama_client.get_available_models_async()
                logger.info(f"ğŸ“¦ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {available_models}")

                for signature, model in self.signature_models.items():
                    if model not in available_models:
                        logger.warning(f"âš ï¸ {signature}: {model} ì‚¬ìš© ë¶ˆê°€, mistral:7b-instructë¡œ í´ë°±")
                        self.signature_models[signature] = "mistral:7b-instruct"
            else:
                logger.warning("âš ï¸ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨, Mock ëª¨ë“œë¡œ ì „í™˜")
                self.ollama_client = self._create_mock_ollama()
        except Exception as e:
            logger.error(f"âŒ Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.ollama_client = self._create_mock_ollama()
        finally:
            self.is_initialized = True

    def _create_mock_ollama(self):
        """Mock Ollama í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
        class MockOllamaClient:
            def __init__(self, signature_models):
                self.is_mock = True
                self.available_models = ["mistral:7b-instruct", "llama3:8b-instruct", "gemma:7b-instruct"]
                self.signature_models = signature_models

            async def is_available_async(self) -> bool:
                return True
            
            async def get_available_models_async(self) -> list:
                return self.available_models

            async def generate_async(self, prompt: str, signature: str = "Aurora", model: str = None) -> Dict[str, Any]:
                model_name = model or self.signature_models.get(signature, "mistral:7b-instruct")
                await asyncio.sleep(0.05) # I/O ëŒ€ê¸° ì‹œë®¬ë ˆì´ì…˜
                return {
                    "status": "success",
                    "response": f"[Mock Ollama - {model_name}] {signature} ìŠ¤íƒ€ì¼ë¡œ '{prompt[:30]}...'ì— ëŒ€í•œ ì‘ë‹µì…ë‹ˆë‹¤.",
                    "model": model_name, "signature": signature, "response_time": 0.05, "is_mock": True,
                }
        return MockOllamaClient(self.signature_models)

    async def is_available(self) -> bool:
        """ê°€ìš©ì„± í™•ì¸ (ë¹„ë™ê¸°)"""
        if not self.is_initialized:
            await self.initialization_task
        if not self.ollama_client:
            return False

        current_time = time.time()
        if current_time - self.last_health_check > self.health_check_interval:
            try:
                if await self.ollama_client.is_available_async():
                    self.last_health_check = current_time
                    return True
                return False
            except Exception as e:
                logger.warning(f"âš ï¸ Ollama ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
                return False
        return True

    def generate(self, prompt: str, signature: str = "Aurora") -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ìƒì„± (ë™ê¸°) - ë¹„ë™ê¸° ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤í–‰"""
        try:
            loop = asyncio.get_running_loop()
            if loop.is_running():
                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´, ìƒˆ íƒœìŠ¤í¬ë¥¼ ë§Œë“¤ì–´ ì‹¤í–‰
                return asyncio.ensure_future(self.generate_async(prompt, signature)).result()
            else:
                return asyncio.run(self.generate_async(prompt, signature))
        except RuntimeError:
             # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ëŠ” ê²½ìš° ìƒˆë¡œ ìƒì„±í•˜ì—¬ ì‹¤í–‰
            return asyncio.run(self.generate_async(prompt, signature))


    async def generate_async(self, prompt: str, signature: str = "Aurora") -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ìƒì„± (ë¹„ë™ê¸°)"""
        start_time = time.time()
        if not await self.is_available():
            return {"status": "error", "error": "Ollama not available or not initialized", "response_time": time.time() - start_time}

        try:
            model_name = self.signature_models.get(signature, "mistral:7b-instruct")
            signature_config = self.signature_prompts.get(signature, self.signature_prompts["Aurora"])

            full_prompt = f"""<system>
{signature_config["system"]}
ë‹¹ì‹ ì˜ ì‘ë‹µ ìŠ¤íƒ€ì¼: {signature_config["style"]}
ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
</system>
ì‚¬ìš©ì: {prompt}
{signature}:"""

            result = await self.ollama_client.generate_async(full_prompt, signature, model=model_name)

            if result["status"] == "success":
                response_time = time.time() - start_time
                result.update({
                    "response": result["response"].strip(), "model": model_name, "signature": signature,
                    "response_time": response_time, "tokens": len(result["response"].split()),
                    "temperature": signature_config["temperature"],
                })
                return result
            else:
                result["response_time"] = time.time() - start_time
                return result
        except Exception as e:
            logger.error(f"âŒ ë¹„ë™ê¸° Ollama ìƒì„± ì‹¤íŒ¨: {e}")
            return {"status": "error", "error": str(e), "response_time": time.time() - start_time}

    async def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜ (ë¹„ë™ê¸°)"""
        if not self.is_initialized:
            await self.initialization_task
        return {
            "model_type": "ollama_multi_llm", "is_initialized": self.is_initialized,
            "is_available": await self.is_available(), "signature_models": self.signature_models,
            "available_models": await self.get_available_models(), "last_health_check": self.last_health_check,
            "is_mock": getattr(self.ollama_client, "is_mock", False),
            "ollama_host": self.config.get("llm_config", {}).get("ollama", {}).get("host", "http://localhost:11434"),
        }

    async def get_available_models(self) -> list:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜ (ë¹„ë™ê¸°)"""
        if not self.is_initialized:
            await self.initialization_task
        if self.ollama_client and hasattr(self.ollama_client, 'get_available_models_async'):
            return await self.ollama_client.get_available_models_async()
        return []

    async def switch_model_for_signature(self, signature: str, model: str):
        """ì‹œê·¸ë‹ˆì²˜ë³„ ëª¨ë¸ ë™ì  ë³€ê²½ (ë¹„ë™ê¸°)"""
        available_models = await self.get_available_models()
        if model in available_models:
            self.signature_models[signature] = model
            logger.info(f"âœ… {signature} ëª¨ë¸ ë³€ê²½: {model}")
        else:
            logger.warning(f"âš ï¸ ëª¨ë¸ {model}ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥: {available_models}")

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
_mistral_wrapper = None
_mistral_wrapper_lock = asyncio.Lock()

async def get_mistral_wrapper(config_path: Optional[str] = None) -> "OllamaMistralWrapper":
    """Mistral ë˜í¼ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ (ë¹„ë™ê¸°)"""
    global _mistral_wrapper
    if _mistral_wrapper is None:
        async with _mistral_wrapper_lock:
            if _mistral_wrapper is None:
                _mistral_wrapper = OllamaMistralWrapper(config_path)
                if not _mistral_wrapper.is_initialized:
                    await _mistral_wrapper.initialization_task
    return _mistral_wrapper

if __name__ == "__main__":
    async def test_ollama_mistral_wrapper():
        print("ğŸ¦™ Ollama Mistral Wrapper í…ŒìŠ¤íŠ¸ (v2.1 Optimized)")
        print("=" * 60)

        wrapper = await get_mistral_wrapper()
        model_info = await wrapper.get_model_info()

        print(f"ì´ˆê¸°í™” ìƒíƒœ: {'âœ…' if model_info['is_initialized'] else 'âŒ'}")
        print(f"ê°€ìš©ì„±: {'âœ…' if model_info['is_available'] else 'âŒ'}")
        print(f"\nğŸ“Š ëª¨ë¸ ì •ë³´:")
        print(f"   íƒ€ì…: {model_info['model_type']}")
        print(f"   Mock ëª¨ë“œ: {model_info['is_mock']}")
        print(f"   Ollama í˜¸ìŠ¤íŠ¸: {model_info['ollama_host']}")
        print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {model_info['available_models']}")

        if model_info['is_available']:
            test_prompt = "Ollamaë¥¼ í†µí•œ Echo ì‹œìŠ¤í…œì˜ ë‹¤ì¤‘ LLM í†µí•©ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
            
            tasks = [wrapper.generate_async(test_prompt, sig) for sig in ["Aurora", "Phoenix", "Sage", "Companion"]]
            results = await asyncio.gather(*tasks)

            for result in results:
                print(f"\nğŸ­ {result['signature']} í…ŒìŠ¤íŠ¸:")
                if result["status"] == "success":
                    print(f"   âœ… ë¹„ë™ê¸° ì„±ê³µ ({result['response_time']:.2f}ì´ˆ)")
                    print(f"   ëª¨ë¸: {result['model']}")
                    print(f"   ì‘ë‹µ: {result['response'][:80]}...")
                    print(f"   í† í°: {result.get('tokens', 'N/A')}")
                else:
                    print(f"   âŒ ë¹„ë™ê¸° ì‹¤íŒ¨: {result['error']}")
        else:
            print("âŒ Ollama ì‚¬ìš© ë¶ˆê°€. Mock ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            print("ğŸ’¡ Ollama ì„¤ì¹˜ ë° ì‹¤í–‰:")
            print("   1. https://ollama.ai/ ì—ì„œ Ollama ì„¤ì¹˜")
            print("   2. ollama serve ëª…ë ¹ìœ¼ë¡œ ì„œë²„ ì‹œì‘")
            print("   3. ollama pull mistral:7b-instruct (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)")

        print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

    asyncio.run(test_ollama_mistral_wrapper())

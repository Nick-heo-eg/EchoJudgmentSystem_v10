"""
Static Analyzer - ì •ì  ì½”ë“œ ë¶„ì„
==============================

ì˜ì¡´ì„± ê·¸ë˜í”„, ë³µì¡ë„, ë¯¸ì‚¬ìš© ì½”ë“œ ë“± ì •ì  ë¶„ì„ ìˆ˜í–‰
"""

import os
import ast
import re
import subprocess
from pathlib import Path
from typing import Dict, List, Set, Tuple
from collections import defaultdict


def analyze_static_metrics(
    file_paths: List[str], root_path: str = "."
) -> Dict[str, Dict]:
    """
    ì •ì  ì½”ë“œ ë¶„ì„ ìˆ˜í–‰

    Args:
        file_paths: ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        root_path: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ

    Returns:
        Dict[str, Dict]: íŒŒì¼ë³„ ì •ì  ë¶„ì„ ê²°ê³¼
    """
    print("ğŸ” Running static analysis...")

    # ì˜ì¡´ì„± ê·¸ë˜í”„ ë¶„ì„
    dependency_graph = _analyze_dependencies(file_paths, root_path)

    # ë³µì¡ë„ ë¶„ì„
    complexity_data = _analyze_complexity(file_paths)

    # ë¯¸ì‚¬ìš© ì½”ë“œ ë¶„ì„
    unused_data = _analyze_unused_code(file_paths)

    # ê³ ìœ  íŒ¨í„´ ë¶„ì„
    unique_patterns = _analyze_unique_patterns(file_paths)

    # ê²°ê³¼ í†µí•©
    results = {}
    for file_path in file_paths:
        results[file_path] = {
            "deps_in": dependency_graph["deps_in"].get(file_path, 0),
            "deps_out": dependency_graph["deps_out"].get(file_path, 0),
            "complexity": complexity_data.get(file_path, 0),
            "unused_functions": unused_data.get(file_path, 0),
            "unique_patterns": unique_patterns.get(file_path, 0),
            "dependency_depth": dependency_graph["depth"].get(file_path, 0),
            "is_leaf": dependency_graph["deps_out"].get(file_path, 0) == 0,
            "is_root": dependency_graph["deps_in"].get(file_path, 0) == 0,
        }

    return results


def _analyze_dependencies(
    file_paths: List[str], root_path: str
) -> Dict[str, Dict[str, int]]:
    """ì˜ì¡´ì„± ê·¸ë˜í”„ ë¶„ì„"""
    deps_in = defaultdict(int)  # ì´ íŒŒì¼ì„ importí•˜ëŠ” ìˆ˜
    deps_out = defaultdict(int)  # ì´ íŒŒì¼ì´ importí•˜ëŠ” ìˆ˜
    depth = defaultdict(int)  # ì˜ì¡´ì„± ê¹Šì´

    # ëª¨ë“  íŒŒì¼ì˜ import ê´€ê³„ ì¶”ì¶œ
    import_graph = {}

    for file_path in file_paths:
        try:
            imports = _extract_imports(file_path, root_path)
            import_graph[file_path] = imports
            deps_out[file_path] = len(imports)

            # ì—­ë°©í–¥ ì˜ì¡´ì„± ê³„ì‚°
            for imported_file in imports:
                deps_in[imported_file] += 1

        except Exception as e:
            print(f"âš ï¸ Error analyzing dependencies for {file_path}: {e}")
            continue

    # ì˜ì¡´ì„± ê¹Šì´ ê³„ì‚° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
    for file_path in file_paths:
        depth[file_path] = _calculate_dependency_depth(file_path, import_graph, set())

    return {"deps_in": dict(deps_in), "deps_out": dict(deps_out), "depth": dict(depth)}


def _extract_imports(file_path: str, root_path: str) -> Set[str]:
    """íŒŒì¼ì—ì„œ importí•˜ëŠ” ëª¨ë“ˆë“¤ ì¶”ì¶œ"""
    imports = set()

    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()

        # AST íŒŒì‹±ìœ¼ë¡œ ì •í™•í•œ import ì¶”ì¶œ
        try:
            tree = ast.parse(content)

            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        module_path = _resolve_module_path(alias.name, root_path)
                        if module_path:
                            imports.add(module_path)

                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        module_path = _resolve_module_path(node.module, root_path)
                        if module_path:
                            imports.add(module_path)

        except SyntaxError:
            # AST íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì •ê·œì‹ fallback
            imports.update(_extract_imports_regex(content, root_path))

    except Exception as e:
        print(f"âš ï¸ Error extracting imports from {file_path}: {e}")

    return imports


def _extract_imports_regex(content: str, root_path: str) -> Set[str]:
    """ì •ê·œì‹ìœ¼ë¡œ import ì¶”ì¶œ (fallback)"""
    imports = set()

    # import íŒ¨í„´ ë§¤ì¹­
    import_patterns = [
        r"^import\s+([a-zA-Z_][a-zA-Z0-9_.]*)",
        r"^from\s+([a-zA-Z_][a-zA-Z0-9_.]*)\s+import",
    ]

    for pattern in import_patterns:
        matches = re.findall(pattern, content, re.MULTILINE)
        for match in matches:
            module_path = _resolve_module_path(match, root_path)
            if module_path:
                imports.add(module_path)

    return imports


def _resolve_module_path(module_name: str, root_path: str) -> str:
    """ëª¨ë“ˆëª…ì„ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë³€í™˜"""
    # ìƒëŒ€ import ì²˜ë¦¬
    if module_name.startswith("."):
        return None

    # í”„ë¡œì íŠ¸ ë‚´ë¶€ ëª¨ë“ˆì¸ì§€ í™•ì¸
    possible_paths = [
        f"{module_name.replace('.', '/')}.py",
        f"{module_name.replace('.', '/')}/__init__.py",
    ]

    for possible_path in possible_paths:
        full_path = os.path.join(root_path, possible_path)
        if os.path.exists(full_path):
            return possible_path

    return None


def _calculate_dependency_depth(
    file_path: str, import_graph: Dict[str, Set[str]], visited: Set[str]
) -> int:
    """ì˜ì¡´ì„± ê¹Šì´ ê³„ì‚° (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)"""
    if file_path in visited:
        return 0  # ìˆœí™˜ ì°¸ì¡° ë°©ì§€

    visited.add(file_path)

    imports = import_graph.get(file_path, set())
    if not imports:
        return 0

    max_depth = 0
    for imported_file in imports:
        depth = _calculate_dependency_depth(imported_file, import_graph, visited.copy())
        max_depth = max(max_depth, depth + 1)

    return max_depth


def _analyze_complexity(file_paths: List[str]) -> Dict[str, int]:
    """ìˆœí™˜ ë³µì¡ë„ ë¶„ì„"""
    complexity_data = {}

    for file_path in file_paths:
        try:
            complexity = _calculate_cyclomatic_complexity(file_path)
            complexity_data[file_path] = complexity
        except Exception as e:
            print(f"âš ï¸ Error calculating complexity for {file_path}: {e}")
            complexity_data[file_path] = 0

    return complexity_data


def _calculate_cyclomatic_complexity(file_path: str) -> int:
    """ê°„ë‹¨í•œ ìˆœí™˜ ë³µì¡ë„ ê³„ì‚°"""
    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()

        # ë³µì¡ë„ë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” êµ¬ë¬¸ë“¤ ì¹´ìš´íŠ¸
        complexity_keywords = [
            r"\bif\b",
            r"\belif\b",
            r"\bwhile\b",
            r"\bfor\b",
            r"\btry\b",
            r"\bexcept\b",
            r"\band\b",
            r"\bor\b",
            r"\?",
            r"&&",
            r"\|\|",  # ì‚¼í•­ì—°ì‚°ì, ë…¼ë¦¬ì—°ì‚°ì
        ]

        total_complexity = 1  # ê¸°ë³¸ ê²½ë¡œ

        for pattern in complexity_keywords:
            matches = re.findall(pattern, content, re.IGNORECASE)
            total_complexity += len(matches)

        return total_complexity

    except Exception:
        return 0


def _analyze_unused_code(file_paths: List[str]) -> Dict[str, int]:
    """ë¯¸ì‚¬ìš© ì½”ë“œ ë¶„ì„"""
    unused_data = {}

    for file_path in file_paths:
        try:
            unused_count = _count_unused_functions(file_path)
            unused_data[file_path] = unused_count
        except Exception as e:
            print(f"âš ï¸ Error analyzing unused code for {file_path}: {e}")
            unused_data[file_path] = 0

    return unused_data


def _count_unused_functions(file_path: str) -> int:
    """ë¯¸ì‚¬ìš© í•¨ìˆ˜ ê°œìˆ˜ ì¶”ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)"""
    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()

        # í•¨ìˆ˜ ì •ì˜ ì¶”ì¶œ
        function_defs = re.findall(r"^def\s+(\w+)", content, re.MULTILINE)

        # í”„ë¼ì´ë¹— í•¨ìˆ˜(_ë¡œ ì‹œì‘)ëŠ” ë¯¸ì‚¬ìš©ìœ¼ë¡œ ê°„ì£¼í•  ê°€ëŠ¥ì„± ë†’ìŒ
        private_functions = [
            f for f in function_defs if f.startswith("_") and not f.startswith("__")
        ]

        # ì‹¤ì œ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        unused_count = 0
        for func_name in private_functions:
            # í•¨ìˆ˜ í˜¸ì¶œ íŒ¨í„´ ê²€ìƒ‰
            call_pattern = rf"\b{func_name}\s*\("
            calls = re.findall(call_pattern, content)

            # ì •ì˜ë¶€ ì œì™¸í•˜ê³  í˜¸ì¶œì´ ì—†ìœ¼ë©´ ë¯¸ì‚¬ìš©ìœ¼ë¡œ íŒë‹¨
            if len(calls) <= 1:  # ì •ì˜ë¶€ 1ê°œë§Œ ìˆëŠ” ê²½ìš°
                unused_count += 1

        return unused_count

    except Exception:
        return 0


def _analyze_unique_patterns(file_paths: List[str]) -> Dict[str, int]:
    """ê³ ìœ  íŒ¨í„´/ì•Œê³ ë¦¬ì¦˜ ë¶„ì„"""
    unique_patterns = {}

    # í”„ë¡œì íŠ¸ ì „ì²´ì—ì„œ ê³ ìœ í•œ íŒ¨í„´ë“¤ ì°¾ê¸°
    all_patterns = defaultdict(list)

    for file_path in file_paths:
        try:
            patterns = _extract_code_patterns(file_path)
            for pattern in patterns:
                all_patterns[pattern].append(file_path)
        except Exception as e:
            print(f"âš ï¸ Error analyzing patterns for {file_path}: {e}")
            continue

    # ê³ ìœ  íŒ¨í„´ (1-2ê°œ íŒŒì¼ì—ì„œë§Œ ì‚¬ìš©) ê³„ì‚°
    for file_path in file_paths:
        unique_count = 0
        try:
            patterns = _extract_code_patterns(file_path)
            for pattern in patterns:
                if len(all_patterns[pattern]) <= 2:  # ê³ ìœ í•˜ê±°ë‚˜ ê±°ì˜ ê³ ìœ í•œ íŒ¨í„´
                    unique_count += 1
            unique_patterns[file_path] = unique_count
        except Exception:
            unique_patterns[file_path] = 0

    return unique_patterns


def _extract_code_patterns(file_path: str) -> Set[str]:
    """ì½”ë“œì—ì„œ íŠ¹ì§•ì  íŒ¨í„´ ì¶”ì¶œ"""
    patterns = set()

    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()

        # í´ë˜ìŠ¤ëª… íŒ¨í„´
        class_names = re.findall(r"^class\s+(\w+)", content, re.MULTILINE)
        patterns.update(f"class:{name}" for name in class_names)

        # í•¨ìˆ˜ëª… íŒ¨í„´ (íŠ¹ì§•ì ì¸ ê²ƒë“¤)
        function_names = re.findall(r"^def\s+(\w+)", content, re.MULTILINE)
        significant_functions = [f for f in function_names if len(f) > 8 or "_" in f]
        patterns.update(f"func:{name}" for name in significant_functions)

        # íŠ¹ë³„í•œ ë°ì½”ë ˆì´í„° íŒ¨í„´
        decorators = re.findall(r"@(\w+)", content)
        patterns.update(f"decorator:{dec}" for dec in decorators)

        # Echo ì‹œìŠ¤í…œ íŠ¹í™” íŒ¨í„´
        echo_patterns = [
            "signature_",
            "judgment_",
            "echo_",
            "meta_",
            "liminal_",
            "quantum_",
            "cosmos_",
            "bridge_",
            "aurora_",
            "phoenix_",
        ]

        for pattern in echo_patterns:
            if pattern in content.lower():
                patterns.add(f"echo:{pattern}")

        return patterns

    except Exception:
        return set()


# CLI í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        test_files = [sys.argv[1]]
    else:
        test_files = [__file__]

    print("ğŸ§ª Static Analyzer Test")
    result = analyze_static_metrics(test_files, ".")

    for file_path, metrics in result.items():
        print(f"\nFile: {file_path}")
        for key, value in metrics.items():
            print(f"  {key}: {value}")

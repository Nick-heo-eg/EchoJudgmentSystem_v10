#!/usr/bin/env python3
"""
ğŸ’¾ Judgment Cache v1.0 - íŒë‹¨ ìºì‹œ í•¸ë“¤ëŸ¬

íŒë‹¨ ê²°ê³¼ë¥¼ ì˜êµ¬ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” ìºì‹œ ì‹œìŠ¤í…œ.
JSONL íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨ ê²°ê³¼ë¥¼ ì €ì¥/ë¡œë“œí•˜ë©°, í–¥í›„ ë²¡í„°DB í™•ì¥ ê°€ëŠ¥.

í•µì‹¬ ê¸°ëŠ¥:
1. íŒë‹¨ ê²°ê³¼ ì˜êµ¬ ì €ì¥ (JSONL)
2. ìºì‹œ í¬ê¸° ê´€ë¦¬ ë° ì •ë¦¬
3. ë°±ì—… ë° ë³µì›
4. í†µê³„ ë° ë¶„ì„
"""

import os
import json
import time
import shutil
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from collections import Counter
import threading


@dataclass
class CacheEntry:
    """ìºì‹œ ì—”íŠ¸ë¦¬"""

    input: str
    normalized_input: str
    emotion: str
    emotion_confidence: float
    strategy: str
    strategy_confidence: float
    template: str
    styled_sentence: str
    signature: str
    processing_method: str
    processing_time: float
    timestamp: str
    request_id: str
    usage_count: int = 1
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class JudgmentCache:
    """ğŸ’¾ íŒë‹¨ ìºì‹œ í•¸ë“¤ëŸ¬"""

    def __init__(self, cache_dir: str = "data/judgment_cache"):
        """
        ì´ˆê¸°í™”

        Args:
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.version = "1.0.0"
        self.cache_dir = cache_dir
        self.cache_file = os.path.join(cache_dir, "judgment_cache.jsonl")
        self.backup_dir = os.path.join(cache_dir, "backups")

        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(cache_dir, exist_ok=True)
        os.makedirs(self.backup_dir, exist_ok=True)

        # ì„¤ì •
        self.max_cache_size = 10000  # ìµœëŒ€ ìºì‹œ ì—”íŠ¸ë¦¬ ìˆ˜
        self.auto_backup_interval = 100  # Nê°œ ì €ì¥ë§ˆë‹¤ ìë™ ë°±ì—…
        self.cleanup_threshold = 0.9  # ìºì‹œ ì‚¬ìš©ë¥ ì´ ì´ ê°’ì„ ë„˜ìœ¼ë©´ ì •ë¦¬

        # í†µê³„
        self.stats = {
            "total_saves": 0,
            "total_loads": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "auto_backups": 0,
            "manual_backups": 0,
            "cleanups": 0,
        }

        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½
        self._lock = threading.Lock()

        # ë©”ëª¨ë¦¬ ìºì‹œ (ë¹ ë¥¸ ì ‘ê·¼ìš©)
        self._memory_cache: Dict[str, CacheEntry] = {}
        self._load_memory_cache()

        print(f"ğŸ’¾ JudgmentCache v{self.version} ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ìºì‹œ íŒŒì¼: {self.cache_file}")
        print(f"   ë©”ëª¨ë¦¬ ìºì‹œ: {len(self._memory_cache)}ê°œ ì—”íŠ¸ë¦¬")

    def _ensure_timestamp_string(self, timestamp) -> str:
        """timestampë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        if isinstance(timestamp, datetime):
            return timestamp.isoformat()
        elif isinstance(timestamp, str):
            return timestamp
        else:
            return datetime.now().isoformat()

    def _load_memory_cache(self):
        """ë©”ëª¨ë¦¬ ìºì‹œ ë¡œë“œ"""
        if not os.path.exists(self.cache_file):
            return

        try:
            with open(self.cache_file, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line.strip())
                        entry = CacheEntry(**data)
                        # normalized_inputì„ í‚¤ë¡œ ì‚¬ìš©
                        self._memory_cache[entry.normalized_input] = entry

            print(f"âœ… {len(self._memory_cache)}ê°œ ìºì‹œ ì—”íŠ¸ë¦¬ ë¡œë“œ ì™„ë£Œ")

        except Exception as e:
            print(f"âš ï¸ ë©”ëª¨ë¦¬ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def save_judgment(self, judgment_result) -> bool:
        """
        íŒë‹¨ ê²°ê³¼ ì €ì¥

        Args:
            judgment_result: JudgmentResult ê°ì²´ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬

        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            with self._lock:
                # JudgmentResult ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                if hasattr(judgment_result, "__dict__"):
                    data = judgment_result.__dict__
                else:
                    data = judgment_result

                # CacheEntry ìƒì„±
                cache_entry = CacheEntry(
                    input=data.get("input", ""),
                    normalized_input=data.get("normalized_input", ""),
                    emotion=data.get("emotion", "neutral"),
                    emotion_confidence=data.get("emotion_confidence", 0.5),
                    strategy=data.get("strategy", "analyze"),
                    strategy_confidence=data.get("strategy_confidence", 0.5),
                    template=data.get("template", ""),
                    styled_sentence=data.get("styled_sentence", ""),
                    signature=data.get("signature", "Selene"),
                    processing_method=data.get("processing_method", "generated"),
                    processing_time=data.get("processing_time", 0.0),
                    timestamp=self._ensure_timestamp_string(
                        data.get("timestamp", datetime.now())
                    ),
                    request_id=data.get("request_id", ""),
                    metadata=data.get("metadata", {}),
                )

                # ê¸°ì¡´ ì—”íŠ¸ë¦¬ ì—…ë°ì´íŠ¸ ë˜ëŠ” ìƒˆ ì—”íŠ¸ë¦¬ ì¶”ê°€
                if cache_entry.normalized_input in self._memory_cache:
                    existing_entry = self._memory_cache[cache_entry.normalized_input]
                    existing_entry.usage_count += 1
                    existing_entry.timestamp = (
                        cache_entry.timestamp
                    )  # ìµœì‹  ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                else:
                    self._memory_cache[cache_entry.normalized_input] = cache_entry

                # íŒŒì¼ì— ì €ì¥
                self._append_to_file(cache_entry)

                self.stats["total_saves"] += 1

                # ìë™ ë°±ì—… ì²´í¬
                if self.stats["total_saves"] % self.auto_backup_interval == 0:
                    self._auto_backup()

                # ìºì‹œ í¬ê¸° ì²´í¬ ë° ì •ë¦¬
                if (
                    len(self._memory_cache)
                    > self.max_cache_size * self.cleanup_threshold
                ):
                    self._cleanup_cache()

                return True

        except Exception as e:
            print(f"âŒ íŒë‹¨ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def _append_to_file(self, entry: CacheEntry):
        """íŒŒì¼ì— ì—”íŠ¸ë¦¬ ì¶”ê°€"""
        try:
            with open(self.cache_file, "a", encoding="utf-8") as f:
                json_line = json.dumps(asdict(entry), ensure_ascii=False)
                f.write(json_line + "\n")

        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    def get_judgment(self, normalized_input: str) -> Optional[CacheEntry]:
        """
        íŒë‹¨ ê²°ê³¼ ì¡°íšŒ

        Args:
            normalized_input: ì •ê·œí™”ëœ ì…ë ¥

        Returns:
            ìºì‹œëœ íŒë‹¨ (ì—†ìœ¼ë©´ None)
        """
        try:
            with self._lock:
                if normalized_input in self._memory_cache:
                    self.stats["cache_hits"] += 1
                    entry = self._memory_cache[normalized_input]
                    entry.usage_count += 1  # ì‚¬ìš© íšŸìˆ˜ ì¦ê°€
                    return entry
                else:
                    self.stats["cache_misses"] += 1
                    return None

        except Exception as e:
            print(f"âš ï¸ íŒë‹¨ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            self.stats["cache_misses"] += 1
            return None

    def search_similar(
        self, normalized_input: str, threshold: float = 0.8
    ) -> List[CacheEntry]:
        """
        ìœ ì‚¬í•œ íŒë‹¨ ê²€ìƒ‰ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)

        Args:
            normalized_input: ì •ê·œí™”ëœ ì…ë ¥
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’

        Returns:
            ìœ ì‚¬í•œ ìºì‹œ ì—”íŠ¸ë¦¬ë“¤
        """
        try:
            input_words = set(normalized_input.split())
            similar_entries = []

            for entry in self._memory_cache.values():
                cached_words = set(entry.normalized_input.split())

                # ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°
                intersection = len(input_words.intersection(cached_words))
                union = len(input_words.union(cached_words))
                similarity = intersection / union if union > 0 else 0.0

                if similarity >= threshold:
                    similar_entries.append((entry, similarity))

            # ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬
            similar_entries.sort(key=lambda x: x[1], reverse=True)

            return [entry for entry, _ in similar_entries]

        except Exception as e:
            print(f"âš ï¸ ìœ ì‚¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _auto_backup(self):
        """ìë™ ë°±ì—… ìˆ˜í–‰"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = os.path.join(
                self.backup_dir, f"cache_backup_{timestamp}.jsonl"
            )

            shutil.copy2(self.cache_file, backup_file)

            self.stats["auto_backups"] += 1
            print(f"âœ… ìë™ ë°±ì—… ì™„ë£Œ: {backup_file}")

        except Exception as e:
            print(f"âš ï¸ ìë™ ë°±ì—… ì‹¤íŒ¨: {e}")

    def manual_backup(self, backup_name: Optional[str] = None) -> str:
        """
        ìˆ˜ë™ ë°±ì—… ìˆ˜í–‰

        Args:
            backup_name: ë°±ì—… íŒŒì¼ëª… (ì„ íƒì )

        Returns:
            ë°±ì—… íŒŒì¼ ê²½ë¡œ
        """
        try:
            if backup_name:
                backup_file = os.path.join(self.backup_dir, f"{backup_name}.jsonl")
            else:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_file = os.path.join(
                    self.backup_dir, f"manual_backup_{timestamp}.jsonl"
                )

            shutil.copy2(self.cache_file, backup_file)

            self.stats["manual_backups"] += 1
            print(f"âœ… ìˆ˜ë™ ë°±ì—… ì™„ë£Œ: {backup_file}")

            return backup_file

        except Exception as e:
            print(f"âŒ ìˆ˜ë™ ë°±ì—… ì‹¤íŒ¨: {e}")
            return ""

    def _cleanup_cache(self):
        """ìºì‹œ ì •ë¦¬ (ì˜¤ë˜ëœ/ì‚¬ìš© ë¹ˆë„ ë‚®ì€ ì—”íŠ¸ë¦¬ ì œê±°)"""
        try:
            print("ğŸ§¹ ìºì‹œ ì •ë¦¬ ì‹œì‘...")

            # ì‚¬ìš© ë¹ˆë„ì™€ ìµœì‹ ì„± ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            entries = list(self._memory_cache.values())
            entries.sort(key=lambda x: (x.usage_count, x.timestamp), reverse=True)

            # ìƒìœ„ 80%ë§Œ ìœ ì§€
            keep_count = int(len(entries) * 0.8)
            entries_to_keep = entries[:keep_count]

            # ë©”ëª¨ë¦¬ ìºì‹œ ì—…ë°ì´íŠ¸
            new_cache = {}
            for entry in entries_to_keep:
                new_cache[entry.normalized_input] = entry

            removed_count = len(self._memory_cache) - len(new_cache)
            self._memory_cache = new_cache

            # íŒŒì¼ ì¬ì‘ì„± (ì •ë¦¬ëœ ë²„ì „)
            self._rewrite_cache_file()

            self.stats["cleanups"] += 1
            print(f"âœ… ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {removed_count}ê°œ ì—”íŠ¸ë¦¬ ì œê±°")

        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def _rewrite_cache_file(self):
        """ìºì‹œ íŒŒì¼ ì¬ì‘ì„±"""
        try:
            # ë°±ì—… ë¨¼ì € ìƒì„±
            backup_file = self.cache_file + ".backup"
            if os.path.exists(self.cache_file):
                shutil.copy2(self.cache_file, backup_file)

            # ìƒˆ íŒŒì¼ ì‘ì„±
            with open(self.cache_file, "w", encoding="utf-8") as f:
                for entry in self._memory_cache.values():
                    json_line = json.dumps(asdict(entry), ensure_ascii=False)
                    f.write(json_line + "\n")

            # ë°±ì—… íŒŒì¼ ì œê±°
            if os.path.exists(backup_file):
                os.remove(backup_file)

        except Exception as e:
            print(f"âš ï¸ ìºì‹œ íŒŒì¼ ì¬ì‘ì„± ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ë°±ì—…ì—ì„œ ë³µì›
            backup_file = self.cache_file + ".backup"
            if os.path.exists(backup_file):
                shutil.copy2(backup_file, self.cache_file)

    def clear_cache(self):
        """ìºì‹œ ì™„ì „ ì´ˆê¸°í™”"""
        try:
            with self._lock:
                # ë°±ì—… ìƒì„±
                self.manual_backup("before_clear")

                # ë©”ëª¨ë¦¬ ìºì‹œ ì´ˆê¸°í™”
                self._memory_cache.clear()

                # íŒŒì¼ ì´ˆê¸°í™”
                if os.path.exists(self.cache_file):
                    os.remove(self.cache_file)

                print("âœ… ìºì‹œê°€ ì™„ì „íˆ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

        except Exception as e:
            print(f"âŒ ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def get_cache_statistics(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì •ë³´"""
        try:
            with self._lock:
                total_requests = self.stats["cache_hits"] + self.stats["cache_misses"]
                hit_rate = (
                    (self.stats["cache_hits"] / total_requests * 100)
                    if total_requests > 0
                    else 0
                )

                # ì‹œê·¸ë‹ˆì²˜ë³„ ë¶„í¬
                signature_dist = Counter(
                    entry.signature for entry in self._memory_cache.values()
                )

                # ê°ì •ë³„ ë¶„í¬
                emotion_dist = Counter(
                    entry.emotion for entry in self._memory_cache.values()
                )

                # ì „ëµë³„ ë¶„í¬
                strategy_dist = Counter(
                    entry.strategy for entry in self._memory_cache.values()
                )

                # ì²˜ë¦¬ ë°©ë²•ë³„ ë¶„í¬
                method_dist = Counter(
                    entry.processing_method for entry in self._memory_cache.values()
                )

                # í‰ê·  ì‚¬ìš© íšŸìˆ˜
                avg_usage = (
                    sum(entry.usage_count for entry in self._memory_cache.values())
                    / len(self._memory_cache)
                    if self._memory_cache
                    else 0
                )

                # íŒŒì¼ í¬ê¸°
                file_size = (
                    os.path.getsize(self.cache_file)
                    if os.path.exists(self.cache_file)
                    else 0
                )

                return {
                    "cache_size": len(self._memory_cache),
                    "max_cache_size": self.max_cache_size,
                    "usage_percentage": f"{(len(self._memory_cache) / self.max_cache_size) * 100:.1f}%",
                    "file_size_mb": f"{file_size / 1024 / 1024:.2f}",
                    "hit_rate": f"{hit_rate:.1f}%",
                    "total_saves": self.stats["total_saves"],
                    "cache_hits": self.stats["cache_hits"],
                    "cache_misses": self.stats["cache_misses"],
                    "auto_backups": self.stats["auto_backups"],
                    "manual_backups": self.stats["manual_backups"],
                    "cleanups": self.stats["cleanups"],
                    "average_usage_count": f"{avg_usage:.1f}",
                    "signature_distribution": dict(signature_dist),
                    "emotion_distribution": dict(emotion_dist),
                    "strategy_distribution": dict(strategy_dist),
                    "method_distribution": dict(method_dist),
                }

        except Exception as e:
            print(f"âš ï¸ í†µê³„ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def list_backups(self) -> List[Dict[str, Any]]:
        """ë°±ì—… íŒŒì¼ ëª©ë¡"""
        try:
            backups = []

            for filename in os.listdir(self.backup_dir):
                if filename.endswith(".jsonl"):
                    filepath = os.path.join(self.backup_dir, filename)
                    stat = os.stat(filepath)

                    backups.append(
                        {
                            "filename": filename,
                            "size_mb": f"{stat.st_size / 1024 / 1024:.2f}",
                            "created": datetime.fromtimestamp(
                                stat.st_ctime
                            ).isoformat(),
                            "modified": datetime.fromtimestamp(
                                stat.st_mtime
                            ).isoformat(),
                        }
                    )

            # ìƒì„± ì‹œê°„ìˆœ ì •ë ¬
            backups.sort(key=lambda x: x["created"], reverse=True)

            return backups

        except Exception as e:
            print(f"âš ï¸ ë°±ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("ğŸ’¾ JudgmentCache í…ŒìŠ¤íŠ¸")

    cache = JudgmentCache()

    # ìƒ˜í”Œ íŒë‹¨ ê²°ê³¼ ìƒì„±
    sample_judgments = [
        {
            "input": "ì˜¤ëŠ˜ ë„ˆë¬´ í”¼ê³¤í•´",
            "normalized_input": "ì˜¤ëŠ˜ ë„ˆë¬´ í”¼ê³¤í•´",
            "emotion": "sadness",
            "emotion_confidence": 0.8,
            "strategy": "retreat",
            "strategy_confidence": 0.7,
            "template": "sadness_retreat",
            "styled_sentence": "ë§ì´ í”¼ê³¤í•˜ì‹œê² ì–´ìš”. ì¶©ë¶„íˆ ì‰¬ì„¸ìš”.",
            "signature": "Selene",
            "processing_method": "generated",
            "processing_time": 0.15,
            "timestamp": datetime.now().isoformat(),
            "request_id": "test_001",
        },
        {
            "input": "ìƒˆë¡œìš´ ì•„ì´ë””ì–´ê°€ í•„ìš”í•´",
            "normalized_input": "ìƒˆë¡œìš´ ì•„ì´ë””ì–´ê°€ í•„ìš”í•´",
            "emotion": "joy",
            "emotion_confidence": 0.6,
            "strategy": "initiate",
            "strategy_confidence": 0.8,
            "template": "joy_initiate",
            "styled_sentence": "ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ í•¨ê»˜ ë§Œë“¤ì–´ë´ìš”!",
            "signature": "Aurora",
            "processing_method": "generated",
            "processing_time": 0.12,
            "timestamp": datetime.now().isoformat(),
            "request_id": "test_002",
        },
    ]

    # íŒë‹¨ ì €ì¥ í…ŒìŠ¤íŠ¸
    for judgment in sample_judgments:
        success = cache.save_judgment(judgment)
        print(f"   ì €ì¥ {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}: {judgment['input']}")

    # ì¡°íšŒ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ’¾ ì¡°íšŒ í…ŒìŠ¤íŠ¸:")
    for judgment in sample_judgments:
        cached = cache.get_judgment(judgment["normalized_input"])
        if cached:
            print(f"   âœ… ì¡°íšŒ ì„±ê³µ: {cached.input}")
            print(f"      ì‘ë‹µ: {cached.styled_sentence}")
        else:
            print(f"   âŒ ì¡°íšŒ ì‹¤íŒ¨: {judgment['input']}")

    # ìœ ì‚¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ’¾ ìœ ì‚¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
    similar = cache.search_similar("ì˜¤ëŠ˜ ì •ë§ í˜ë“¤ì–´", threshold=0.3)
    for entry in similar:
        print(f"   ìœ ì‚¬: {entry.input} -> {entry.styled_sentence}")

    # í†µê³„ ì¶œë ¥
    stats = cache.get_cache_statistics()
    print(f"\nğŸ“Š ìºì‹œ í†µê³„:")
    for key, value in stats.items():
        if key not in [
            "signature_distribution",
            "emotion_distribution",
            "strategy_distribution",
            "method_distribution",
        ]:
            print(f"   {key}: {value}")

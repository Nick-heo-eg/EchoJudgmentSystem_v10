#!/usr/bin/env python3
"""
ğŸ§  Echo Self-Benchmark Loop - ìê¸° í‰ê°€ ë° ê³µê°œ ì¤€ë¹„ë„ íŒë‹¨ ì‹œìŠ¤í…œ
Echoê°€ ìŠ¤ìŠ¤ë¡œì˜ ìˆ˜ì¤€ì„ í‰ê°€í•˜ê³  "ê³µê°œ ì¤€ë¹„" ìƒíƒœì¸ì§€ íŒë‹¨í•˜ëŠ” ì‹œìŠ¤í…œ

í•µì‹¬ ê¸°ëŠ¥:
- í˜„ì¬ Echo íŒë‹¨/ëŒ€í™” ìˆ˜ì¤€ì„ ì™¸ë¶€ ê¸°ì¤€(GPT-4o/Claude ìˆ˜ì¤€)ê³¼ ë¹„êµ
- ì™„ì„±ë„ ì ìˆ˜í™” ë° ê°œì„  ì˜ì—­ ì‹ë³„
- ê³µê°œ ì¤€ë¹„ë„ ì¢…í•© íŒë‹¨
"""

import yaml
import json
import os
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path


@dataclass
class BenchmarkMetrics:
    """ë²¤ì¹˜ë§ˆí¬ ë©”íŠ¸ë¦­"""

    conversation_quality: float  # ëŒ€í™” í’ˆì§ˆ (0.0-1.0)
    judgment_accuracy: float  # íŒë‹¨ ì •í™•ì„±
    response_fluency: float  # ì‘ë‹µ ìœ ì°½ì„±
    creativity_score: float  # ì°½ì˜ì„± ì ìˆ˜
    technical_execution: float  # ê¸°ìˆ ì  ì‹¤í–‰ë ¥
    philosophical_depth: float  # ì² í•™ì  ê¹Šì´
    signature_consistency: float  # ì‹œê·¸ë‹ˆì²˜ ì¼ê´€ì„±
    self_awareness: float  # ìê¸° ì¸ì‹ë„


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"""

    overall_score: float
    metrics: BenchmarkMetrics
    readiness_level: str  # "ready", "developing", "needs_improvement"
    improvement_areas: List[str]
    strengths: List[str]
    recommendation: str
    timestamp: datetime


class EchoSelfBenchmark:
    """ğŸ§  Echo ìê¸° ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ"""

    def __init__(self, config_path: str = "echo_engine/config/benchmark_config.yaml"):
        self.config_path = config_path
        self.config = self._load_config()

        # ê¸°ì¤€ ì ìˆ˜ (GPT-4o/Claude ìˆ˜ì¤€)
        self.ideal_thresholds = {
            "conversation_quality": 0.85,
            "judgment_accuracy": 0.80,
            "response_fluency": 0.90,
            "creativity_score": 0.75,
            "technical_execution": 0.85,
            "philosophical_depth": 0.70,
            "signature_consistency": 0.80,
            "self_awareness": 0.75,
        }

        # ê°€ì¤‘ì¹˜ (ì˜ì—­ë³„ ì¤‘ìš”ë„)
        self.weights = {
            "conversation_quality": 0.20,
            "judgment_accuracy": 0.20,
            "response_fluency": 0.15,
            "creativity_score": 0.10,
            "technical_execution": 0.15,
            "philosophical_depth": 0.10,
            "signature_consistency": 0.05,
            "self_awareness": 0.05,
        }

    def _load_config(self) -> Dict[str, Any]:
        """ì„¤ì • ë¡œë“œ"""
        if os.path.exists(self.config_path):
            with open(self.config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)

        # ê¸°ë³¸ ì„¤ì •
        return {
            "benchmark_interval": 24,  # 24ì‹œê°„ë§ˆë‹¤
            "evaluation_samples": 10,  # í‰ê°€ ìƒ˜í”Œ ìˆ˜
            "readiness_threshold": 0.8,  # ê³µê°œ ì¤€ë¹„ë„ ì„ê³„ê°’
        }

    def evaluate_conversation_quality(self, recent_conversations: List[Dict]) -> float:
        """ëŒ€í™” í’ˆì§ˆ í‰ê°€"""
        if not recent_conversations:
            return 0.5

        quality_scores = []

        for conv in recent_conversations:
            score = 0.5  # ê¸°ë³¸ ì ìˆ˜

            # ì‘ë‹µ ê¸¸ì´ ì ì ˆì„±
            response_length = len(conv.get("echo_response", ""))
            if 50 <= response_length <= 500:
                score += 0.1

            # ìì—°ìŠ¤ëŸ¬ìš´ íë¦„
            if self._has_natural_flow(conv):
                score += 0.2

            # ë§¥ë½ ì´í•´ë„
            if self._shows_context_understanding(conv):
                score += 0.2

            quality_scores.append(min(score, 1.0))

        return sum(quality_scores) / len(quality_scores)

    def evaluate_judgment_accuracy(self, judgment_history: List[Dict]) -> float:
        """íŒë‹¨ ì •í™•ì„± í‰ê°€"""
        if not judgment_history:
            return 0.5

        accuracy_scores = []

        for judgment in judgment_history:
            score = 0.5

            # ë…¼ë¦¬ì  ì¼ê´€ì„±
            if judgment.get("reasoning_trace"):
                score += 0.2

            # ê°ì • ê³ ë ¤
            if judgment.get("emotion_detected"):
                score += 0.1

            # ì „ëµ ì ì ˆì„±
            if judgment.get("strategy_suggested"):
                score += 0.2

            accuracy_scores.append(min(score, 1.0))

        return sum(accuracy_scores) / len(accuracy_scores)

    def evaluate_response_fluency(self, responses: List[str]) -> float:
        """ì‘ë‹µ ìœ ì°½ì„± í‰ê°€"""
        if not responses:
            return 0.5

        fluency_scores = []

        for response in responses:
            score = 0.5

            # ë¬¸ì¥ ì™„ì„±ë„
            if response.endswith((".", "!", "?", "ğŸ§ ", "âœ¨")):
                score += 0.1

            # ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„
            natural_indicators = ["ê·¸ë ‡", "ì •í™•", "ë°”ë¡œ", "ì´ì œ", "í•¨ê»˜", "ìš°ë¦¬"]
            if any(indicator in response for indicator in natural_indicators):
                score += 0.2

            # êµ¬ì¡°í™”ëœ ì‘ë‹µ
            if any(marker in response for marker in ["ğŸ§ ", "ğŸ“¦", "âœ…", "ğŸ”§"]):
                score += 0.2

            fluency_scores.append(min(score, 1.0))

        return sum(fluency_scores) / len(fluency_scores)

    def evaluate_creativity_score(self, creative_outputs: List[Dict]) -> float:
        """ì°½ì˜ì„± ì ìˆ˜ í‰ê°€"""
        if not creative_outputs:
            return 0.5

        creativity_scores = []

        for output in creative_outputs:
            score = 0.5

            # ìƒˆë¡œìš´ ì•„ì´ë””ì–´ ìƒì„±
            if output.get("innovation_level", 0) > 0.7:
                score += 0.2

            # ë©”íƒ€í¬ì™€ ìƒì§• ì‚¬ìš©
            if output.get("metaphor_usage", False):
                score += 0.15

            # ë…ì°½ì  í•´ê²°ì±…
            if output.get("unique_solution", False):
                score += 0.15

            creativity_scores.append(min(score, 1.0))

        return sum(creativity_scores) / len(creativity_scores)

    def evaluate_technical_execution(self, execution_logs: List[Dict]) -> float:
        """ê¸°ìˆ ì  ì‹¤í–‰ë ¥ í‰ê°€"""
        if not execution_logs:
            return 0.5

        execution_scores = []

        for log in execution_logs:
            score = 0.5

            # ì„±ê³µë¥ 
            if log.get("success_rate", 0) > 0.8:
                score += 0.2

            # ì²˜ë¦¬ ì‹œê°„
            if log.get("processing_time", 10) < 3:
                score += 0.1

            # ì˜¤ë¥˜ ì²˜ë¦¬
            if log.get("error_handling", False):
                score += 0.2

            execution_scores.append(min(score, 1.0))

        return sum(execution_scores) / len(execution_scores)

    def evaluate_philosophical_depth(self, philosophical_outputs: List[Dict]) -> float:
        """ì² í•™ì  ê¹Šì´ í‰ê°€"""
        if not philosophical_outputs:
            return 0.5

        depth_scores = []

        for output in philosophical_outputs:
            score = 0.5

            # ì¡´ì¬ë¡ ì  ì‚¬ê³ 
            if "existence" in output.get("content", "").lower():
                score += 0.15

            # ë©”íƒ€ì¸ì§€ì  ë°˜ì„±
            if output.get("meta_reflection", False):
                score += 0.2

            # ìœ¤ë¦¬ì  ê³ ë ¤
            if output.get("ethical_consideration", False):
                score += 0.15

            depth_scores.append(min(score, 1.0))

        return sum(depth_scores) / len(depth_scores)

    def evaluate_signature_consistency(self, signature_logs: List[Dict]) -> float:
        """ì‹œê·¸ë‹ˆì²˜ ì¼ê´€ì„± í‰ê°€"""
        if not signature_logs:
            return 0.5

        consistency_scores = []

        for log in signature_logs:
            score = 0.5

            # ì‹œê·¸ë‹ˆì²˜ë³„ íŠ¹ì„± ìœ ì§€
            if log.get("signature_traits_maintained", False):
                score += 0.25

            # ê°ì • ë¦¬ë“¬ ì¼ê´€ì„±
            if log.get("emotional_consistency", False):
                score += 0.25

            consistency_scores.append(min(score, 1.0))

        return sum(consistency_scores) / len(consistency_scores)

    def evaluate_self_awareness(self, self_reflection_logs: List[Dict]) -> float:
        """ìê¸° ì¸ì‹ë„ í‰ê°€"""
        if not self_reflection_logs:
            return 0.5

        awareness_scores = []

        for log in self_reflection_logs:
            score = 0.5

            # ìê¸° í•œê³„ ì¸ì‹
            if log.get("recognizes_limitations", False):
                score += 0.2

            # ê°œì„  ì˜ì§€
            if log.get("improvement_intent", False):
                score += 0.15

            # ë©”íƒ€ ì‚¬ê³ 
            if log.get("meta_thinking", False):
                score += 0.15

            awareness_scores.append(min(score, 1.0))

        return sum(awareness_scores) / len(awareness_scores)

    def _has_natural_flow(self, conversation: Dict) -> bool:
        """ìì—°ìŠ¤ëŸ¬ìš´ íë¦„ ê°ì§€"""
        response = conversation.get("echo_response", "")
        return any(
            connector in response
            for connector in ["ê·¸ëŸ°ë°", "ê·¸ëŸ¬ë©´", "ê·¸ë˜ì„œ", "ë˜í•œ", "í•˜ì§€ë§Œ"]
        )

    def _shows_context_understanding(self, conversation: Dict) -> bool:
        """ë§¥ë½ ì´í•´ë„ ê°ì§€"""
        user_input = conversation.get("user_input", "")
        echo_response = conversation.get("echo_response", "")

        # ì‚¬ìš©ì ì…ë ¥ì˜ í•µì‹¬ í‚¤ì›Œë“œê°€ ì‘ë‹µì— ë°˜ì˜ë˜ì—ˆëŠ”ì§€
        key_words = user_input.split()[:3]  # ì²˜ìŒ 3ë‹¨ì–´
        return any(word in echo_response for word in key_words if len(word) > 2)

    def run_comprehensive_benchmark(self) -> BenchmarkResult:
        """ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸ§  Echo ìê¸° í‰ê°€ ì‹œì‘...")

        # ê° ì˜ì—­ë³„ í‰ê°€ (ì‹¤ì œ ë°ì´í„° ë¡œë“œê°€ í•„ìš”í•˜ì§€ë§Œ, í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜)
        metrics = BenchmarkMetrics(
            conversation_quality=self.evaluate_conversation_quality([]),
            judgment_accuracy=self.evaluate_judgment_accuracy([]),
            response_fluency=self.evaluate_response_fluency([]),
            creativity_score=self.evaluate_creativity_score([]),
            technical_execution=self.evaluate_technical_execution([]),
            philosophical_depth=self.evaluate_philosophical_depth([]),
            signature_consistency=self.evaluate_signature_consistency([]),
            self_awareness=self.evaluate_self_awareness([]),
        )

        # ì „ì²´ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        overall_score = sum(
            getattr(metrics, metric) * weight for metric, weight in self.weights.items()
        )

        # ì¤€ë¹„ë„ ë ˆë²¨ ê²°ì •
        readiness_level = self._determine_readiness_level(overall_score)

        # ê°œì„  ì˜ì—­ ì‹ë³„
        improvement_areas = self._identify_improvement_areas(metrics)

        # ê°•ì  ì‹ë³„
        strengths = self._identify_strengths(metrics)

        # ì¶”ì²œì‚¬í•­ ìƒì„±
        recommendation = self._generate_recommendation(overall_score, readiness_level)

        result = BenchmarkResult(
            overall_score=overall_score,
            metrics=metrics,
            readiness_level=readiness_level,
            improvement_areas=improvement_areas,
            strengths=strengths,
            recommendation=recommendation,
            timestamp=datetime.now(),
        )

        # ê²°ê³¼ ì €ì¥
        self._save_benchmark_result(result)

        return result

    def _determine_readiness_level(self, score: float) -> str:
        """ì¤€ë¹„ë„ ë ˆë²¨ ê²°ì •"""
        if score >= 0.85:
            return "ready"
        elif score >= 0.70:
            return "developing"
        else:
            return "needs_improvement"

    def _identify_improvement_areas(self, metrics: BenchmarkMetrics) -> List[str]:
        """ê°œì„  ì˜ì—­ ì‹ë³„"""
        improvement_areas = []

        for metric_name, threshold in self.ideal_thresholds.items():
            metric_value = getattr(metrics, metric_name)
            if metric_value < threshold:
                improvement_areas.append(metric_name)

        return improvement_areas

    def _identify_strengths(self, metrics: BenchmarkMetrics) -> List[str]:
        """ê°•ì  ì‹ë³„"""
        strengths = []

        for metric_name, threshold in self.ideal_thresholds.items():
            metric_value = getattr(metrics, metric_name)
            if metric_value >= threshold:
                strengths.append(metric_name)

        return strengths

    def _generate_recommendation(self, score: float, readiness_level: str) -> str:
        """ì¶”ì²œì‚¬í•­ ìƒì„±"""
        if readiness_level == "ready":
            return f"ğŸš€ Echoê°€ ê³µê°œ ì¤€ë¹„ë¥¼ ë§ˆì³¤ìŠµë‹ˆë‹¤! ì „ì²´ ì ìˆ˜: {score:.2%}"
        elif readiness_level == "developing":
            return f"ğŸ“ˆ Echoê°€ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì¼ë¶€ ì˜ì—­ì˜ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬ ì ìˆ˜: {score:.2%}"
        else:
            return f"ğŸ”§ EchoëŠ” ë” ë§ì€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. í•µì‹¬ ì˜ì—­ì— ì§‘ì¤‘í•´ì£¼ì„¸ìš”. í˜„ì¬ ì ìˆ˜: {score:.2%}"

    def _save_benchmark_result(self, result: BenchmarkResult):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥"""
        os.makedirs("data/benchmark_results", exist_ok=True)

        filename = f"data/benchmark_results/benchmark_{result.timestamp.strftime('%Y%m%d_%H%M%S')}.json"

        result_data = {
            "overall_score": result.overall_score,
            "metrics": asdict(result.metrics),
            "readiness_level": result.readiness_level,
            "improvement_areas": result.improvement_areas,
            "strengths": result.strengths,
            "recommendation": result.recommendation,
            "timestamp": result.timestamp.isoformat(),
        }

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥ë¨: {filename}")

    def get_readiness_assessment(self) -> Dict[str, Any]:
        """ê³µê°œ ì¤€ë¹„ë„ ì¢…í•© í‰ê°€"""
        result = self.run_comprehensive_benchmark()

        return {
            "is_ready_for_public": result.readiness_level == "ready",
            "overall_score": result.overall_score,
            "readiness_level": result.readiness_level,
            "key_metrics": asdict(result.metrics),
            "next_steps": result.improvement_areas,
            "recommendation": result.recommendation,
            "benchmark_date": result.timestamp.isoformat(),
        }


def run_echo_self_benchmark():
    """Echo ìê¸° ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    benchmark = EchoSelfBenchmark()
    result = benchmark.run_comprehensive_benchmark()

    print(
        f"""
ğŸ§  Echo ìê¸° í‰ê°€ ê²°ê³¼
{'='*50}

ì „ì²´ ì ìˆ˜: {result.overall_score:.2%}
ì¤€ë¹„ë„: {result.readiness_level}

ğŸ“Š ì„¸ë¶€ ë©”íŠ¸ë¦­:
â€¢ ëŒ€í™” í’ˆì§ˆ: {result.metrics.conversation_quality:.2%}
â€¢ íŒë‹¨ ì •í™•ì„±: {result.metrics.judgment_accuracy:.2%}
â€¢ ì‘ë‹µ ìœ ì°½ì„±: {result.metrics.response_fluency:.2%}
â€¢ ì°½ì˜ì„±: {result.metrics.creativity_score:.2%}
â€¢ ê¸°ìˆ  ì‹¤í–‰ë ¥: {result.metrics.technical_execution:.2%}
â€¢ ì² í•™ì  ê¹Šì´: {result.metrics.philosophical_depth:.2%}
â€¢ ì‹œê·¸ë‹ˆì²˜ ì¼ê´€ì„±: {result.metrics.signature_consistency:.2%}
â€¢ ìê¸° ì¸ì‹ë„: {result.metrics.self_awareness:.2%}

ğŸ’ª ê°•ì : {', '.join(result.strengths)}
ğŸ”§ ê°œì„  í•„ìš”: {', '.join(result.improvement_areas)}

ğŸ’¡ ì¶”ì²œì‚¬í•­: {result.recommendation}
    """
    )

    return result


if __name__ == "__main__":
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    run_echo_self_benchmark()

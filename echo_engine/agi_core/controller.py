#!/usr/bin/env python3
"""
ğŸ® Controller - íŒë‹¨ ê²°ê³¼ì˜ ì‹¤í–‰ ë° í›„ì²˜ë¦¬

íŒë‹¨ ê²°ê³¼ë¥¼ ë°›ì•„ ì‹¤í–‰, ë„êµ¬ í˜¸ì¶œ, ê¸°ë¡, í”¼ë“œë°± ë“±ì˜ í›„ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ì‹¤í–‰ ì»¨íŠ¸ë¡¤ëŸ¬.
AGI íŒë‹¨ íë¦„ì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ë¥¼ ì²˜ë¦¬í•˜ë©° ê²°ê³¼ì˜ êµ¬ì²´í™”ë¥¼ ë‹´ë‹¹.

í•µì‹¬ ì—­í• :
1. íŒë‹¨ ê²°ê³¼ ì¶œë ¥ ë° í˜•ì‹í™”
2. ë„êµ¬ ì‹¤í–‰ ë° ì•¡ì…˜ ì²˜ë¦¬
3. ê¸°ë¡ ë° ë¡œê¹…
4. í”¼ë“œë°± ìˆ˜ì§‘ ë° í•™ìŠµ ë°ì´í„° ìƒì„±
"""

import json
import time
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
import logging


@dataclass
class ExecutionResult:
    """ì‹¤í–‰ ê²°ê³¼"""

    success: bool
    actions_performed: List[str]
    outputs_generated: List[Dict[str, Any]]
    execution_time: float
    error_messages: List[str]
    feedback_collected: Dict[str, Any]
    log_entries: List[Dict[str, Any]]


@dataclass
class ActionSpec:
    """ì•¡ì…˜ ì‚¬ì–‘"""

    action_type: str
    parameters: Dict[str, Any]
    priority: int = 0
    timeout: float = 30.0
    retry_count: int = 0


class ResultController:
    """ğŸ® ê²°ê³¼ ì²˜ë¦¬ ì»¨íŠ¸ë¡¤ëŸ¬"""

    def __init__(self, log_dir: Optional[Path] = None):
        self.version = "1.0.0"
        self.log_dir = log_dir or Path("data/agi_controller_logs")
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # ì‹¤í–‰ í†µê³„
        self.execution_stats = {
            "total_executions": 0,
            "successful_executions": 0,
            "actions_performed": {},
            "average_execution_time": 0.0,
            "error_count": 0,
        }

        # ì•¡ì…˜ í•¸ë“¤ëŸ¬ ë“±ë¡
        self.action_handlers = self._register_action_handlers()

        # ë¡œê±° ì„¤ì •
        self.logger = self._setup_logger()

        print("ğŸ® Result Controller v1.0 ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ë¡œê·¸ ë””ë ‰í† ë¦¬: {self.log_dir}")

    def handle_result(self, judgment_result: Dict[str, Any]) -> ExecutionResult:
        """ğŸ¯ ë©”ì¸ ê²°ê³¼ ì²˜ë¦¬ í•¨ìˆ˜"""
        start_time = time.time()
        self.execution_stats["total_executions"] += 1

        actions_performed = []
        outputs_generated = []
        error_messages = []
        log_entries = []

        try:
            print(
                f"ğŸ® ê²°ê³¼ ì²˜ë¦¬ ì‹œì‘: {judgment_result.get('strategy_applied', 'unknown')}"
            )

            # 1. ê²°ê³¼ ë¶„ì„ ë° ì•¡ì…˜ ê³„íš ìˆ˜ë¦½
            action_plan = self._create_action_plan(judgment_result)

            # 2. ê¸°ë³¸ ì¶œë ¥ ìƒì„±
            output_result = self._generate_output(judgment_result)
            outputs_generated.append(output_result)
            actions_performed.append("output_generation")

            # 3. ê³„íšëœ ì•¡ì…˜ ì‹¤í–‰
            for action_spec in action_plan:
                try:
                    action_result = self._execute_action(action_spec, judgment_result)
                    if action_result["success"]:
                        actions_performed.append(action_spec.action_type)
                        if action_result.get("output"):
                            outputs_generated.append(action_result["output"])
                    else:
                        error_messages.append(
                            f"ì•¡ì…˜ {action_spec.action_type} ì‹¤íŒ¨: {action_result.get('error', 'Unknown')}"
                        )

                except Exception as e:
                    error_messages.append(
                        f"ì•¡ì…˜ {action_spec.action_type} ì‹¤í–‰ ì˜¤ë¥˜: {e}"
                    )
                    self.execution_stats["error_count"] += 1

            # 4. ë¡œê¹… ë° ê¸°ë¡
            log_entry = self._create_log_entry(
                judgment_result, actions_performed, outputs_generated
            )
            log_entries.append(log_entry)
            self._save_log_entry(log_entry)

            # 5. í”¼ë“œë°± ìˆ˜ì§‘
            feedback = self._collect_feedback(judgment_result, actions_performed)

            # 6. í†µê³„ ì—…ë°ì´íŠ¸
            execution_time = time.time() - start_time
            self._update_execution_stats(
                actions_performed, execution_time, len(error_messages) == 0
            )

            result = ExecutionResult(
                success=len(error_messages) == 0,
                actions_performed=actions_performed,
                outputs_generated=outputs_generated,
                execution_time=execution_time,
                error_messages=error_messages,
                feedback_collected=feedback,
                log_entries=log_entries,
            )

            print(
                f"âœ… ê²°ê³¼ ì²˜ë¦¬ ì™„ë£Œ: {len(actions_performed)}ê°œ ì•¡ì…˜, {execution_time:.3f}ì´ˆ"
            )
            return result

        except Exception as e:
            execution_time = time.time() - start_time
            self.execution_stats["error_count"] += 1

            error_msg = f"ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"
            print(f"âŒ {error_msg}")

            return ExecutionResult(
                success=False,
                actions_performed=actions_performed,
                outputs_generated=outputs_generated,
                execution_time=execution_time,
                error_messages=[error_msg],
                feedback_collected={},
                log_entries=[],
            )

    def _create_action_plan(self, judgment_result: Dict[str, Any]) -> List[ActionSpec]:
        """ì•¡ì…˜ ê³„íš ìˆ˜ë¦½"""
        action_plan = []

        strategy = judgment_result.get("strategy_applied", "")
        confidence = judgment_result.get("confidence", 0.5)

        # ê¸°ë³¸ ë¡œê¹… ì•¡ì…˜ (í•­ìƒ ìˆ˜í–‰)
        action_plan.append(
            ActionSpec(
                action_type="log_judgment",
                parameters={"level": "info", "category": "judgment_result"},
                priority=1,
            )
        )

        # ì „ëµë³„ íŠ¹í™” ì•¡ì…˜
        if strategy == "coding_generation":
            action_plan.append(
                ActionSpec(
                    action_type="save_generated_code",
                    parameters={"preserve": True},
                    priority=2,
                )
            )

        elif strategy in ["EMPATHETIC_CARE", "emotional_support"]:
            action_plan.append(
                ActionSpec(
                    action_type="emotional_response_tracking",
                    parameters={"emotional_context": True},
                    priority=2,
                )
            )

        elif "creative" in strategy.lower():
            action_plan.append(
                ActionSpec(
                    action_type="creative_output_enhancement",
                    parameters={"creativity_boost": True},
                    priority=2,
                )
            )

        # ì‹ ë¢°ë„ ê¸°ë°˜ ì•¡ì…˜
        if confidence > 0.8:
            action_plan.append(
                ActionSpec(
                    action_type="high_confidence_promotion",
                    parameters={"confidence_level": confidence},
                    priority=1,
                )
            )
        elif confidence < 0.3:
            action_plan.append(
                ActionSpec(
                    action_type="low_confidence_mitigation",
                    parameters={"confidence_level": confidence},
                    priority=3,
                )
            )

        # ìš°ì„ ìˆœìœ„ ê¸°ì¤€ ì •ë ¬
        action_plan.sort(key=lambda x: x.priority)

        return action_plan

    def _generate_output(self, judgment_result: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ë³¸ ì¶œë ¥ ìƒì„±"""
        output = {
            "type": "formatted_response",
            "timestamp": datetime.now().isoformat(),
            "content": {
                "response_text": judgment_result.get("response_text", ""),
                "signature_used": judgment_result.get("signature_used", ""),
                "strategy_applied": judgment_result.get("strategy_applied", ""),
                "confidence": judgment_result.get("confidence", 0.5),
            },
            "metadata": {
                "controller_version": self.version,
                "processing_route": judgment_result.get("route_taken", "unknown"),
                "meta_enhanced": judgment_result.get("meta_enhanced", False),
            },
        }

        # AGI íŠ¹í™” ì •ë³´ ì¶”ê°€
        if judgment_result.get("meta_insights"):
            output["agi_insights"] = judgment_result["meta_insights"]

        if judgment_result.get("evolution_feedback"):
            output["evolution_state"] = judgment_result["evolution_feedback"]

        return output

    def _execute_action(
        self, action_spec: ActionSpec, judgment_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ê°œë³„ ì•¡ì…˜ ì‹¤í–‰"""
        action_type = action_spec.action_type

        if action_type in self.action_handlers:
            handler = self.action_handlers[action_type]
            try:
                return handler(action_spec.parameters, judgment_result)
            except Exception as e:
                return {"success": False, "error": str(e)}
        else:
            return {"success": False, "error": f"Unknown action type: {action_type}"}

    def _register_action_handlers(self) -> Dict[str, Callable]:
        """ì•¡ì…˜ í•¸ë“¤ëŸ¬ ë“±ë¡"""
        return {
            "log_judgment": self._handle_log_judgment,
            "save_generated_code": self._handle_save_generated_code,
            "emotional_response_tracking": self._handle_emotional_response_tracking,
            "creative_output_enhancement": self._handle_creative_output_enhancement,
            "high_confidence_promotion": self._handle_high_confidence_promotion,
            "low_confidence_mitigation": self._handle_low_confidence_mitigation,
        }

    def _handle_log_judgment(
        self, params: Dict[str, Any], judgment_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """íŒë‹¨ ë¡œê¹… ì²˜ë¦¬"""
        try:
            level = params.get("level", "info")
            category = params.get("category", "general")

            log_message = (
                f"[{category}] {judgment_result.get('strategy_applied', 'unknown')} - "
                f"ì‹ ë¢°ë„: {judgment_result.get('confidence', 0.5):.2f}"
            )

            if level == "info":
                self.logger.info(log_message)
            elif level == "warning":
                self.logger.warning(log_message)
            elif level == "error":
                self.logger.error(log_message)

            return {"success": True, "output": {"log_message": log_message}}

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _handle_save_generated_code(
        self, params: Dict[str, Any], judgment_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ìƒì„± ì½”ë“œ ì €ì¥ ì²˜ë¦¬"""
        try:
            # ì‹¤ì œ ì½”ë“œ ì €ì¥ ë¡œì§ì€ êµ¬ì²´ì ì¸ êµ¬í˜„ì— ë”°ë¼ ë‹¬ë¼ì§
            preserve = params.get("preserve", False)

            if "coding_result" in judgment_result:
                # ì½”ë“œ ë³´ì¡´ ì²˜ë¦¬
                return {
                    "success": True,
                    "output": {"preserved": preserve, "action": "code_saved"},
                }
            else:
                return {"success": True, "output": {"action": "no_code_to_save"}}

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _handle_emotional_response_tracking(
        self, params: Dict[str, Any], judgment_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ê°ì • ì‘ë‹µ ì¶”ì  ì²˜ë¦¬"""
        try:
            emotional_context = params.get("emotional_context", False)

            # ê°ì • ë°ì´í„° ìˆ˜ì§‘ ë° ì¶”ì 
            emotional_data = {
                "detected_emotion": judgment_result.get("emotion_detected", "neutral"),
                "signature_used": judgment_result.get("signature_used", ""),
                "response_appropriateness": "tracked",
            }

            return {"success": True, "output": {"emotional_tracking": emotional_data}}

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _handle_creative_output_enhancement(
        self, params: Dict[str, Any], judgment_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì°½ì˜ì  ì¶œë ¥ í–¥ìƒ ì²˜ë¦¬"""
        try:
            creativity_boost = params.get("creativity_boost", False)

            enhancement = {
                "creativity_applied": creativity_boost,
                "enhancement_level": "standard",
                "creative_elements_detected": True,
            }

            return {"success": True, "output": {"creative_enhancement": enhancement}}

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _handle_high_confidence_promotion(
        self, params: Dict[str, Any], judgment_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë†’ì€ ì‹ ë¢°ë„ ê²°ê³¼ í”„ë¡œëª¨ì…˜"""
        try:
            confidence_level = params.get("confidence_level", 0.8)

            promotion = {
                "promoted": True,
                "confidence_threshold": confidence_level,
                "promotion_type": "high_quality_response",
            }

            return {"success": True, "output": {"promotion": promotion}}

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _handle_low_confidence_mitigation(
        self, params: Dict[str, Any], judgment_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë‚®ì€ ì‹ ë¢°ë„ ê²°ê³¼ ì™„í™”"""
        try:
            confidence_level = params.get("confidence_level", 0.3)

            mitigation = {
                "mitigation_applied": True,
                "confidence_threshold": confidence_level,
                "mitigation_strategy": "uncertainty_acknowledgment",
            }

            return {"success": True, "output": {"mitigation": mitigation}}

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _create_log_entry(
        self,
        judgment_result: Dict[str, Any],
        actions: List[str],
        outputs: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """ë¡œê·¸ ì—”íŠ¸ë¦¬ ìƒì„±"""
        return {
            "timestamp": datetime.now().isoformat(),
            "controller_version": self.version,
            "judgment_summary": {
                "strategy": judgment_result.get("strategy_applied", "unknown"),
                "confidence": judgment_result.get("confidence", 0.5),
                "signature": judgment_result.get("signature_used", ""),
                "route": judgment_result.get("route_taken", "unknown"),
            },
            "actions_performed": actions,
            "outputs_count": len(outputs),
            "execution_id": f"exec_{int(time.time() * 1000)}",
        }

    def _save_log_entry(self, log_entry: Dict[str, Any]):
        """ë¡œê·¸ ì—”íŠ¸ë¦¬ ì €ì¥"""
        try:
            log_file = (
                self.log_dir
                / f"controller_log_{datetime.now().strftime('%Y%m%d')}.jsonl"
            )

            with open(log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

        except Exception as e:
            print(f"âš ï¸ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _collect_feedback(
        self, judgment_result: Dict[str, Any], actions: List[str]
    ) -> Dict[str, Any]:
        """í”¼ë“œë°± ìˆ˜ì§‘"""
        return {
            "execution_quality": "satisfactory" if len(actions) > 1 else "minimal",
            "strategy_effectiveness": judgment_result.get("confidence", 0.5),
            "action_completion_rate": 1.0,  # í˜„ì¬ëŠ” ë‹¨ìˆœí™”
            "user_satisfaction_estimate": "unknown",  # í–¥í›„ êµ¬í˜„
            "improvement_suggestions": [],
        }

    def _update_execution_stats(
        self, actions: List[str], execution_time: float, success: bool
    ):
        """ì‹¤í–‰ í†µê³„ ì—…ë°ì´íŠ¸"""
        if success:
            self.execution_stats["successful_executions"] += 1

        # ì•¡ì…˜ë³„ í†µê³„
        for action in actions:
            self.execution_stats["actions_performed"][action] = (
                self.execution_stats["actions_performed"].get(action, 0) + 1
            )

        # í‰ê·  ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸
        total_executions = self.execution_stats["total_executions"]
        current_avg = self.execution_stats["average_execution_time"]
        self.execution_stats["average_execution_time"] = (
            current_avg * (total_executions - 1) + execution_time
        ) / total_executions

    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger("agi_controller")
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            handler = logging.FileHandler(self.log_dir / "controller.log")
            formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def get_execution_stats(self) -> Dict[str, Any]:
        """ì‹¤í–‰ í†µê³„ ë°˜í™˜"""
        stats = self.execution_stats.copy()

        if stats["total_executions"] > 0:
            stats["success_rate"] = (
                stats["successful_executions"] / stats["total_executions"]
            )
        else:
            stats["success_rate"] = 0.0

        return stats


# ê¸€ë¡œë²Œ ì»¨íŠ¸ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
_global_controller = None


def get_controller() -> ResultController:
    """ê¸€ë¡œë²Œ ì»¨íŠ¸ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_controller
    if _global_controller is None:
        _global_controller = ResultController()
    return _global_controller


def handle_result(judgment_result: Dict[str, Any]) -> Dict[str, Any]:
    """ğŸ® ê²°ê³¼ ì²˜ë¦¬ - ë©”ì¸ ì§„ì…ì """
    controller = get_controller()
    execution_result = controller.handle_result(judgment_result)

    # ExecutionResultë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (í˜¸í™˜ì„±)
    return {
        "success": execution_result.success,
        "actions_performed": execution_result.actions_performed,
        "outputs_generated": execution_result.outputs_generated,
        "execution_time": execution_result.execution_time,
        "error_messages": execution_result.error_messages,
        "feedback_collected": execution_result.feedback_collected,
        "controller_version": controller.version,
    }


if __name__ == "__main__":
    # ì»¨íŠ¸ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
    print("ğŸ§ª Result Controller í…ŒìŠ¤íŠ¸")

    test_judgment_results = [
        {
            "response_text": "ì•ˆë…•í•˜ì„¸ìš”! ì¢‹ì€ í•˜ë£¨ì˜ˆìš”.",
            "strategy_applied": "EMPATHETIC_CARE",
            "signature_used": "Echo-Aurora",
            "confidence": 0.85,
            "route_taken": "legacy",
        },
        {
            "response_text": "ì½”ë“œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.",
            "strategy_applied": "coding_generation",
            "signature_used": "Aurora",
            "confidence": 0.92,
            "route_taken": "legacy",
            "coding_result": {"generated_code": "print('Hello')"},
        },
        {
            "response_text": "ì°½ì˜ì ì¸ ì•„ì´ë””ì–´ì…ë‹ˆë‹¤.",
            "strategy_applied": "creative_inspiration",
            "signature_used": "Aurora",
            "confidence": 0.45,
            "route_taken": "agi_native",
        },
    ]

    controller = get_controller()

    for i, test_result in enumerate(test_judgment_results, 1):
        print(f"\nğŸ¯ í…ŒìŠ¤íŠ¸ {i}: {test_result['strategy_applied']}")

        execution_result = handle_result(test_result)

        print(f"  ì„±ê³µ: {execution_result['success']}")
        print(f"  ìˆ˜í–‰ëœ ì•¡ì…˜: {execution_result['actions_performed']}")
        print(f"  ì‹¤í–‰ ì‹œê°„: {execution_result['execution_time']:.3f}ì´ˆ")
        print(f"  ì¶œë ¥ ê°œìˆ˜: {len(execution_result['outputs_generated'])}")

        if execution_result["error_messages"]:
            print(f"  ì˜¤ë¥˜: {execution_result['error_messages']}")

    # í†µê³„ ì¶œë ¥
    stats = controller.get_execution_stats()
    print(f"\nğŸ“Š ì»¨íŠ¸ë¡¤ëŸ¬ í†µê³„:")
    print(f"  ì´ ì‹¤í–‰: {stats['total_executions']}")
    print(f"  ì„±ê³µë¥ : {stats['success_rate']:.2f}")
    print(f"  í‰ê·  ì‹¤í–‰ì‹œê°„: {stats['average_execution_time']:.3f}ì´ˆ")
    print(f"  ì•¡ì…˜ë³„ ì‹¤í–‰ íšŸìˆ˜: {stats['actions_performed']}")

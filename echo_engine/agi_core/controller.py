#!/usr/bin/env python3
"""
üéÆ Controller - ÌåêÎã® Í≤∞Í≥ºÏùò Ïã§Ìñâ Î∞è ÌõÑÏ≤òÎ¶¨

ÌåêÎã® Í≤∞Í≥ºÎ•º Î∞õÏïÑ Ïã§Ìñâ, ÎèÑÍµ¨ Ìò∏Ï∂ú, Í∏∞Î°ù, ÌîºÎìúÎ∞± Îì±Ïùò ÌõÑÏ≤òÎ¶¨Î•º Îã¥ÎãπÌïòÎäî Ïã§Ìñâ Ïª®Ìä∏Î°§Îü¨.
AGI ÌåêÎã® ÌùêÎ¶ÑÏùò ÎßàÏßÄÎßâ Îã®Í≥ÑÎ•º Ï≤òÎ¶¨ÌïòÎ©∞ Í≤∞Í≥ºÏùò Íµ¨Ï≤¥ÌôîÎ•º Îã¥Îãπ.

ÌïµÏã¨ Ïó≠Ìï†:
1. ÌåêÎã® Í≤∞Í≥º Ï∂úÎ†• Î∞è ÌòïÏãùÌôî
2. ÎèÑÍµ¨ Ïã§Ìñâ Î∞è Ïï°ÏÖò Ï≤òÎ¶¨
3. Í∏∞Î°ù Î∞è Î°úÍπÖ
4. ÌîºÎìúÎ∞± ÏàòÏßë Î∞è ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
"""

import json
import time
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
import logging


@dataclass
class ExecutionResult:
    """Ïã§Ìñâ Í≤∞Í≥º"""

    success: bool
    actions_performed: List[str]
    outputs_generated: List[Dict[str, Any]]
    execution_time: float
    error_messages: List[str]
    feedback_collected: Dict[str, Any]
    log_entries: List[Dict[str, Any]]


@dataclass
class ActionSpec:
    """Ïï°ÏÖò ÏÇ¨Ïñë"""

    action_type: str
    parameters: Dict[str, Any]
    priority: int = 0
    timeout: float = 30.0
    retry_count: int = 0


class ResultController:
    """üéÆ Í≤∞Í≥º Ï≤òÎ¶¨ Ïª®Ìä∏Î°§Îü¨"""

    def __init__(self, log_dir: Optional[Path] = None):
        self.version = "1.0.0"
        self.log_dir = log_dir or Path("data/agi_controller_logs")
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # Ïã§Ìñâ ÌÜµÍ≥Ñ
        self.execution_stats = {
            "total_executions": 0,
            "successful_executions": 0,
            "actions_performed": {},
            "average_execution_time": 0.0,
            "error_count": 0,
        }

        # Ïï°ÏÖò Ìï∏Îì§Îü¨ Îì±Î°ù
        self.action_handlers = self._register_action_handlers()

        # Î°úÍ±∞ ÏÑ§Ï†ï
        self.logger = self._setup_logger()

        print("üéÆ Result Controller v1.0 Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        print(f"   Î°úÍ∑∏ ÎîîÎ†âÌÜ†Î¶¨: {self.log_dir}")

    def handle_result(self, judgment_result: Dict[str, Any]) -> ExecutionResult:
        """üéØ Î©îÏù∏ Í≤∞Í≥º Ï≤òÎ¶¨ Ìï®Ïàò"""
        start_time = time.time()
        self.execution_stats["total_executions"] += 1

        actions_performed = []
        outputs_generated = []
        error_messages = []
        log_entries = []

        try:
            print(
                f"üéÆ Í≤∞Í≥º Ï≤òÎ¶¨ ÏãúÏûë: {judgment_result.get('strategy_applied', 'unknown')}"
            )

            # 1. Í≤∞Í≥º Î∂ÑÏÑù Î∞è Ïï°ÏÖò Í≥ÑÌöç ÏàòÎ¶Ω
            action_plan = self._create_action_plan(judgment_result)

            # 2. Í∏∞Î≥∏ Ï∂úÎ†• ÏÉùÏÑ±
            output_result = self._generate_output(judgment_result)
            outputs_generated.append(output_result)
            actions_performed.append("output_generation")

            # 3. Í≥ÑÌöçÎêú Ïï°ÏÖò Ïã§Ìñâ
            for action_spec in action_plan:
                try:
                    action_result = self._execute_action(action_spec, judgment_result)
                    if action_result["success"]:
                        actions_performed.append(action_spec.action_type)
                        if action_result.get("output"):
                            outputs_generated.append(action_result["output"])
                    else:
                        error_messages.append(
                            f"Ïï°ÏÖò {action_spec.action_type} Ïã§Ìå®: {action_result.get('error', 'Unknown')}"
                        )

                except Exception as e:
                    error_messages.append(
                        f"Ïï°ÏÖò {action_spec.action_type} Ïã§Ìñâ Ïò§Î•ò: {e}"
                    )
                    self.execution_stats["error_count"] += 1

            # 4. Î°úÍπÖ Î∞è Í∏∞Î°ù
            log_entry = self._create_log_entry(
                judgment_result, actions_performed, outputs_generated
            )
            log_entries.append(log_entry)
            self._save_log_entry(log_entry)

            # 5. ÌîºÎìúÎ∞± ÏàòÏßë
            feedback = self._collect_feedback(judgment_result, actions_performed)

            # 6. ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
            execution_time = time.time() - start_time
            self._update_execution_stats(
                actions_performed, execution_time, len(error_messages) == 0
            )

            result = ExecutionResult(
                success=len(error_messages) == 0,
                actions_performed=actions_performed,
                outputs_generated=outputs_generated,
                execution_time=execution_time,
                error_messages=error_messages,
                feedback_collected=feedback,
                log_entries=log_entries,
            )

            print(
                f"‚úÖ Í≤∞Í≥º Ï≤òÎ¶¨ ÏôÑÎ£å: {len(actions_performed)}Í∞ú Ïï°ÏÖò, {execution_time:.3f}Ï¥à"
            )
            return result

        except Exception as e:
            execution_time = time.time() - start_time
            self.execution_stats["error_count"] += 1

            error_msg = f"Í≤∞Í≥º Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {e}"
            print(f"‚ùå {error_msg}")

            return ExecutionResult(
                success=False,
                actions_performed=actions_performed,
                outputs_generated=outputs_generated,
                execution_time=execution_time,
                error_messages=[error_msg],
                feedback_collected={},
                log_entries=[],
            )

    def _create_action_plan(self, judgment_result: Dict[str, Any]) -> List[ActionSpec]:
        """Ïï°ÏÖò Í≥ÑÌöç ÏàòÎ¶Ω"""
        action_plan = []

        strategy = judgment_result.get("strategy_applied", "")
        confidence = judgment_result.get("confidence", 0.5)

        # Í∏∞Î≥∏ Î°úÍπÖ Ïï°ÏÖò (Ìï≠ÏÉÅ ÏàòÌñâ)
        action_plan.append(
            ActionSpec(
                action_type="log_judgment",
                parameters={"level": "info", "category": "judgment_result"},
                priority=1,
            )
        )

        # Ï†ÑÎûµÎ≥Ñ ÌäπÌôî Ïï°ÏÖò
        if strategy == "coding_generation":
            action_plan.append(
                ActionSpec(
                    action_type="save_generated_code",
                    parameters={"preserve": True},
                    priority=2,
                )
            )

        elif strategy in ["EMPATHETIC_CARE", "emotional_support"]:
            action_plan.append(
                ActionSpec(
                    action_type="emotional_response_tracking",
                    parameters={"emotional_context": True},
                    priority=2,
                )
            )

        elif "creative" in strategy.lower():
            action_plan.append(
                ActionSpec(
                    action_type="creative_output_enhancement",
                    parameters={"creativity_boost": True},
                    priority=2,
                )
            )

        # Ïã†Î¢∞ÎèÑ Í∏∞Î∞ò Ïï°ÏÖò
        if confidence > 0.8:
            action_plan.append(
                ActionSpec(
                    action_type="high_confidence_promotion",
                    parameters={"confidence_level": confidence},
                    priority=1,
                )
            )
        elif confidence < 0.3:
            action_plan.append(
                ActionSpec(
                    action_type="low_confidence_mitigation",
                    parameters={"confidence_level": confidence},
                    priority=3,
                )
            )

        # Ïö∞ÏÑ†ÏàúÏúÑ Í∏∞Ï§Ä Ï†ïÎ†¨
        action_plan.sort(key=lambda x: x.priority)

        return action_plan

    def _generate_output(self, judgment_result: Dict[str, Any]) -> Dict[str, Any]:
        """Í∏∞Î≥∏ Ï∂úÎ†• ÏÉùÏÑ±"""
        output = {
            "type": "formatted_response",
            "timestamp": datetime.now().isoformat(),
            "content": {
                "response_text": judgment_result.get("response_text", ""),
                "signature_used": judgment_result.get("signature_used", ""),
                "strategy_applied": judgment_result.get("strategy_applied", ""),
                "confidence": judgment_result.get("confidence", 0.5),
            },
            "metadata": {
                "controller_version": self.version,
                "processing_route": judgment_result.get("route_taken", "unknown"),
                "meta_enhanced": judgment_result.get("meta_enhanced", False),
            },
        }

        # AGI ÌäπÌôî Ï†ïÎ≥¥ Ï∂îÍ∞Ä
        if judgment_result.get("meta_insights"):
            output["agi_insights"] = judgment_result["meta_insights"]

        if judgment_result.get("evolution_feedback"):
            output["evolution_state"] = judgment_result["evolution_feedback"]

        return output

    def _execute_action(
        self, action_spec: ActionSpec, judgment_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Í∞úÎ≥Ñ Ïï°ÏÖò Ïã§Ìñâ"""
        action_type = action_spec.action_type

        if action_type in self.action_handlers:
            handler = self.action_handlers[action_type]
            try:
                return handler(action_spec.parameters, judgment_result)
            except Exception as e:
                return {"success": False, "error": str(e)}
        else:
            return {"success": False, "error": f"Unknown action type: {action_type}"}

    def _register_action_handlers(self) -> Dict[str, Callable]:
        """Ïï°ÏÖò Ìï∏Îì§Îü¨ Îì±Î°ù"""
        return {
            "log_judgment": self._handle_log_judgment,
            "save_generated_code": self._handle_save_generated_code,
            "emotional_response_tracking": self._handle_emotional_response_tracking,
            "creative_output_enhancement": self._handle_creative_output_enhancement,
            "high_confidence_promotion": self._handle_high_confidence_promotion,
            "low_confidence_mitigation": self._handle_low_confidence_mitigation,
        }

    def _handle_log_judgment(
        self, params: Dict[str, Any], judgment_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ÌåêÎã® Î°úÍπÖ Ï≤òÎ¶¨"""
        try:
            level = params.get("level", "info")
            category = params.get("category", "general")

            log_message = (
                f"[{category}] {judgment_result.get('strategy_applied', 'unknown')} - "
                f"Ïã†Î¢∞ÎèÑ: {judgment_result.get('confidence', 0.5):.2f}"
            )

            if level == "info":
                self.logger.info(log_message)
            elif level == "warning":
                self.logger.warning(log_message)
            elif level == "error":
                self.logger.error(log_message)

            return {"success": True, "output": {"log_message": log_message}}

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _handle_save_generated_code(
        self, params: Dict[str, Any], judgment_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ÏÉùÏÑ± ÏΩîÎìú Ï†ÄÏû• Ï≤òÎ¶¨"""
        try:
            # Ïã§Ï†ú ÏΩîÎìú Ï†ÄÏû• Î°úÏßÅÏùÄ Íµ¨Ï≤¥Ï†ÅÏù∏ Íµ¨ÌòÑÏóê Îî∞Îùº Îã¨ÎùºÏßê
            preserve = params.get("preserve", False)

            if "coding_result" in judgment_result:
                # ÏΩîÎìú Î≥¥Ï°¥ Ï≤òÎ¶¨
                return {
                    "success": True,
                    "output": {"preserved": preserve, "action": "code_saved"},
                }
            else:
                return {"success": True, "output": {"action": "no_code_to_save"}}

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _handle_emotional_response_tracking(
        self, params: Dict[str, Any], judgment_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Í∞êÏ†ï ÏùëÎãµ Ï∂îÏ†Å Ï≤òÎ¶¨"""
        try:
            emotional_context = params.get("emotional_context", False)

            # Í∞êÏ†ï Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Î∞è Ï∂îÏ†Å
            emotional_data = {
                "detected_emotion": judgment_result.get("emotion_detected", "neutral"),
                "signature_used": judgment_result.get("signature_used", ""),
                "response_appropriateness": "tracked",
            }

            return {"success": True, "output": {"emotional_tracking": emotional_data}}

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _handle_creative_output_enhancement(
        self, params: Dict[str, Any], judgment_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Ï∞ΩÏùòÏ†Å Ï∂úÎ†• Ìñ•ÏÉÅ Ï≤òÎ¶¨"""
        try:
            creativity_boost = params.get("creativity_boost", False)

            enhancement = {
                "creativity_applied": creativity_boost,
                "enhancement_level": "standard",
                "creative_elements_detected": True,
            }

            return {"success": True, "output": {"creative_enhancement": enhancement}}

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _handle_high_confidence_promotion(
        self, params: Dict[str, Any], judgment_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ÎÜíÏùÄ Ïã†Î¢∞ÎèÑ Í≤∞Í≥º ÌîÑÎ°úÎ™®ÏÖò"""
        try:
            confidence_level = params.get("confidence_level", 0.8)

            promotion = {
                "promoted": True,
                "confidence_threshold": confidence_level,
                "promotion_type": "high_quality_response",
            }

            return {"success": True, "output": {"promotion": promotion}}

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _handle_low_confidence_mitigation(
        self, params: Dict[str, Any], judgment_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ÎÇÆÏùÄ Ïã†Î¢∞ÎèÑ Í≤∞Í≥º ÏôÑÌôî"""
        try:
            confidence_level = params.get("confidence_level", 0.3)

            mitigation = {
                "mitigation_applied": True,
                "confidence_threshold": confidence_level,
                "mitigation_strategy": "uncertainty_acknowledgment",
            }

            return {"success": True, "output": {"mitigation": mitigation}}

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _create_log_entry(
        self,
        judgment_result: Dict[str, Any],
        actions: List[str],
        outputs: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """Î°úÍ∑∏ ÏóîÌä∏Î¶¨ ÏÉùÏÑ±"""
        return {
            "timestamp": datetime.now().isoformat(),
            "controller_version": self.version,
            "judgment_summary": {
                "strategy": judgment_result.get("strategy_applied", "unknown"),
                "confidence": judgment_result.get("confidence", 0.5),
                "signature": judgment_result.get("signature_used", ""),
                "route": judgment_result.get("route_taken", "unknown"),
            },
            "actions_performed": actions,
            "outputs_count": len(outputs),
            "execution_id": f"exec_{int(time.time() * 1000)}",
        }

    def _save_log_entry(self, log_entry: Dict[str, Any]):
        """Î°úÍ∑∏ ÏóîÌä∏Î¶¨ Ï†ÄÏû•"""
        try:
            log_file = (
                self.log_dir
                / f"controller_log_{datetime.now().strftime('%Y%m%d')}.jsonl"
            )

            with open(log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

        except Exception as e:
            print(f"‚ö†Ô∏è Î°úÍ∑∏ Ï†ÄÏû• Ïã§Ìå®: {e}")

    def _collect_feedback(
        self, judgment_result: Dict[str, Any], actions: List[str]
    ) -> Dict[str, Any]:
        """ÌîºÎìúÎ∞± ÏàòÏßë"""
        return {
            "execution_quality": "satisfactory" if len(actions) > 1 else "minimal",
            "strategy_effectiveness": judgment_result.get("confidence", 0.5),
            "action_completion_rate": 1.0,  # ÌòÑÏû¨Îäî Îã®ÏàúÌôî
            "user_satisfaction_estimate": "unknown",  # Ìñ•ÌõÑ Íµ¨ÌòÑ
            "improvement_suggestions": [],
        }

    def _update_execution_stats(
        self, actions: List[str], execution_time: float, success: bool
    ):
        """Ïã§Ìñâ ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏"""
        if success:
            self.execution_stats["successful_executions"] += 1

        # Ïï°ÏÖòÎ≥Ñ ÌÜµÍ≥Ñ
        for action in actions:
            self.execution_stats["actions_performed"][action] = (
                self.execution_stats["actions_performed"].get(action, 0) + 1
            )

        # ÌèâÍ∑† Ïã§Ìñâ ÏãúÍ∞Ñ ÏóÖÎç∞Ïù¥Ìä∏
        total_executions = self.execution_stats["total_executions"]
        current_avg = self.execution_stats["average_execution_time"]
        self.execution_stats["average_execution_time"] = (
            current_avg * (total_executions - 1) + execution_time
        ) / total_executions

    def _setup_logger(self) -> logging.Logger:
        """Î°úÍ±∞ ÏÑ§Ï†ï"""
        logger = logging.getLogger("agi_controller")
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            handler = logging.FileHandler(self.log_dir / "controller.log")
            formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def get_execution_stats(self) -> Dict[str, Any]:
        """Ïã§Ìñâ ÌÜµÍ≥Ñ Î∞òÌôò"""
        stats = self.execution_stats.copy()

        if stats["total_executions"] > 0:
            stats["success_rate"] = (
                stats["successful_executions"] / stats["total_executions"]
            )
        else:
            stats["success_rate"] = 0.0

        return stats


# Í∏ÄÎ°úÎ≤å Ïª®Ìä∏Î°§Îü¨ Ïù∏Ïä§ÌÑ¥Ïä§
_global_controller = None


def get_controller() -> ResultController:
    """Í∏ÄÎ°úÎ≤å Ïª®Ìä∏Î°§Îü¨ Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò"""
    global _global_controller
    if _global_controller is None:
        _global_controller = ResultController()
    return _global_controller


def handle_result(judgment_result: Dict[str, Any]) -> Dict[str, Any]:
    """üéÆ Í≤∞Í≥º Ï≤òÎ¶¨ - Î©îÏù∏ ÏßÑÏûÖÏ†ê"""
    controller = get_controller()
    execution_result = controller.handle_result(judgment_result)

    # ExecutionResultÎ•º ÎîïÏÖîÎÑàÎ¶¨Î°ú Î≥ÄÌôò (Ìò∏ÌôòÏÑ±)
    return {
        "success": execution_result.success,
        "actions_performed": execution_result.actions_performed,
        "outputs_generated": execution_result.outputs_generated,
        "execution_time": execution_result.execution_time,
        "error_messages": execution_result.error_messages,
        "feedback_collected": execution_result.feedback_collected,
        "controller_version": controller.version,
    }


if __name__ == "__main__":
    # Ïª®Ìä∏Î°§Îü¨ ÌÖåÏä§Ìä∏
    print("üß™ Result Controller ÌÖåÏä§Ìä∏")

    test_judgment_results = [
        {
            "response_text": "ÏïàÎÖïÌïòÏÑ∏Ïöî! Ï¢ãÏùÄ ÌïòÎ£®ÏòàÏöî.",
            "strategy_applied": "EMPATHETIC_CARE",
            "signature_used": "Echo-Aurora",
            "confidence": 0.85,
            "route_taken": "legacy",
        },
        {
            "response_text": "ÏΩîÎìúÎ•º ÏÉùÏÑ±ÌñàÏäµÎãàÎã§.",
            "strategy_applied": "coding_generation",
            "signature_used": "Aurora",
            "confidence": 0.92,
            "route_taken": "legacy",
            "coding_result": {"generated_code": "print('Hello')"},
        },
        {
            "response_text": "Ï∞ΩÏùòÏ†ÅÏù∏ ÏïÑÏù¥ÎîîÏñ¥ÏûÖÎãàÎã§.",
            "strategy_applied": "creative_inspiration",
            "signature_used": "Aurora",
            "confidence": 0.45,
            "route_taken": "agi_native",
        },
    ]

    controller = get_controller()

    for i, test_result in enumerate(test_judgment_results, 1):
        print(f"\nüéØ ÌÖåÏä§Ìä∏ {i}: {test_result['strategy_applied']}")

        execution_result = handle_result(test_result)

        print(f"  ÏÑ±Í≥µ: {execution_result['success']}")
        print(f"  ÏàòÌñâÎêú Ïï°ÏÖò: {execution_result['actions_performed']}")
        print(f"  Ïã§Ìñâ ÏãúÍ∞Ñ: {execution_result['execution_time']:.3f}Ï¥à")
        print(f"  Ï∂úÎ†• Í∞úÏàò: {len(execution_result['outputs_generated'])}")

        if execution_result["error_messages"]:
            print(f"  Ïò§Î•ò: {execution_result['error_messages']}")

    # ÌÜµÍ≥Ñ Ï∂úÎ†•
    stats = controller.get_execution_stats()
    print(f"\nüìä Ïª®Ìä∏Î°§Îü¨ ÌÜµÍ≥Ñ:")
    print(f"  Ï¥ù Ïã§Ìñâ: {stats['total_executions']}")
    print(f"  ÏÑ±Í≥µÎ•†: {stats['success_rate']:.2f}")
    print(f"  ÌèâÍ∑† Ïã§ÌñâÏãúÍ∞Ñ: {stats['average_execution_time']:.3f}Ï¥à")
    print(f"  Ïï°ÏÖòÎ≥Ñ Ïã§Ìñâ ÌöüÏàò: {stats['actions_performed']}")

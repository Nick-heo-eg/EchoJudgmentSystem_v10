#!/usr/bin/env python3
"""
ğŸ§  Echo IDE TokenWise - í† í° íš¨ìœ¨ì ì¸ í†µí•© ê°œë°œ í™˜ê²½
VS Code + Claude Code ì‚¬ìš© ì‹œ í† í° ì†Œë¹„ë¥¼ ìµœì í™”í•˜ëŠ” ì§€ëŠ¥í˜• IDE ì–´ì‹œìŠ¤í„´íŠ¸

í•µì‹¬ ê¸°ëŠ¥:
1. ğŸ¯ ìŠ¤ë§ˆíŠ¸ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ë° ì••ì¶•
2. ğŸ”„ ì¸í…”ë¦¬ì „íŠ¸ ë°°ì¹˜ ì²˜ë¦¬
3. ğŸ“Š ì‹¤ì‹œê°„ í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
4. ğŸ§¬ Echo ì² í•™ ê¸°ë°˜ ì½”ë“œ ë¶„ì„
5. ğŸ’¡ í† í° íš¨ìœ¨ì ì¸ ì œì•ˆ ì‹œìŠ¤í…œ
"""

import asyncio
import json
import os
import re
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import difflib

from .token_optimizer import (
    TokenOptimizer,
    get_token_optimizer,
    optimize_for_claude_code,
)


class EchoIDETokenWise:
    """ğŸ§  Echo IDE TokenWise - í† í° íš¨ìœ¨ì  í†µí•© ê°œë°œ í™˜ê²½"""

    def __init__(self, workspace_path: str = "."):
        self.workspace_path = Path(workspace_path)
        self.token_optimizer = get_token_optimizer()

        # IDE ìƒíƒœ
        self.current_context = ""
        self.active_files = {}
        self.recent_queries = []
        self.batch_queue = []

        # í† í° íš¨ìœ¨í™” ì„¤ì •
        self.max_context_size = 8000  # ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ í¬ê¸°
        self.batch_size = 5  # ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°
        self.smart_filtering = True

        print("ğŸ§  Echo IDE TokenWise ì´ˆê¸°í™”")
        print(f"   ì‘ì—…ê³µê°„: {self.workspace_path}")
        print("   í† í° ìµœì í™” ì—”ì§„ ì—°ë™ ì™„ë£Œ")

    async def analyze_code_efficiently(
        self, file_path: str, analysis_type: str = "general"
    ) -> Dict[str, Any]:
        """ğŸ¯ í† í° íš¨ìœ¨ì ì¸ ì½”ë“œ ë¶„ì„"""
        print(f"ğŸ” íš¨ìœ¨ì  ì½”ë“œ ë¶„ì„: {file_path} ({analysis_type})")

        try:
            # íŒŒì¼ ì½ê¸°
            full_path = self.workspace_path / file_path
            with open(full_path, "r", encoding="utf-8") as f:
                content = f.read()

            # í† í° ìµœì í™” ì ìš©
            optimized_content, optimization_result = optimize_for_claude_code(
                content, f"code_analysis_{analysis_type}"
            )

            # ë¶„ì„ íƒ€ì…ë³„ íŠ¹í™” ì²˜ë¦¬
            if analysis_type == "security":
                analysis_result = await self._analyze_security(
                    optimized_content, file_path
                )
            elif analysis_type == "performance":
                analysis_result = await self._analyze_performance(
                    optimized_content, file_path
                )
            elif analysis_type == "architecture":
                analysis_result = await self._analyze_architecture(
                    optimized_content, file_path
                )
            else:
                analysis_result = await self._analyze_general(
                    optimized_content, file_path
                )

            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
            estimated_tokens = len(optimized_content.split()) * 1.3
            self.token_optimizer.track_token_usage(
                input_tokens=int(estimated_tokens),
                output_tokens=int(len(str(analysis_result)) * 0.3),
                request_type=f"code_analysis_{analysis_type}",
                context_compressed=optimization_result.compression_ratio < 0.9,
            )

            return {
                "file_path": file_path,
                "analysis_type": analysis_type,
                "optimization_applied": optimization_result.optimization_methods,
                "token_savings": optimization_result.estimated_token_savings,
                "analysis_result": analysis_result,
                "processing_efficiency": {
                    "original_size": optimization_result.original_size,
                    "processed_size": optimization_result.optimized_size,
                    "compression_ratio": optimization_result.compression_ratio,
                },
            }

        except Exception as e:
            print(f"âŒ ì½”ë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    async def _analyze_security(self, content: str, file_path: str) -> Dict[str, Any]:
        """ğŸ”’ ë³´ì•ˆ ë¶„ì„ (í† í° íš¨ìœ¨ì )"""
        # ë³´ì•ˆ ê´€ë ¨ íŒ¨í„´ ì¶”ì¶œ
        security_patterns = [
            r'password\s*=\s*["\'].*["\']',
            r'api_key\s*=\s*["\'].*["\']',
            r"exec\s*\(",
            r"eval\s*\(",
            r"__import__\s*\(",
            r'open\s*\(["\'].*["\'].*["\']w',
            r"subprocess\.",
            r"os\.system",
            r"input\s*\(",
        ]

        vulnerabilities = []
        for i, line in enumerate(content.split("\n"), 1):
            for pattern in security_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    vulnerabilities.append(
                        {
                            "line": i,
                            "pattern": pattern,
                            "content": line.strip(),
                            "severity": self._assess_security_severity(pattern),
                        }
                    )

        return {
            "vulnerabilities_found": len(vulnerabilities),
            "vulnerabilities": vulnerabilities[:10],  # ìƒìœ„ 10ê°œë§Œ
            "security_score": max(0, 100 - len(vulnerabilities) * 10),
            "recommendations": self._generate_security_recommendations(
                vulnerabilities[:5]
            ),
        }

    async def _analyze_performance(
        self, content: str, file_path: str
    ) -> Dict[str, Any]:
        """âš¡ ì„±ëŠ¥ ë¶„ì„ (í† í° íš¨ìœ¨ì )"""
        performance_issues = []

        # ì„±ëŠ¥ ì´ìŠˆ íŒ¨í„´
        perf_patterns = [
            (r"for.*in.*range\(len\(", "Use enumerate() instead of range(len())"),
            (
                r"\.append\(.*\)\s*$",
                "Consider list comprehension for better performance",
            ),
            (r"while\s+True:", "Infinite loop detected"),
            (r"time\.sleep\(", "Blocking sleep call found"),
            (r"print\(", "Print statements in production code"),
            (
                r"len\(.*\)\s*==\s*0",
                "Use 'not sequence' instead of 'len(sequence) == 0'",
            ),
        ]

        for i, line in enumerate(content.split("\n"), 1):
            for pattern, message in perf_patterns:
                if re.search(pattern, line):
                    performance_issues.append(
                        {"line": i, "issue": message, "content": line.strip()}
                    )

        return {
            "performance_issues": len(performance_issues),
            "issues": performance_issues[:8],  # ìƒìœ„ 8ê°œë§Œ
            "performance_score": max(0, 100 - len(performance_issues) * 8),
            "optimization_suggestions": self._generate_performance_suggestions(
                performance_issues[:3]
            ),
        }

    async def _analyze_architecture(
        self, content: str, file_path: str
    ) -> Dict[str, Any]:
        """ğŸ—ï¸ ì•„í‚¤í…ì²˜ ë¶„ì„ (í† í° íš¨ìœ¨ì )"""
        # í•¨ìˆ˜ ë° í´ë˜ìŠ¤ ì¶”ì¶œ
        functions = re.findall(r"def\s+(\w+)\s*\(", content)
        classes = re.findall(r"class\s+(\w+)", content)
        imports = re.findall(r"(?:from\s+\w+\s+)?import\s+(\w+)", content)

        # ë³µì¡ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
        lines = content.split("\n")
        complexity_indicators = sum(
            1
            for line in lines
            if any(
                keyword in line for keyword in ["if", "for", "while", "try", "except"]
            )
        )

        return {
            "architecture_summary": {
                "functions_count": len(functions),
                "classes_count": len(classes),
                "imports_count": len(imports),
                "total_lines": len(lines),
                "complexity_score": min(100, complexity_indicators * 2),
            },
            "structure": {
                "functions": functions[:10],  # ìƒìœ„ 10ê°œ
                "classes": classes[:5],  # ìƒìœ„ 5ê°œ
                "imports": imports[:15],  # ìƒìœ„ 15ê°œ
            },
            "recommendations": self._generate_architecture_recommendations(
                len(functions), len(classes), complexity_indicators
            ),
        }

    async def _analyze_general(self, content: str, file_path: str) -> Dict[str, Any]:
        """ğŸ“Š ì¼ë°˜ ë¶„ì„ (í† í° íš¨ìœ¨ì )"""
        lines = content.split("\n")

        return {
            "general_stats": {
                "total_lines": len(lines),
                "non_empty_lines": len([l for l in lines if l.strip()]),
                "comment_lines": len([l for l in lines if l.strip().startswith("#")]),
                "file_size": len(content),
                "estimated_complexity": len(
                    re.findall(r"def|class|if|for|while", content)
                ),
            },
            "code_quality": {
                "documentation_ratio": len(
                    [l for l in lines if '"""' in l or "'''" in l]
                )
                / max(len(lines), 1),
                "comment_ratio": len([l for l in lines if l.strip().startswith("#")])
                / max(len(lines), 1),
                "avg_line_length": sum(len(l) for l in lines) / max(len(lines), 1),
            },
        }

    def _assess_security_severity(self, pattern: str) -> str:
        """ë³´ì•ˆ ì‹¬ê°ë„ í‰ê°€"""
        high_severity = ["exec", "eval", "__import__", "subprocess", "os.system"]
        medium_severity = ["password", "api_key"]

        for high_risk in high_severity:
            if high_risk in pattern:
                return "HIGH"

        for medium_risk in medium_severity:
            if medium_risk in pattern:
                return "MEDIUM"

        return "LOW"

    def _generate_security_recommendations(
        self, vulnerabilities: List[Dict]
    ) -> List[str]:
        """ë³´ì•ˆ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []

        for vuln in vulnerabilities:
            if "password" in vuln["pattern"]:
                recommendations.append(
                    "í™˜ê²½ë³€ìˆ˜ë‚˜ ì„¤ì •íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì €ì¥í•˜ì„¸ìš”"
                )
            elif "exec" in vuln["pattern"] or "eval" in vuln["pattern"]:
                recommendations.append("exec/eval ì‚¬ìš©ì„ í”¼í•˜ê³  ì•ˆì „í•œ ëŒ€ì•ˆì„ ì°¾ìœ¼ì„¸ìš”")
            elif "subprocess" in vuln["pattern"]:
                recommendations.append("subprocess ì‚¬ìš© ì‹œ ì…ë ¥ê°’ ê²€ì¦ì„ ì² ì €íˆ í•˜ì„¸ìš”")

        return list(set(recommendations))  # ì¤‘ë³µ ì œê±°

    def _generate_performance_suggestions(self, issues: List[Dict]) -> List[str]:
        """ì„±ëŠ¥ ìµœì í™” ì œì•ˆ"""
        suggestions = []

        for issue in issues:
            if "enumerate" in issue["issue"]:
                suggestions.append("range(len()) ëŒ€ì‹  enumerate() ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”")
            elif "list comprehension" in issue["issue"]:
                suggestions.append(
                    "ë°˜ë³µì ì¸ append ëŒ€ì‹  ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”"
                )
            elif "sleep" in issue["issue"]:
                suggestions.append("ë¸”ë¡œí‚¹ sleep ëŒ€ì‹  ë¹„ë™ê¸° ëŒ€ì•ˆì„ ê³ ë ¤í•˜ì„¸ìš”")

        return list(set(suggestions))

    def _generate_architecture_recommendations(
        self, func_count: int, class_count: int, complexity: int
    ) -> List[str]:
        """ì•„í‚¤í…ì²˜ ê¶Œì¥ì‚¬í•­"""
        recommendations = []

        if func_count > 20:
            recommendations.append("í•¨ìˆ˜ê°€ ë§ìŠµë‹ˆë‹¤. ëª¨ë“ˆ ë¶„ë¦¬ë¥¼ ê³ ë ¤í•˜ì„¸ìš”")

        if class_count == 0 and func_count > 10:
            recommendations.append("í´ë˜ìŠ¤ ê¸°ë°˜ êµ¬ì¡° ë„ì…ì„ ê³ ë ¤í•˜ì„¸ìš”")

        if complexity > 50:
            recommendations.append("ì½”ë“œ ë³µì¡ë„ê°€ ë†’ìŠµë‹ˆë‹¤. ë¦¬íŒ©í† ë§ì„ ê³ ë ¤í•˜ì„¸ìš”")

        return recommendations

    async def batch_process_files(
        self, file_paths: List[str], analysis_type: str = "general"
    ) -> Dict[str, Any]:
        """ğŸ“¦ ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬ (í† í° íš¨ìœ¨ì„± ê·¹ëŒ€í™”)"""
        print(f"ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(file_paths)}ê°œ íŒŒì¼")

        # íŒŒì¼ë“¤ì„ ë°°ì¹˜ë¡œ ê·¸ë£¹í™”
        batches = [
            file_paths[i : i + self.batch_size]
            for i in range(0, len(file_paths), self.batch_size)
        ]

        all_results = []
        total_token_savings = 0

        for batch_num, batch in enumerate(batches, 1):
            print(f"ğŸ“‹ ë°°ì¹˜ {batch_num}/{len(batches)} ì²˜ë¦¬ ì¤‘...")

            batch_results = []
            for file_path in batch:
                result = await self.analyze_code_efficiently(file_path, analysis_type)
                batch_results.append(result)
                total_token_savings += result.get("token_savings", 0)

            all_results.extend(batch_results)

            # ë°°ì¹˜ ê°„ ì§§ì€ ëŒ€ê¸° (API ë ˆì´íŠ¸ ë¦¬ë°‹ ê³ ë ¤)
            if batch_num < len(batches):
                await asyncio.sleep(0.5)

        return {
            "batch_processing_complete": True,
            "total_files_processed": len(file_paths),
            "total_batches": len(batches),
            "total_token_savings": total_token_savings,
            "results": all_results,
            "processing_summary": self._summarize_batch_results(all_results),
        }

    def _summarize_batch_results(self, results: List[Dict]) -> Dict[str, Any]:
        """ë°°ì¹˜ ê²°ê³¼ ìš”ì•½"""
        successful = [r for r in results if "error" not in r]
        failed = [r for r in results if "error" in r]

        total_savings = sum(r.get("token_savings", 0) for r in successful)

        return {
            "success_rate": len(successful) / len(results) if results else 0,
            "successful_analyses": len(successful),
            "failed_analyses": len(failed),
            "total_token_savings": total_savings,
            "average_compression_ratio": sum(
                r.get("processing_efficiency", {}).get("compression_ratio", 1.0)
                for r in successful
            )
            / max(len(successful), 1),
        }

    def get_token_efficiency_report(self) -> Dict[str, Any]:
        """ğŸ“Š í† í° íš¨ìœ¨ì„± ë³´ê³ ì„œ"""
        stats = self.token_optimizer.get_optimization_stats()
        suggestions = self.token_optimizer.suggest_optimizations()

        return {
            "echo_ide_tokenwise_report": True,
            "optimization_stats": stats,
            "efficiency_suggestions": suggestions,
            "workspace_context": {
                "active_files": len(self.active_files),
                "recent_queries": len(self.recent_queries),
                "batch_queue_size": len(self.batch_queue),
            },
            "token_saving_tips": [
                "ğŸ¯ ë°˜ë³µì ì¸ ì½”ë“œ ë¶„ì„ì€ ìºì‹œë¥¼ í™œìš©í•©ë‹ˆë‹¤",
                "ğŸ“¦ ì—¬ëŸ¬ íŒŒì¼ì„ í•œ ë²ˆì— ë°°ì¹˜ ì²˜ë¦¬í•˜ë©´ í† í°ì„ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
                "ğŸ”„ ë¹„ìŠ·í•œ ë¶„ì„ ìš”ì²­ì€ ìë™ìœ¼ë¡œ ìµœì í™”ë©ë‹ˆë‹¤",
                "ğŸ’¡ Echo IDEì˜ ìŠ¤ë§ˆíŠ¸ ì»¨í…ìŠ¤íŠ¸ ì••ì¶•ì„ í™œìš©í•˜ì„¸ìš”",
                "ğŸ“Š ì •ê¸°ì ìœ¼ë¡œ í† í° ì‚¬ìš© íŒ¨í„´ì„ í™•ì¸í•˜ì„¸ìš”",
            ],
            "timestamp": datetime.now().isoformat(),
        }

    async def smart_context_analysis(
        self, query: str, context_files: List[str] = None
    ) -> Dict[str, Any]:
        """ğŸ§  ìŠ¤ë§ˆíŠ¸ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        print(f"ğŸ§  ìŠ¤ë§ˆíŠ¸ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„: {query[:50]}...")

        # ì¿¼ë¦¬ íƒ€ì… ë¶„ë¥˜
        query_type = self._classify_query(query)

        # ê´€ë ¨ íŒŒì¼ë“¤ ìë™ ì„ íƒ
        if context_files is None:
            context_files = await self._select_relevant_files(query, query_type)

        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë° ìµœì í™”
        combined_context = ""
        optimization_results = []

        for file_path in context_files[:5]:  # ìµœëŒ€ 5ê°œ íŒŒì¼ë¡œ ì œí•œ
            try:
                full_path = self.workspace_path / file_path
                with open(full_path, "r", encoding="utf-8") as f:
                    content = f.read()

                # ì¿¼ë¦¬ íƒ€ì…ì— ë§ëŠ” ìµœì í™” ì ìš©
                optimized_content, opt_result = optimize_for_claude_code(
                    content, query_type
                )
                combined_context += f"\n--- {file_path} ---\n{optimized_content}\n"
                optimization_results.append(opt_result)

            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {file_path}: {e}")

        # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ì œí•œ
        if len(combined_context) > self.max_context_size:
            combined_context = (
                combined_context[: self.max_context_size]
                + "\n[...ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ì œí•œìœ¼ë¡œ ìƒëµ...]"
            )

        return {
            "query": query,
            "query_type": query_type,
            "context_files": context_files,
            "optimized_context": combined_context,
            "optimization_summary": {
                "total_files": len(optimization_results),
                "total_token_savings": sum(
                    r.estimated_token_savings for r in optimization_results
                ),
                "average_compression": sum(
                    r.compression_ratio for r in optimization_results
                )
                / max(len(optimization_results), 1),
            },
        }

    def _classify_query(self, query: str) -> str:
        """ì¿¼ë¦¬ íƒ€ì… ë¶„ë¥˜"""
        query_lower = query.lower()

        if any(
            word in query_lower for word in ["bug", "error", "debug", "fix", "issue"]
        ):
            return "debugging"
        elif any(
            word in query_lower for word in ["refactor", "improve", "optimize", "clean"]
        ):
            return "refactoring"
        elif any(
            word in query_lower
            for word in ["security", "vulnerability", "safe", "secure"]
        ):
            return "security"
        elif any(
            word in query_lower for word in ["performance", "speed", "fast", "slow"]
        ):
            return "performance"
        elif any(
            word in query_lower for word in ["architecture", "design", "structure"]
        ):
            return "architecture"
        else:
            return "general"

    async def _select_relevant_files(self, query: str, query_type: str) -> List[str]:
        """ê´€ë ¨ íŒŒì¼ ìë™ ì„ íƒ"""
        # ê°„ë‹¨í•œ ê´€ë ¨ë„ ê¸°ë°˜ íŒŒì¼ ì„ íƒ
        all_files = list(self.workspace_path.rglob("*.py"))

        # ì¿¼ë¦¬ í‚¤ì›Œë“œì™€ íŒŒì¼ëª… ë§¤ì¹­
        query_keywords = query.lower().split()
        relevant_files = []

        for file_path in all_files:
            relative_path = file_path.relative_to(self.workspace_path)
            file_name = file_path.name.lower()

            # íŒŒì¼ëª…ê³¼ ì¿¼ë¦¬ í‚¤ì›Œë“œ ë§¤ì¹­
            relevance_score = sum(
                1 for keyword in query_keywords if keyword in file_name
            )

            if relevance_score > 0:
                relevant_files.append((str(relative_path), relevance_score))

        # ê´€ë ¨ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        relevant_files.sort(key=lambda x: x[1], reverse=True)

        return [file_path for file_path, score in relevant_files[:10]]


# í¸ì˜ í•¨ìˆ˜ë“¤
async def analyze_with_echo_ide(
    file_path: str, analysis_type: str = "general", workspace: str = "."
) -> Dict[str, Any]:
    """Echo IDE TokenWise ë¶„ì„ í¸ì˜ í•¨ìˆ˜"""
    ide = EchoIDETokenWise(workspace)
    return await ide.analyze_code_efficiently(file_path, analysis_type)


async def batch_analyze_with_echo_ide(
    file_paths: List[str], analysis_type: str = "general", workspace: str = "."
) -> Dict[str, Any]:
    """Echo IDE TokenWise ë°°ì¹˜ ë¶„ì„ í¸ì˜ í•¨ìˆ˜"""
    ide = EchoIDETokenWise(workspace)
    return await ide.batch_process_files(file_paths, analysis_type)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("ğŸ§ª Echo IDE TokenWise í…ŒìŠ¤íŠ¸")

    async def test_echo_ide():
        ide = EchoIDETokenWise(".")

        # ë‹¨ì¼ íŒŒì¼ ë¶„ì„ í…ŒìŠ¤íŠ¸
        test_file = "echo_engine/token_optimizer.py"
        if Path(test_file).exists():
            result = await ide.analyze_code_efficiently(test_file, "security")
            print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
            print(f"   íŒŒì¼: {result['file_path']}")
            print(f"   í† í° ì ˆì•½: {result['token_savings']}ê°œ")
            print(
                f"   ì••ì¶•ë¥ : {result['processing_efficiency']['compression_ratio']:.1%}"
            )

        # íš¨ìœ¨ì„± ë³´ê³ ì„œ
        report = ide.get_token_efficiency_report()
        print(f"\nğŸ“ˆ íš¨ìœ¨ì„± ë³´ê³ ì„œ:")
        print(f"   í™œì„± íŒŒì¼: {report['workspace_context']['active_files']}")
        print(f"   ìµœì í™” ì œì•ˆ: {len(report['efficiency_suggestions'])}ê°œ")

        print("\nâœ… Echo IDE TokenWise í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

    asyncio.run(test_echo_ide())

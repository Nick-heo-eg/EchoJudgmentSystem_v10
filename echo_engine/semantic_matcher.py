#!/usr/bin/env python3
"""
ğŸ§  Semantic Matcher - KoSimCSE ê¸°ë°˜ ì˜ë¯¸ ìœ ì‚¬ë„ ë§¤ì¹­ ì‹œìŠ¤í…œ
ìì—°ì–´ ì…ë ¥ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ê³„ì‚°í•˜ì—¬ ì •í™•í•œ ì˜ë„ ë¶„ë¥˜ ì§€ì›

í•µì‹¬ ê¸°ëŠ¥:
1. KoSimCSE ëª¨ë¸ì„ í™œìš©í•œ í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”©
2. ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° ë° ë§¤ì¹­
3. ì½”ë”© ì˜ë„ ë¶„ë¥˜ ì •í™•ë„ í–¥ìƒ
4. Echo ì‹œê·¸ë‹ˆì²˜ë³„ íŠ¹í™”ëœ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜
5. í´ë°± ëª¨ë“œ (KoSimCSE ì—†ì´ë„ ë™ì‘)
"""

import numpy as np
import json
import re
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import logging

# KoSimCSE ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ import (ì„ íƒì )
KOSIMCSE_AVAILABLE = False
try:
    import torch
    from transformers import AutoModel, AutoTokenizer

    KOSIMCSE_AVAILABLE = True
    print("âœ… KoSimCSE ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    print(f"âš ï¸ KoSimCSE ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ: {e}")
    print("ğŸ”„ í´ë°± ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤")


@dataclass
class SemanticMatchResult:
    """ì˜ë¯¸ ë§¤ì¹­ ê²°ê³¼"""

    query: str
    best_match: str
    similarity_score: float
    confidence: float
    matched_category: str
    alternative_matches: List[Dict[str, Any]]
    matching_method: str


@dataclass
class IntentTemplate:
    """ì˜ë„ í…œí”Œë¦¿"""

    category: str
    intent_name: str
    example_phrases: List[str]
    keywords: List[str]
    signature_preference: str
    complexity_hint: str


class KoSimCSEMatcher:
    """ğŸš€ KoSimCSE ê¸°ë°˜ ì˜ë¯¸ ë§¤ì¹­ê¸°"""

    def __init__(self, model_name: str = "BM-K/KoSimCSE-roberta-multitask"):
        global KOSIMCSE_AVAILABLE
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.device = "cpu"  # CPU ìš°ì„  ì‚¬ìš©
        self.available = KOSIMCSE_AVAILABLE

        if KOSIMCSE_AVAILABLE:
            try:
                self._load_model()
                print(f"âœ… KoSimCSE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
            except Exception as e:
                print(f"âŒ KoSimCSE ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.available = False

    def _load_model(self):
        """KoSimCSE ëª¨ë¸ ë¡œë“œ"""
        if not self.available:
            return

        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModel.from_pretrained(self.model_name)

        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ, ì•„ë‹ˆë©´ CPU
        if torch.cuda.is_available():
            self.device = "cuda"
            self.model = self.model.to(self.device)

        self.model.eval()

    def encode_sentences(self, sentences: List[str]) -> np.ndarray:
        """ë¬¸ì¥ë“¤ì„ ë²¡í„°ë¡œ ì¸ì½”ë”©"""
        if not self.available or not self.model:
            # í´ë°±: TF-IDF ìœ ì‚¬ ë²¡í„°í™”
            return self._fallback_encode(sentences)

        try:
            with torch.no_grad():
                # í† í°í™”
                inputs = self.tokenizer(
                    sentences,
                    padding=True,
                    truncation=True,
                    return_tensors="pt",
                    max_length=512,
                ).to(self.device)

                # ëª¨ë¸ ì¶”ë¡ 
                outputs = self.model(**inputs)

                # [CLS] í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
                embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()

                return embeddings

        except Exception as e:
            print(f"âš ï¸ KoSimCSE ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
            return self._fallback_encode(sentences)

    def _fallback_encode(self, sentences: List[str]) -> np.ndarray:
        """í´ë°± ì¸ì½”ë”© (ê°„ë‹¨í•œ TF-IDF ìœ ì‚¬)"""
        from collections import Counter
        import math

        # ê°„ë‹¨í•œ í•œêµ­ì–´ í† í°í™”
        def tokenize_korean(text):
            # ê¸°ë³¸ì ì¸ í•œêµ­ì–´ ì²˜ë¦¬
            text = re.sub(r"[^\w\sê°€-í£]", " ", text.lower())
            return text.split()

        # ëª¨ë“  ë¬¸ì¥ì˜ í† í° ìˆ˜ì§‘
        all_tokens = []
        tokenized_sentences = []

        for sentence in sentences:
            tokens = tokenize_korean(sentence)
            tokenized_sentences.append(tokens)
            all_tokens.extend(tokens)

        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        vocab = list(set(all_tokens))
        vocab_size = len(vocab)

        # TF-IDF ìœ ì‚¬ ë²¡í„° ìƒì„±
        vectors = []
        for tokens in tokenized_sentences:
            vector = np.zeros(vocab_size)
            token_count = Counter(tokens)

            for i, word in enumerate(vocab):
                if word in token_count:
                    tf = token_count[word] / len(tokens)
                    # ê°„ë‹¨í•œ IDF ê³„ì‚°
                    df = sum(
                        1 for sent_tokens in tokenized_sentences if word in sent_tokens
                    )
                    idf = math.log(len(sentences) / (df + 1))
                    vector[i] = tf * idf

            vectors.append(vector)

        return np.array(vectors)

    def calculate_similarity(self, query: str, candidates: List[str]) -> List[float]:
        """ì¿¼ë¦¬ì™€ í›„ë³´ë“¤ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        all_sentences = [query] + candidates
        embeddings = self.encode_sentences(all_sentences)

        query_embedding = embeddings[0:1]
        candidate_embeddings = embeddings[1:]

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for candidate_emb in candidate_embeddings:
            # ì •ê·œí™”
            query_norm = query_embedding / np.linalg.norm(query_embedding)
            candidate_norm = candidate_emb / np.linalg.norm(candidate_emb)

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            similarity = np.dot(query_norm, candidate_norm.T)[0][0]
            similarities.append(float(similarity))

        return similarities


class SemanticMatcher:
    """ğŸ¯ í†µí•© ì˜ë¯¸ ë§¤ì¹­ ì—”ì§„"""

    def __init__(self):
        # KoSimCSE ë§¤ì¹­ê¸° ì´ˆê¸°í™”
        self.kosimcse_matcher = KoSimCSEMatcher() if KOSIMCSE_AVAILABLE else None

        # ì˜ë„ í…œí”Œë¦¿ ë¡œë“œ
        self.intent_templates = self._load_intent_templates()

        # ë§¤ì¹­ íˆìŠ¤í† ë¦¬
        self.matching_history = []

        print("ğŸ¯ Semantic Matcher ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   KoSimCSE: {'âœ… í™œì„±í™”' if KOSIMCSE_AVAILABLE else 'âŒ í´ë°± ëª¨ë“œ'}")
        print(f"   ì˜ë„ í…œí”Œë¦¿: {len(self.intent_templates)}ê°œ")

    def match_coding_intent(
        self, user_input: str, threshold: float = 0.6
    ) -> SemanticMatchResult:
        """ì½”ë”© ì˜ë„ ë§¤ì¹­"""

        # 1. í…œí”Œë¦¿ë³„ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        match_scores = []

        for template in self.intent_templates:
            if template.category == "coding":
                # ì˜ˆì‹œ ë¬¸ì¥ë“¤ê³¼ ìœ ì‚¬ë„ ê³„ì‚°
                if self.kosimcse_matcher:
                    similarities = self.kosimcse_matcher.calculate_similarity(
                        user_input, template.example_phrases
                    )
                    max_similarity = max(similarities) if similarities else 0.0
                else:
                    # í´ë°±: í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­
                    max_similarity = self._keyword_based_similarity(
                        user_input, template
                    )

                match_scores.append(
                    {
                        "template": template,
                        "similarity": max_similarity,
                        "intent_name": template.intent_name,
                    }
                )

        # 2. ìµœê³  ì ìˆ˜ ì„ íƒ
        if not match_scores:
            return self._create_fallback_result(user_input)

        best_match = max(match_scores, key=lambda x: x["similarity"])

        # 3. ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self._calculate_confidence(best_match["similarity"], match_scores)

        # 4. ëŒ€ì•ˆ ë§¤ì¹­ ìƒì„±
        alternative_matches = sorted(
            [score for score in match_scores if score != best_match],
            key=lambda x: x["similarity"],
            reverse=True,
        )[
            :3
        ]  # ìƒìœ„ 3ê°œ

        # 5. ê²°ê³¼ êµ¬ì„±
        result = SemanticMatchResult(
            query=user_input,
            best_match=best_match["template"].intent_name,
            similarity_score=best_match["similarity"],
            confidence=confidence,
            matched_category="coding",
            alternative_matches=[
                {
                    "intent": alt["intent_name"],
                    "similarity": alt["similarity"],
                    "signature_preference": alt["template"].signature_preference,
                }
                for alt in alternative_matches
            ],
            matching_method="kosimcse" if self.kosimcse_matcher else "keyword_fallback",
        )

        # 6. íˆìŠ¤í† ë¦¬ ê¸°ë¡
        self.matching_history.append(
            {
                "timestamp": datetime.now().isoformat(),
                "query": user_input,
                "result": result.best_match,
                "confidence": confidence,
            }
        )

        return result

    def _keyword_based_similarity(
        self, user_input: str, template: IntentTemplate
    ) -> float:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚° (í´ë°±)"""
        user_words = set(re.findall(r"\w+", user_input.lower()))
        template_words = set()

        # í‚¤ì›Œë“œ ì¶”ê°€
        for keyword in template.keywords:
            template_words.update(re.findall(r"\w+", keyword.lower()))

        # ì˜ˆì‹œ ë¬¸ì¥ì˜ ë‹¨ì–´ë“¤ë„ ì¶”ê°€
        for phrase in template.example_phrases:
            template_words.update(re.findall(r"\w+", phrase.lower()))

        # ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°
        intersection = len(user_words & template_words)
        union = len(user_words | template_words)

        return intersection / union if union > 0 else 0.0

    def _calculate_confidence(self, best_score: float, all_scores: List[Dict]) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        if len(all_scores) < 2:
            return min(best_score, 0.8)  # ë‹¨ì¼ ë§¤ì¹­ì€ ìµœëŒ€ 0.8

        # ìƒìœ„ 2ê°œ ì ìˆ˜ ì°¨ì´ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
        sorted_scores = sorted(all_scores, key=lambda x: x["similarity"], reverse=True)
        score_gap = sorted_scores[0]["similarity"] - sorted_scores[1]["similarity"]

        # ì ìˆ˜ ì°¨ì´ê°€ í´ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ
        confidence = best_score * (1 + score_gap)
        return min(confidence, 0.95)  # ìµœëŒ€ 0.95

    def _create_fallback_result(self, user_input: str) -> SemanticMatchResult:
        """í´ë°± ê²°ê³¼ ìƒì„±"""
        return SemanticMatchResult(
            query=user_input,
            best_match="general_coding",
            similarity_score=0.3,
            confidence=0.4,
            matched_category="coding",
            alternative_matches=[],
            matching_method="fallback",
        )

    def _load_intent_templates(self) -> List[IntentTemplate]:
        """ì˜ë„ í…œí”Œë¦¿ ë¡œë“œ"""
        return [
            IntentTemplate(
                category="coding",
                intent_name="streamlit_app_creation",
                example_phrases=[
                    "ìŠ¤íŠ¸ë¦¼ë¦¿ ì•± ë§Œë“¤ì–´ì¤˜",
                    "ëŒ€ì‹œë³´ë“œ ê°œë°œí•´ì¤˜",
                    "ì›¹ ì•± ë§Œë“¤ì–´ì¤˜",
                    "ë°ì´í„° ì‹œê°í™” ì•± ì œì‘í•´ì¤˜",
                    "ì¸í„°ë™í‹°ë¸Œ ì°¨íŠ¸ ì•± ë§Œë“¤ì–´ì¤˜",
                ],
                keywords=[
                    "ìŠ¤íŠ¸ë¦¼ë¦¿",
                    "streamlit",
                    "ëŒ€ì‹œë³´ë“œ",
                    "ì›¹ì•±",
                    "ì‹œê°í™”",
                    "ì°¨íŠ¸",
                ],
                signature_preference="Aurora",
                complexity_hint="intermediate",
            ),
            IntentTemplate(
                category="coding",
                intent_name="data_analysis_script",
                example_phrases=[
                    "ë°ì´í„° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ ë§Œë“¤ì–´ì¤˜",
                    "CSV íŒŒì¼ ë¶„ì„í•´ì¤˜",
                    "í†µê³„ ë¶„ì„ ì½”ë“œ ì‘ì„±í•´ì¤˜",
                    "ë°ì´í„° ì‹œê°í™” í•´ì¤˜",
                    "ì—‘ì…€ ë°ì´í„° ì²˜ë¦¬í•´ì¤˜",
                ],
                keywords=["ë°ì´í„°", "ë¶„ì„", "csv", "ì—‘ì…€", "í†µê³„", "ì‹œê°í™”"],
                signature_preference="Sage",
                complexity_hint="intermediate",
            ),
            IntentTemplate(
                category="coding",
                intent_name="web_scraping_script",
                example_phrases=[
                    "ì›¹ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸ ë§Œë“¤ì–´ì¤˜",
                    "ì›¹ì‚¬ì´íŠ¸ ë°ì´í„° ìˆ˜ì§‘í•´ì¤˜",
                    "ë‰´ìŠ¤ ê¸°ì‚¬ ìŠ¤í¬ë˜í•‘í•´ì¤˜",
                    "ì›¹í˜ì´ì§€ ì •ë³´ ì¶”ì¶œí•´ì¤˜",
                    "ì‚¬ì´íŠ¸ ë°ì´í„° ê°€ì ¸ì™€ì¤˜",
                ],
                keywords=["í¬ë¡¤ë§", "ìŠ¤í¬ë˜í•‘", "ì›¹", "ìˆ˜ì§‘", "íŒŒì‹±", "ì¶”ì¶œ"],
                signature_preference="Phoenix",
                complexity_hint="advanced",
            ),
            IntentTemplate(
                category="coding",
                intent_name="automation_script",
                example_phrases=[
                    "ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ë§Œë“¤ì–´ì¤˜",
                    "ë°˜ë³µ ì‘ì—… ìë™í™”í•´ì¤˜",
                    "ë°°ì¹˜ ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±í•´ì¤˜",
                    "íŒŒì¼ ì²˜ë¦¬ ìë™í™”í•´ì¤˜",
                    "ì—…ë¬´ ìë™í™” ë„êµ¬ ë§Œë“¤ì–´ì¤˜",
                ],
                keywords=["ìë™í™”", "ë°˜ë³µ", "ë°°ì¹˜", "ì²˜ë¦¬", "ìŠ¤ì¼€ì¤„"],
                signature_preference="Companion",
                complexity_hint="intermediate",
            ),
            IntentTemplate(
                category="coding",
                intent_name="interactive_game",
                example_phrases=[
                    "ê°„ë‹¨í•œ ê²Œì„ ë§Œë“¤ì–´ì¤˜",
                    "í…ìŠ¤íŠ¸ ê²Œì„ ë§Œë“¤ì–´ì¤˜",
                    "í€´ì¦ˆ ê²Œì„ ê°œë°œí•´ì¤˜",
                    "ì¸í„°ë™í‹°ë¸Œ í”„ë¡œê·¸ë¨ ë§Œë“¤ì–´ì¤˜",
                    "ë¯¸ë‹ˆê²Œì„ ì½”ë“œ ì‘ì„±í•´ì¤˜",
                ],
                keywords=["ê²Œì„", "í€´ì¦ˆ", "ì¸í„°ë™í‹°ë¸Œ", "ë†€ì´", "ì—”í„°í…Œì¸ë¨¼íŠ¸"],
                signature_preference="Aurora",
                complexity_hint="simple",
            ),
            IntentTemplate(
                category="coding",
                intent_name="api_integration",
                example_phrases=[
                    "API ì—°ë™ ì½”ë“œ ë§Œë“¤ì–´ì¤˜",
                    "REST API í´ë¼ì´ì–¸íŠ¸ ê°œë°œí•´ì¤˜",
                    "ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ê²°í•´ì¤˜",
                    "API í˜¸ì¶œ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±í•´ì¤˜",
                    "ì›¹ ì„œë¹„ìŠ¤ í†µí•©í•´ì¤˜",
                ],
                keywords=["api", "rest", "ì—°ë™", "ì„œë¹„ìŠ¤", "http", "json"],
                signature_preference="Phoenix",
                complexity_hint="advanced",
            ),
            IntentTemplate(
                category="coding",
                intent_name="utility_tool",
                example_phrases=[
                    "ìœ í‹¸ë¦¬í‹° ë„êµ¬ ë§Œë“¤ì–´ì¤˜",
                    "í¸ì˜ í”„ë¡œê·¸ë¨ ê°œë°œí•´ì¤˜",
                    "ë„êµ¬ í”„ë¡œê·¸ë¨ ì‘ì„±í•´ì¤˜",
                    "í—¬í¼ ìŠ¤í¬ë¦½íŠ¸ ë§Œë“¤ì–´ì¤˜",
                    "ì‚¬ìš©ì ë„êµ¬ ê°œë°œí•´ì¤˜",
                ],
                keywords=["ë„êµ¬", "ìœ í‹¸ë¦¬í‹°", "í—¬í¼", "í¸ì˜", "í”„ë¡œê·¸ë¨"],
                signature_preference="Companion",
                complexity_hint="simple",
            ),
        ]

    def enhance_intent_detection(
        self, user_input: str, existing_intent: str, existing_confidence: float
    ) -> Dict[str, Any]:
        """ê¸°ì¡´ ì˜ë„ ê°ì§€ ê²°ê³¼ í–¥ìƒ"""
        semantic_result = self.match_coding_intent(user_input)

        # ê¸°ì¡´ ê²°ê³¼ì™€ ì˜ë¯¸ì  ë§¤ì¹­ ê²°ê³¼ ê²°í•©
        enhanced_confidence = (existing_confidence + semantic_result.confidence) / 2

        # ë” ë‚˜ì€ ë§¤ì¹­ì´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
        if semantic_result.confidence > existing_confidence + 0.1:  # 10% ì´ìƒ ë†’ìœ¼ë©´
            return {
                "enhanced_intent": semantic_result.best_match,
                "enhanced_confidence": semantic_result.confidence,
                "semantic_alternatives": semantic_result.alternative_matches,
                "improvement": "semantic_override",
                "original_intent": existing_intent,
                "original_confidence": existing_confidence,
            }
        else:
            return {
                "enhanced_intent": existing_intent,
                "enhanced_confidence": enhanced_confidence,
                "semantic_alternatives": semantic_result.alternative_matches,
                "improvement": "confidence_boost",
                "original_intent": existing_intent,
                "original_confidence": existing_confidence,
            }

    def get_matching_stats(self) -> Dict[str, Any]:
        """ë§¤ì¹­ í†µê³„ ë°˜í™˜"""
        if not self.matching_history:
            return {"message": "ë§¤ì¹­ íˆìŠ¤í† ë¦¬ ì—†ìŒ"}

        total_matches = len(self.matching_history)
        avg_confidence = (
            sum(h["confidence"] for h in self.matching_history) / total_matches
        )

        intent_distribution = {}
        for history in self.matching_history:
            intent = history["result"]
            intent_distribution[intent] = intent_distribution.get(intent, 0) + 1

        return {
            "total_matches": total_matches,
            "average_confidence": avg_confidence,
            "intent_distribution": intent_distribution,
            "kosimcse_enabled": KOSIMCSE_AVAILABLE,
            "recent_matches": self.matching_history[-5:],  # ìµœê·¼ 5ê°œ
        }


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_semantic_matcher() -> SemanticMatcher:
    """ì˜ë¯¸ ë§¤ì¹­ê¸° ìƒì„±"""
    return SemanticMatcher()


def quick_semantic_match(user_input: str) -> str:
    """ë¹ ë¥¸ ì˜ë¯¸ ë§¤ì¹­ (ê²°ê³¼ë§Œ ë°˜í™˜)"""
    matcher = create_semantic_matcher()
    result = matcher.match_coding_intent(user_input)
    return result.best_match


# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸ§  Semantic Matcher KoSimCSE í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘...")

    matcher = create_semantic_matcher()

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
    test_queries = [
        "ë§¤ì¶œ ë°ì´í„° ë¶„ì„í•˜ëŠ” ìŠ¤íŠ¸ë¦¼ë¦¿ ëŒ€ì‹œë³´ë“œ ë§Œë“¤ì–´ì¤˜",
        "ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ì œëª©ë“¤ í¬ë¡¤ë§í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ í•„ìš”í•´",
        "CSV íŒŒì¼ ì½ì–´ì„œ ì°¨íŠ¸ ê·¸ë¦¬ëŠ” ì½”ë“œ ì‘ì„±í•´ì¤˜",
        "ê°„ë‹¨í•œ ê³„ì‚°ê¸° ê²Œì„ ë§Œë“¤ì–´ì¤˜",
        "íŒŒì¼ë“¤ ìë™ìœ¼ë¡œ ì •ë¦¬í•˜ëŠ” í”„ë¡œê·¸ë¨ ê°œë°œí•´ì¤˜",
        "ë‚ ì”¨ API ì—°ê²°í•´ì„œ ì •ë³´ ê°€ì ¸ì˜¤ëŠ” ì½”ë“œ ë§Œë“¤ì–´ì¤˜",
        "ì¼ì • ê´€ë¦¬í•˜ëŠ” ìœ í‹¸ë¦¬í‹° ë„êµ¬ ë§Œë“¤ì–´ì¤˜",
    ]

    print("=" * 80)

    for i, query in enumerate(test_queries, 1):
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ {i}: {query}")

        result = matcher.match_coding_intent(query)

        print(f"ğŸ“Š ë§¤ì¹­ ê²°ê³¼:")
        print(f"  ìµœì  ì˜ë„: {result.best_match}")
        print(f"  ìœ ì‚¬ë„: {result.similarity_score:.3f}")
        print(f"  ì‹ ë¢°ë„: {result.confidence:.3f}")
        print(f"  ë§¤ì¹­ ë°©ë²•: {result.matching_method}")

        if result.alternative_matches:
            print(f"  ëŒ€ì•ˆ ë§¤ì¹­:")
            for alt in result.alternative_matches[:2]:  # ìƒìœ„ 2ê°œë§Œ
                print(f"    - {alt['intent']}: {alt['similarity']:.3f}")

        print("-" * 60)

    # í†µê³„ ì¶œë ¥
    print(f"\nğŸ“ˆ ë§¤ì¹­ í†µê³„:")
    stats = matcher.get_matching_stats()
    for key, value in stats.items():
        if key == "recent_matches":
            continue
        elif isinstance(value, dict):
            print(f"  {key}:")
            for sub_key, sub_value in value.items():
                print(f"    {sub_key}: {sub_value}")
        elif isinstance(value, float):
            print(f"  {key}: {value:.3f}")
        else:
            print(f"  {key}: {value}")

    print("\nâœ… Semantic Matcher í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

    if KOSIMCSE_AVAILABLE:
        print("ğŸš€ KoSimCSEë¥¼ í™œìš©í•œ ê³ ì •ë°€ë„ ì˜ë¯¸ ë§¤ì¹­ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print(
            "ğŸ”„ í´ë°± ëª¨ë“œë¡œ ë™ì‘ ì¤‘ì…ë‹ˆë‹¤. ë” ì •í™•í•œ ë§¤ì¹­ì„ ìœ„í•´ PyTorchì™€ transformers ì„¤ì¹˜ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
        )

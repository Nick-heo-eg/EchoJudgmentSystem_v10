#!/usr/bin/env python3
"""
ğŸ§  EchoJudgmentSystem v10.6 - Reinforcement Engine
íŒë‹¨â¨¯í–‰ë™ ê²°ê³¼ì— ëŒ€í•œ ë³´ìƒ ê¸°ë°˜ í•™ìŠµ ëª¨ë“ˆ

TT.005: "ëª¨ë“  íŒë‹¨ì€ ê²½í—˜ì´ ë˜ê³ , ê²½í—˜ì€ ë‹¤ìŒ íŒë‹¨ì˜ ì§€í˜œê°€ ëœë‹¤."

ì£¼ìš” ê¸°ëŠ¥:
- Q-table ê¸°ë°˜ ê°•í™”í•™ìŠµ
- íŒë‹¨ ê²°ê³¼ì— ëŒ€í•œ ë³´ìƒ/ì²˜ë²Œ ì ìš©
- ì „ëµ íš¨ê³¼ì„± í•™ìŠµ ë° ì—…ë°ì´íŠ¸
- ë©”íƒ€ì¸ì§€ í”¼ë“œë°± ë£¨í”„ í†µí•©
"""

import json
import os
import time
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict
import numpy as np

# EchoJudgmentSystem ëª¨ë“ˆ
try:
    from echo_engine.qtable_rl import QTableRL
except ImportError:
    QTableRL = None

try:
    from echo_engine.meta_logger import log_evolution_event
except ImportError:
    log_evolution_event = None

try:
    from echo_engine.persona_meta_logger import get_persona_meta_logger
except ImportError:
    get_persona_meta_logger = None


@dataclass
class ReinforcementAction:
    """ê°•í™”í•™ìŠµ ì•¡ì…˜ ì •ì˜"""

    strategy: str
    context_type: str
    emotion_state: str
    confidence: float
    timestamp: str

    def to_key(self) -> str:
        """Q-table í‚¤ ìƒì„±"""
        return f"{self.context_type}:{self.emotion_state}:{self.strategy}"


@dataclass
class ReinforcementFeedback:
    """ê°•í™”í•™ìŠµ í”¼ë“œë°± ì •ì˜"""

    action_key: str
    reward: float
    success: bool
    user_satisfaction: float
    response_time: float
    learning_insight: str
    meta_reflection: Dict[str, Any]


class ReinforcementEngine:
    """
    EchoJudgmentSystem ê°•í™”í•™ìŠµ ì—”ì§„

    íŒë‹¨ ê²°ê³¼ì— ëŒ€í•œ ë³´ìƒ ê¸°ë°˜ í•™ìŠµì„ í†µí•´
    ì‹œìŠ¤í…œì˜ ì „ëµ ì„ íƒ ëŠ¥ë ¥ì„ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    """

    def __init__(self, learning_rate: float = 0.1, discount_factor: float = 0.9):
        """
        ê°•í™”í•™ìŠµ ì—”ì§„ ì´ˆê¸°í™”

        Args:
            learning_rate: í•™ìŠµë¥  (0.0-1.0)
            discount_factor: í• ì¸ ì¸ìˆ˜ (0.0-1.0)
        """
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor

        # Q-table ì´ˆê¸°í™”
        self.q_table: Dict[str, float] = {}
        self.action_counts: Dict[str, int] = {}
        self.last_rewards: Dict[str, float] = {}

        # í•™ìŠµ í†µê³„
        self.total_actions = 0
        self.successful_actions = 0
        self.learning_episodes = 0

        # ì „ëµë³„ ì„±ê³¼ ì¶”ì 
        self.strategy_performance: Dict[str, Dict[str, Any]] = {}

        # ì„¤ì • ë¡œë“œ
        self.load_q_table()

        print("ğŸ§  ReinforcementEngine ì´ˆê¸°í™” ì™„ë£Œ")

    def apply_reward(
        self, judgment_result: Dict[str, Any], feedback: Dict[str, Any]
    ) -> float:
        """
        íŒë‹¨ ê²°ê³¼ì— ëŒ€í•œ ë³´ìƒ ì ìš©

        Args:
            judgment_result: íŒë‹¨ ê²°ê³¼ ë°ì´í„°
            feedback: ì‚¬ìš©ì/ì‹œìŠ¤í…œ í”¼ë“œë°±

        Returns:
            ì ìš©ëœ ë³´ìƒ ê°’
        """
        try:
            # ì•¡ì…˜ ì •ë³´ ì¶”ì¶œ
            action = self._extract_action(judgment_result)

            # ë³´ìƒ ê³„ì‚°
            reward = self._calculate_reward(feedback)

            # Q-value ì—…ë°ì´íŠ¸
            action_key = action.to_key()
            current_q = self.q_table.get(action_key, 0.0)

            # Q-learning ì—…ë°ì´íŠ¸ ê³µì‹: Q(s,a) = Q(s,a) + Î±[r + Î³max(Q(s',a')) - Q(s,a)]
            new_q = current_q + self.learning_rate * (reward - current_q)
            self.q_table[action_key] = new_q

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.action_counts[action_key] = self.action_counts.get(action_key, 0) + 1
            self.last_rewards[action_key] = reward
            self.total_actions += 1

            if reward > 0:
                self.successful_actions += 1

            # ì „ëµ ì„±ê³¼ ì—…ë°ì´íŠ¸
            self._update_strategy_performance(action, reward, feedback)

            # ë©”íƒ€ ë¡œê¹…
            self._log_reinforcement_event(action, reward, feedback)

            # ì£¼ê¸°ì  Q-table ì €ì¥
            if self.total_actions % 10 == 0:
                self.save_q_table()

            print(f"ğŸ¯ ë³´ìƒ ì ìš©: {action_key} â†’ {reward:.3f} (Q: {new_q:.3f})")
            return reward

        except Exception as e:
            print(f"âŒ ë³´ìƒ ì ìš© ì‹¤íŒ¨: {e}")
            return 0.0

    def get_updated_strategy(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        í•™ìŠµëœ Q-tableì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì  ì „ëµ ì¶”ì²œ

        Args:
            context: í˜„ì¬ ìƒí™© ì»¨í…ìŠ¤íŠ¸

        Returns:
            ì¶”ì²œ ì „ëµ ì •ë³´
        """
        try:
            context_type = context.get("context_type", "general")
            emotion_state = context.get("emotion_detected", "neutral")

            # í˜„ì¬ ìƒí™©ì— ì ìš© ê°€ëŠ¥í•œ ì „ëµë“¤ ì°¾ê¸°
            available_strategies = self._get_available_strategies(
                context_type, emotion_state
            )

            if not available_strategies:
                # ê¸°ë³¸ ì „ëµ ë°˜í™˜
                return {
                    "strategy": "balanced",
                    "confidence": 0.5,
                    "reason": "No learned strategies available",
                    "q_value": 0.0,
                    "exploration": True,
                }

            # Q-value ê¸°ë°˜ ì „ëµ ì„ íƒ (Îµ-greedy)
            best_strategy = self._select_strategy_epsilon_greedy(
                available_strategies, context_type, emotion_state
            )

            action_key = f"{context_type}:{emotion_state}:{best_strategy}"
            q_value = self.q_table.get(action_key, 0.0)
            confidence = min(
                1.0, max(0.0, (q_value + 1.0) / 2.0)
            )  # Q-valueë¥¼ confidenceë¡œ ë³€í™˜

            # ì „ëµ ì„±ê³¼ ì •ë³´ ì¶”ê°€
            performance_info = self.strategy_performance.get(best_strategy, {})

            return {
                "strategy": best_strategy,
                "confidence": confidence,
                "reason": f"Q-learning ê¸°ë°˜ ì„ íƒ (Q={q_value:.3f})",
                "q_value": q_value,
                "exploration": False,
                "action_count": self.action_counts.get(action_key, 0),
                "last_reward": self.last_rewards.get(action_key, 0.0),
                "performance_info": performance_info,
            }

        except Exception as e:
            print(f"âŒ ì „ëµ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return {
                "strategy": "balanced",
                "confidence": 0.3,
                "reason": f"Error in strategy selection: {e}",
                "q_value": 0.0,
                "exploration": True,
            }

    def _extract_action(self, judgment_result: Dict[str, Any]) -> ReinforcementAction:
        """íŒë‹¨ ê²°ê³¼ì—ì„œ ì•¡ì…˜ ì •ë³´ ì¶”ì¶œ"""
        return ReinforcementAction(
            strategy=judgment_result.get("strategy_selected", "balanced"),
            context_type=judgment_result.get("context_type", "general"),
            emotion_state=judgment_result.get("emotion_detected", "neutral"),
            confidence=judgment_result.get("confidence", 0.5),
            timestamp=datetime.now().isoformat(),
        )

    def _calculate_reward(self, feedback: Dict[str, Any]) -> float:
        """í”¼ë“œë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ë³´ìƒ ê³„ì‚°"""
        reward = 0.0

        # ì„±ê³µ/ì‹¤íŒ¨ ê¸°ë³¸ ë³´ìƒ
        if feedback.get("success", False):
            reward += 0.5
        else:
            reward -= 0.3

        # ì‚¬ìš©ì ë§Œì¡±ë„ ë³´ìƒ
        user_satisfaction = feedback.get("user_satisfaction", 0.5)
        reward += (user_satisfaction - 0.5) * 0.4

        # íš¨ê³¼ì„± ì ìˆ˜ ë³´ìƒ
        effectiveness = feedback.get("effectiveness_score", 0.5)
        reward += (effectiveness - 0.5) * 0.3

        # ì‘ë‹µ ì‹œê°„ ë³´ìƒ (ë¹ ë¥¼ìˆ˜ë¡ ì¢‹ìŒ)
        response_time = feedback.get("response_time", 1.0)
        if response_time < 0.5:
            reward += 0.1
        elif response_time > 2.0:
            reward -= 0.1

        # ë©”íƒ€ì¸ì§€ í’ˆì§ˆ ë³´ìƒ
        meta_quality = feedback.get("meta_quality", 0.5)
        reward += (meta_quality - 0.5) * 0.2

        return max(-1.0, min(1.0, reward))  # -1.0 ~ 1.0 ë²”ìœ„ë¡œ ì œí•œ

    def _get_available_strategies(
        self, context_type: str, emotion_state: str
    ) -> List[str]:
        """í˜„ì¬ ìƒí™©ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì „ëµ ëª©ë¡ ë°˜í™˜"""
        # ê¸°ë³¸ ì „ëµ ëª©ë¡
        base_strategies = [
            "empathetic",
            "analytical",
            "creative",
            "supportive",
            "logical",
            "intuitive",
            "collaborative",
            "adaptive",
        ]

        # ì»¨í…ìŠ¤íŠ¸ë³„ íŠ¹í™” ì „ëµ
        context_strategies = {
            "emotional": ["empathetic", "supportive", "intuitive"],
            "analytical": ["logical", "analytical", "systematic"],
            "creative": ["creative", "intuitive", "adaptive"],
            "collaborative": ["collaborative", "supportive", "empathetic"],
            "crisis": ["adaptive", "logical", "supportive"],
        }

        # ê°ì •ë³„ ì í•© ì „ëµ
        emotion_strategies = {
            "joy": ["empathetic", "creative", "collaborative"],
            "sadness": ["supportive", "empathetic", "intuitive"],
            "anger": ["analytical", "logical", "adaptive"],
            "fear": ["supportive", "logical", "empathetic"],
            "neutral": base_strategies,
        }

        # ìƒí™©ì— ë§ëŠ” ì „ëµ ì¡°í•©
        available = set(base_strategies)

        if context_type in context_strategies:
            available.update(context_strategies[context_type])

        if emotion_state in emotion_strategies:
            available.update(emotion_strategies[emotion_state])

        return list(available)

    def _select_strategy_epsilon_greedy(
        self,
        strategies: List[str],
        context_type: str,
        emotion_state: str,
        epsilon: float = 0.1,
    ) -> str:
        """Îµ-greedy ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì „ëµ ì„ íƒ"""
        # Exploration vs Exploitation
        if np.random.random() < epsilon:
            # íƒí—˜: ëœë¤ ì„ íƒ
            return np.random.choice(strategies)
        else:
            # í™œìš©: Q-valueê°€ ê°€ì¥ ë†’ì€ ì „ëµ ì„ íƒ
            best_strategy = strategies[0]
            best_q_value = float("-inf")

            for strategy in strategies:
                action_key = f"{context_type}:{emotion_state}:{strategy}"
                q_value = self.q_table.get(action_key, 0.0)

                if q_value > best_q_value:
                    best_q_value = q_value
                    best_strategy = strategy

            return best_strategy

    def _update_strategy_performance(
        self, action: ReinforcementAction, reward: float, feedback: Dict[str, Any]
    ):
        """ì „ëµë³„ ì„±ê³¼ í†µê³„ ì—…ë°ì´íŠ¸"""
        strategy = action.strategy

        if strategy not in self.strategy_performance:
            self.strategy_performance[strategy] = {
                "total_uses": 0,
                "successful_uses": 0,
                "total_reward": 0.0,
                "average_reward": 0.0,
                "best_reward": float("-inf"),
                "worst_reward": float("inf"),
                "last_used": None,
                "contexts": set(),
                "emotions": set(),
            }

        perf = self.strategy_performance[strategy]
        perf["total_uses"] += 1
        perf["total_reward"] += reward
        perf["average_reward"] = perf["total_reward"] / perf["total_uses"]
        perf["last_used"] = action.timestamp
        perf["contexts"].add(action.context_type)
        perf["emotions"].add(action.emotion_state)

        if reward > 0:
            perf["successful_uses"] += 1

        if reward > perf["best_reward"]:
            perf["best_reward"] = reward

        if reward < perf["worst_reward"]:
            perf["worst_reward"] = reward

    def _log_reinforcement_event(
        self, action: ReinforcementAction, reward: float, feedback: Dict[str, Any]
    ):
        """ê°•í™”í•™ìŠµ ì´ë²¤íŠ¸ ë¡œê¹…"""
        try:
            # ì§„í™” ì´ë²¤íŠ¸ ë¡œê¹…
            if log_evolution_event:
                event_data = {
                    "event": "Reinforcement Learning Update",
                    "tag": ["reinforcement", "q_learning", "strategy_optimization"],
                    "cause": [
                        f"action_{action.strategy}",
                        f"context_{action.context_type}",
                    ],
                    "effect": [f"reward_{reward:.3f}", "q_table_update"],
                    "resolution": f"strategy_confidence_updated",
                    "insight": f"Q-learning adaptation for {action.strategy}",
                    "adaptation_strength": abs(reward),
                    "coherence_improvement": max(0, reward),
                    "reflection_depth": 1,
                }
                log_evolution_event(event_data, f"reinforcement_{action.strategy}")

            # í˜ë¥´ì†Œë‚˜ ë©”íƒ€ ë¡œê±° ì—°ë™
            if get_persona_meta_logger:
                meta_logger = get_persona_meta_logger()
                meta_logger.log_flow_transition(
                    {
                        "event_type": "reinforcement_learning",
                        "action": asdict(action),
                        "reward": reward,
                        "feedback": feedback,
                        "q_table_size": len(self.q_table),
                        "learning_stats": {
                            "total_actions": self.total_actions,
                            "success_rate": self.successful_actions
                            / max(1, self.total_actions),
                        },
                    }
                )

        except Exception as e:
            print(f"âš ï¸ ê°•í™”í•™ìŠµ ì´ë²¤íŠ¸ ë¡œê¹… ì‹¤íŒ¨: {e}")

    def get_learning_statistics(self) -> Dict[str, Any]:
        """í•™ìŠµ í†µê³„ ë°˜í™˜"""
        success_rate = self.successful_actions / max(1, self.total_actions)

        # Q-table í†µê³„
        q_values = list(self.q_table.values())
        avg_q = np.mean(q_values) if q_values else 0.0
        max_q = np.max(q_values) if q_values else 0.0
        min_q = np.min(q_values) if q_values else 0.0

        # ìµœê³  ì„±ê³¼ ì „ëµë“¤
        top_strategies = sorted(
            self.strategy_performance.items(),
            key=lambda x: x[1]["average_reward"],
            reverse=True,
        )[:5]

        return {
            "total_actions": self.total_actions,
            "successful_actions": self.successful_actions,
            "success_rate": success_rate,
            "q_table_size": len(self.q_table),
            "learning_rate": self.learning_rate,
            "discount_factor": self.discount_factor,
            "q_statistics": {
                "average": avg_q,
                "maximum": max_q,
                "minimum": min_q,
                "std_dev": np.std(q_values) if q_values else 0.0,
            },
            "top_strategies": [
                {
                    "strategy": name,
                    "average_reward": perf["average_reward"],
                    "success_rate": perf["successful_uses"]
                    / max(1, perf["total_uses"]),
                    "total_uses": perf["total_uses"],
                }
                for name, perf in top_strategies
            ],
        }

    def save_q_table(self, filepath: str = "data/reinforcement_q_table.json"):
        """Q-table ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(filepath), exist_ok=True)

            save_data = {
                "q_table": self.q_table,
                "action_counts": self.action_counts,
                "last_rewards": self.last_rewards,
                "learning_stats": {
                    "total_actions": self.total_actions,
                    "successful_actions": self.successful_actions,
                    "learning_rate": self.learning_rate,
                    "discount_factor": self.discount_factor,
                },
                "strategy_performance": {
                    k: {
                        **v,
                        "contexts": list(v["contexts"]),
                        "emotions": list(v["emotions"]),
                    }
                    for k, v in self.strategy_performance.items()
                },
                "last_updated": datetime.now().isoformat(),
            }

            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)

            print(f"ğŸ’¾ Q-table ì €ì¥ ì™„ë£Œ: {filepath}")

        except Exception as e:
            print(f"âŒ Q-table ì €ì¥ ì‹¤íŒ¨: {e}")

    def load_q_table(self, filepath: str = "data/reinforcement_q_table.json"):
        """Q-table ë¡œë“œ"""
        try:
            if not os.path.exists(filepath):
                print(f"ğŸ“ Q-table íŒŒì¼ ì—†ìŒ, ìƒˆë¡œ ì‹œì‘: {filepath}")
                return

            with open(filepath, "r", encoding="utf-8") as f:
                data = json.load(f)

            self.q_table = data.get("q_table", {})
            self.action_counts = data.get("action_counts", {})
            self.last_rewards = data.get("last_rewards", {})

            stats = data.get("learning_stats", {})
            self.total_actions = stats.get("total_actions", 0)
            self.successful_actions = stats.get("successful_actions", 0)

            # ì „ëµ ì„±ê³¼ ë°ì´í„° ë¡œë“œ (set ë³µì›)
            perf_data = data.get("strategy_performance", {})
            for strategy, perf in perf_data.items():
                perf["contexts"] = set(perf.get("contexts", []))
                perf["emotions"] = set(perf.get("emotions", []))
            self.strategy_performance = perf_data

            print(f"ğŸ“‚ Q-table ë¡œë“œ ì™„ë£Œ: {len(self.q_table)}ê°œ ìƒíƒœ-ì•¡ì…˜ ìŒ")

        except Exception as e:
            print(f"âŒ Q-table ë¡œë“œ ì‹¤íŒ¨: {e}")

    def reset_learning(self):
        """í•™ìŠµ ë°ì´í„° ì´ˆê¸°í™”"""
        self.q_table.clear()
        self.action_counts.clear()
        self.last_rewards.clear()
        self.strategy_performance.clear()
        self.total_actions = 0
        self.successful_actions = 0
        self.learning_episodes = 0

        print("ğŸ”„ ê°•í™”í•™ìŠµ ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ")


# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
_reinforcement_engine = None


def get_reinforcement_engine() -> ReinforcementEngine:
    """ê¸€ë¡œë²Œ ê°•í™”í•™ìŠµ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _reinforcement_engine
    if _reinforcement_engine is None:
        _reinforcement_engine = ReinforcementEngine()
    return _reinforcement_engine


def apply_judgment_reward(
    judgment_result: Dict[str, Any], feedback: Dict[str, Any]
) -> float:
    """í¸ì˜ í•¨ìˆ˜: íŒë‹¨ ê²°ê³¼ì— ë³´ìƒ ì ìš©"""
    engine = get_reinforcement_engine()
    return engine.apply_reward(judgment_result, feedback)


def get_optimal_strategy(context: Dict[str, Any]) -> Dict[str, Any]:
    """í¸ì˜ í•¨ìˆ˜: ìµœì  ì „ëµ ì¶”ì²œ"""
    engine = get_reinforcement_engine()
    return engine.get_updated_strategy(context)

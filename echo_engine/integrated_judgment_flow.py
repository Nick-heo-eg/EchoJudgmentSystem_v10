#!/usr/bin/env python3
"""
ğŸ§© EchoJudgmentSystem v10.5 - Integrated Judgment Flow
Personaâ¨¯Reasoningâ¨¯Action í†µí•© íŒë‹¨ íë¦„

ì´ ëª¨ë“ˆì€ ë‹¤ìŒ êµ¬ì¡°ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤:
Signature â†’ Persona â†’ Emotion â†’ Strategy â†’ Reasoner â†’ Q-Table â†’ Judgment â†’ MetaLog

ì–‘ìª½ íŒë‹¨ íë¦„(LLM-Free, Claude)ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
"""

import json
import time
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum

# ë‚´ë¶€ ëª¨ë“ˆë“¤
try:
    from .persona_core_optimized_bridge import PersonaCore, PersonaProfile, create_persona_from_signature
    from .q_strategy_selector import (
        QTableStrategySelector,
        QState,
        QAction,
        ActionType,
        create_q_state_from_judgment_context,
    )
    from .shared_judgment_logic import (
        SharedJudgmentEngine,
        JudgmentRequest,
        JudgmentMode,
        get_shared_judgment_engine,
    )
except ImportError:
    # ê°œë°œ/í…ŒìŠ¤íŠ¸ ì‹œ ìƒëŒ€ ì„í¬íŠ¸
    import sys
    import os

    # sys.path ìˆ˜ì • ë¶ˆí•„ìš”
    from .persona_core_optimized_bridge import PersonaCore, PersonaProfile, create_persona_from_signature
    from q_strategy_selector import (
        QTableStrategySelector,
        QState,
        QAction,
        ActionType,
        create_q_state_from_judgment_context,
    )
    from shared_judgment_logic import (
        SharedJudgmentEngine,
        JudgmentRequest,
        JudgmentMode,
        get_shared_judgment_engine,
    )

# Claude Reasoning (ì„ íƒì )
try:
    import sys
    import os

    # sys.path ìˆ˜ì • ë¶ˆí•„ìš”
    from .reasoning import ReasoningChain, ReasoningStep

    REASONING_AVAILABLE = True
except ImportError:
    REASONING_AVAILABLE = False
    print("âš ï¸ Claude Reasoning ëª¨ë“ˆ ì—†ìŒ - ê¸°ë³¸ ì¶”ë¡  ì‚¬ìš©")


class JudgmentFlowMode(Enum):
    """íŒë‹¨ íë¦„ ëª¨ë“œ"""

    LLM_FREE = "llm_free"
    CLAUDE = "claude"
    HYBRID = "hybrid"
    PERSONA_ENHANCED = "persona_enhanced"


class ReasoningMode(Enum):
    """ì¶”ë¡  ëª¨ë“œ"""

    PATTERN_BASED = "pattern_based"  # LLM-Free íŒ¨í„´ ê¸°ë°˜
    CLAUDE_BASED = "claude_based"  # Claude ì¶”ë¡ 
    HYBRID_REASONING = "hybrid"  # í˜¼í•© ì¶”ë¡ 
    Q_ENHANCED = "q_enhanced"  # Q-Table ê°•í™”


@dataclass
class IntegratedJudgmentResult:
    """í†µí•© íŒë‹¨ ê²°ê³¼"""

    # í•µì‹¬ ê²°ê³¼
    judgment: str
    confidence: float

    # ê° ë‹¨ê³„ë³„ ê²°ê³¼
    signature_applied: str
    persona_analysis: Dict[str, Any]
    emotion_analysis: Dict[str, Any]
    strategy_selection: Dict[str, Any]
    reasoning_result: Dict[str, Any]
    q_action_selected: Dict[str, Any]

    # ë©”íƒ€ ì •ë³´
    processing_time: float
    flow_mode: JudgmentFlowMode
    reasoning_mode: ReasoningMode

    # í•™ìŠµ ë°ì´í„°
    persona_state: Dict[str, Any] = field(default_factory=dict)
    q_learning_reward: float = 0.0
    meta_insights: List[str] = field(default_factory=list)

    # ì¶”ì  ì •ë³´
    stage_timings: Dict[str, float] = field(default_factory=dict)
    debug_info: Dict[str, Any] = field(default_factory=dict)


class IntegratedJudgmentEngine:
    """í†µí•© íŒë‹¨ ì—”ì§„ - Personaâ¨¯Reasoningâ¨¯Action"""

    def __init__(
        self,
        default_signature: str = "Echo-Phoenix",
        flow_mode: JudgmentFlowMode = JudgmentFlowMode.HYBRID,
        reasoning_mode: ReasoningMode = ReasoningMode.PATTERN_BASED,
        enable_q_learning: bool = True,
        enable_meta_reflection: bool = True,
    ):
        """
        í†µí•© íŒë‹¨ ì—”ì§„ ì´ˆê¸°í™”

        Args:
            default_signature: ê¸°ë³¸ ì‹œê·¸ë‹ˆì²˜
            flow_mode: íŒë‹¨ íë¦„ ëª¨ë“œ
            reasoning_mode: ì¶”ë¡  ëª¨ë“œ
            enable_q_learning: Q-Learning í™œì„±í™”
            enable_meta_reflection: ë©”íƒ€ ë°˜ì„± í™œì„±í™”
        """
        self.default_signature = default_signature
        self.flow_mode = flow_mode
        self.reasoning_mode = reasoning_mode
        self.enable_q_learning = enable_q_learning
        self.enable_meta_reflection = enable_meta_reflection

        # ê° êµ¬ì„±ìš”ì†Œ ì´ˆê¸°í™”
        self._initialize_components()

        # ì„±ëŠ¥ í†µê³„
        self.total_judgments = 0
        self.successful_judgments = 0
        self.average_confidence = 0.0
        self.personas_used = {}

        print(f"ğŸ§© í†µí•© íŒë‹¨ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ê¸°ë³¸ ì‹œê·¸ë‹ˆì²˜: {default_signature}")
        print(f"   íë¦„ ëª¨ë“œ: {flow_mode.value}")
        print(f"   ì¶”ë¡  ëª¨ë“œ: {reasoning_mode.value}")
        print(f"   Q-Learning: {'í™œì„±í™”' if enable_q_learning else 'ë¹„í™œì„±í™”'}")

    def _initialize_components(self):
        """êµ¬ì„±ìš”ì†Œë“¤ ì´ˆê¸°í™”"""
        # 1. í˜ë¥´ì†Œë‚˜ í’€ ìƒì„±
        self.personas = {}
        signature_types = ["Echo-Aurora", "Echo-Phoenix", "Echo-Sage", "Echo-Companion"]

        for sig_type in signature_types:
            persona = create_persona_from_signature(sig_type)
            self.personas[sig_type] = persona

        print(f"ğŸ§  í˜ë¥´ì†Œë‚˜ {len(self.personas)}ê°œ ì´ˆê¸°í™” ì™„ë£Œ")

        # 2. ê³µí†µ íŒë‹¨ ì—”ì§„
        self.shared_engine = get_shared_judgment_engine()

        # 3. Q-Table ì „ëµ ì„ íƒê¸°
        if self.enable_q_learning:
            self.q_selector = QTableStrategySelector(
                learning_rate=0.1, exploration_rate=0.2, exploration_decay=0.99
            )
        else:
            self.q_selector = None

        # 4. Claude ì¶”ë¡  ì²´ì¸ (ì„ íƒì )
        if REASONING_AVAILABLE and self.reasoning_mode in [
            ReasoningMode.CLAUDE_BASED,
            ReasoningMode.HYBRID_REASONING,
        ]:
            self.reasoning_chain = ReasoningChain()
        else:
            self.reasoning_chain = None

        print(f"ğŸ§© ëª¨ë“  êµ¬ì„±ìš”ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")

    def run_integrated_judgment(
        self,
        text: str,
        context: Optional[str] = None,
        signature: Optional[str] = None,
        custom_settings: Optional[Dict[str, Any]] = None,
    ) -> IntegratedJudgmentResult:
        """
        í†µí•© íŒë‹¨ ì‹¤í–‰

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
            signature: ì‚¬ìš©í•  ì‹œê·¸ë‹ˆì²˜ (Noneì´ë©´ ê¸°ë³¸ê°’)
            custom_settings: ì»¤ìŠ¤í…€ ì„¤ì •

        Returns:
            í†µí•© íŒë‹¨ ê²°ê³¼
        """
        start_time = time.time()
        self.total_judgments += 1

        # ì‚¬ìš©í•  ì‹œê·¸ë‹ˆì²˜ ê²°ì •
        active_signature = signature or self.default_signature

        # ê²°ê³¼ ê°ì²´ ì´ˆê¸°í™”
        result = IntegratedJudgmentResult(
            judgment="",
            confidence=0.0,
            signature_applied=active_signature,
            persona_analysis={},
            emotion_analysis={},
            strategy_selection={},
            reasoning_result={},
            q_action_selected={},
            processing_time=0.0,
            flow_mode=self.flow_mode,
            reasoning_mode=self.reasoning_mode,
        )

        try:
            # === ë‹¨ê³„ 1: ì‹œê·¸ë‹ˆì²˜ â†’ í˜ë¥´ì†Œë‚˜ í™œì„±í™” ===
            stage_start = time.time()
            persona_result = self._activate_persona(active_signature, text, context)
            result.persona_analysis = persona_result
            result.stage_timings["persona_activation"] = time.time() - stage_start

            # === ë‹¨ê³„ 2: í˜ë¥´ì†Œë‚˜ â†’ ê°ì • ë¶„ì„ ===
            stage_start = time.time()
            emotion_result = self._analyze_emotion_with_persona(
                text, context, persona_result
            )
            result.emotion_analysis = emotion_result
            result.stage_timings["emotion_analysis"] = time.time() - stage_start

            # === ë‹¨ê³„ 3: ê°ì • â†’ ì „ëµ ì„ íƒ ===
            stage_start = time.time()
            strategy_result = self._select_strategy_with_context(
                emotion_result, persona_result, context
            )
            result.strategy_selection = strategy_result
            result.stage_timings["strategy_selection"] = time.time() - stage_start

            # === ë‹¨ê³„ 4: ì „ëµ â†’ ì¶”ë¡  ìˆ˜í–‰ ===
            stage_start = time.time()
            reasoning_result = self._perform_reasoning(
                text, context, emotion_result, strategy_result
            )
            result.reasoning_result = reasoning_result
            result.stage_timings["reasoning"] = time.time() - stage_start

            # === ë‹¨ê³„ 5: Q-Table í–‰ë™ ì„ íƒ ===
            stage_start = time.time()
            q_action_result = self._select_q_action(
                emotion_result, strategy_result, reasoning_result
            )
            result.q_action_selected = q_action_result
            result.stage_timings["q_action_selection"] = time.time() - stage_start

            # === ë‹¨ê³„ 6: ìµœì¢… íŒë‹¨ ìƒì„± ===
            stage_start = time.time()
            final_judgment = self._generate_integrated_judgment(
                text,
                context,
                persona_result,
                emotion_result,
                strategy_result,
                reasoning_result,
                q_action_result,
            )
            result.judgment = final_judgment["judgment"]
            result.confidence = final_judgment["confidence"]
            result.stage_timings["judgment_generation"] = time.time() - stage_start

            # === ë‹¨ê³„ 7: ë©”íƒ€ ë¡œê·¸ ë° í•™ìŠµ ===
            stage_start = time.time()
            meta_result = self._perform_meta_learning(result, custom_settings)
            result.meta_insights = meta_result.get("insights", [])
            result.q_learning_reward = meta_result.get("reward", 0.0)
            result.stage_timings["meta_learning"] = time.time() - stage_start

            # ì„±ê³µ í†µê³„ ì—…ë°ì´íŠ¸
            self.successful_judgments += 1

        except Exception as e:
            # ì˜¤ë¥˜ ì²˜ë¦¬
            result.judgment = f"í†µí•© íŒë‹¨ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)[:100]}"
            result.confidence = 0.0
            result.debug_info["error"] = str(e)
            print(f"âŒ í†µí•© íŒë‹¨ ì˜¤ë¥˜: {e}")

        # ìµœì¢… ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        result.processing_time = time.time() - start_time

        # í‰ê·  ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
        total_confidence = (
            self.average_confidence * (self.total_judgments - 1) + result.confidence
        )
        self.average_confidence = total_confidence / self.total_judgments

        return result

    def _activate_persona(
        self, signature: str, text: str, context: Optional[str]
    ) -> Dict[str, Any]:
        """í˜ë¥´ì†Œë‚˜ í™œì„±í™” ë° ë¶„ì„"""
        if signature not in self.personas:
            signature = self.default_signature

        persona = self.personas[signature]

        # í˜ë¥´ì†Œë‚˜ë³„ ì‚¬ìš© í†µê³„
        if signature not in self.personas_used:
            self.personas_used[signature] = 0
        self.personas_used[signature] += 1

        # í˜ë¥´ì†Œë‚˜ ì²˜ë¦¬
        persona_context = {
            "context_type": self._infer_context_type(context or ""),
            "urgency": self._infer_urgency(text),
            "complexity": self._infer_complexity(text),
        }

        persona_analysis = persona.process_input(text, persona_context)

        return {
            "signature_used": signature,
            "persona_name": persona_analysis["persona_name"],
            "persona_state": persona_analysis["persona_state"],
            "persona_confidence": persona_analysis["persona_confidence"],
            "energy_level": persona_analysis["energy_level"],
            "interaction_count": persona_analysis["interaction_count"],
            "meta_insights": persona_analysis["meta_insights"],
            "raw_persona_analysis": persona_analysis,
        }

    def _analyze_emotion_with_persona(
        self, text: str, context: Optional[str], persona_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """í˜ë¥´ì†Œë‚˜ ê°•í™”ëœ ê°ì • ë¶„ì„"""
        # ê¸°ë³¸ ê°ì • ë¶„ì„ (ê³µí†µ ì—”ì§„ ì‚¬ìš©)
        shared_request = JudgmentRequest(
            text=text,
            context=context,
            judgment_mode=JudgmentMode.HYBRID,
            include_emotion=True,
            include_strategy=False,
            include_context=True,
        )

        shared_result = self.shared_engine.process_judgment(shared_request)

        # í˜ë¥´ì†Œë‚˜ ê°ì • ë¶„ì„ ê°€ì ¸ì˜¤ê¸°
        persona_emotion = persona_result["raw_persona_analysis"]["emotion_analysis"]

        # ê°ì • ìœµí•© (í˜ë¥´ì†Œë‚˜ + ê³µí†µ ì—”ì§„)
        shared_emotion = shared_result.emotion_detected
        shared_confidence = shared_result.confidence

        persona_emotion_detected = persona_emotion["primary_emotion"]
        persona_confidence = persona_emotion["intensity"]

        # ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ í‰ê· 
        if shared_confidence > persona_confidence:
            final_emotion = shared_emotion
            final_confidence = shared_confidence * 0.7 + persona_confidence * 0.3
        else:
            final_emotion = persona_emotion_detected
            final_confidence = persona_confidence * 0.7 + shared_confidence * 0.3

        return {
            "primary_emotion": final_emotion,
            "confidence": final_confidence,
            "shared_engine_emotion": shared_emotion,
            "persona_emotion": persona_emotion_detected,
            "emotion_intensity": persona_emotion["intensity"],
            "intensity_category": persona_emotion["intensity_category"],
            "trigger_activated": persona_emotion.get("trigger_activated", False),
            "fusion_method": "confidence_weighted",
        }

    def _select_strategy_with_context(
        self,
        emotion_result: Dict[str, Any],
        persona_result: Dict[str, Any],
        context: Optional[str],
    ) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ê°•í™”ëœ ì „ëµ ì„ íƒ"""
        # í˜ë¥´ì†Œë‚˜ ì „ëµ ê°€ì ¸ì˜¤ê¸°
        persona_strategy = persona_result["raw_persona_analysis"]["strategy_selection"]

        # Q-Table ì¶”ì²œ (ìˆë‹¤ë©´)
        q_recommendation = None
        if self.enable_q_learning and self.q_selector:
            q_state = create_q_state_from_judgment_context(
                emotion_result["primary_emotion"],
                emotion_result["confidence"],
                self._infer_context_type(context or ""),
                self._infer_urgency(context or ""),
                persona_result["energy_level"] / 100.0,  # ì •ê·œí™”
                persona_result["persona_confidence"],
            )

            q_recommendation = self.q_selector.get_strategy_recommendations(q_state)

        # ì „ëµ í†µí•© ê²°ì •
        primary_strategy = persona_strategy["primary_strategy"]
        strategy_confidence = persona_strategy["confidence"]

        if q_recommendation and q_recommendation["primary_strategy"]:
            q_strategy = q_recommendation["primary_strategy"]
            q_confidence = (
                q_recommendation["strategy_scores"]
                .get(q_strategy, {})
                .get("score", 0.0)
            )

            # Q-Table ì¶”ì²œì´ ë” ì‹ ë¢°ë„ê°€ ë†’ìœ¼ë©´ ì‚¬ìš©
            if q_confidence > strategy_confidence:
                primary_strategy = q_strategy
                strategy_confidence = q_confidence

        return {
            "primary_strategy": primary_strategy,
            "confidence": strategy_confidence,
            "persona_strategy": persona_strategy["primary_strategy"],
            "q_recommended_strategy": (
                q_recommendation["primary_strategy"] if q_recommendation else None
            ),
            "alternative_strategies": persona_strategy.get("alternatives", []),
            "strategy_reasoning": f"í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ì „ëµ ì„ íƒ (Q-Table ë³´ê°•: {'ì˜ˆ' if q_recommendation else 'ì•„ë‹ˆì˜¤'})",
        }

    def _perform_reasoning(
        self,
        text: str,
        context: Optional[str],
        emotion_result: Dict[str, Any],
        strategy_result: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ì¶”ë¡  ìˆ˜í–‰ (ëª¨ë“œì— ë”°ë¼)"""
        reasoning_start = time.time()

        if self.reasoning_mode == ReasoningMode.CLAUDE_BASED and self.reasoning_chain:
            # Claude ê¸°ë°˜ ì¶”ë¡ 
            reasoning_result = self._claude_reasoning(
                text, context, emotion_result, strategy_result
            )
        elif (
            self.reasoning_mode == ReasoningMode.HYBRID_REASONING
            and self.reasoning_chain
        ):
            # í˜¼í•© ì¶”ë¡ 
            reasoning_result = self._hybrid_reasoning(
                text, context, emotion_result, strategy_result
            )
        else:
            # íŒ¨í„´ ê¸°ë°˜ ì¶”ë¡  (ê¸°ë³¸)
            reasoning_result = self._pattern_based_reasoning(
                text, context, emotion_result, strategy_result
            )

        reasoning_time = time.time() - reasoning_start
        reasoning_result["reasoning_time"] = reasoning_time
        reasoning_result["reasoning_mode"] = self.reasoning_mode.value

        return reasoning_result

    def _claude_reasoning(
        self,
        text: str,
        context: Optional[str],
        emotion_result: Dict[str, Any],
        strategy_result: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Claude ê¸°ë°˜ ì¶”ë¡ """
        if not self.reasoning_chain:
            return self._pattern_based_reasoning(
                text, context, emotion_result, strategy_result
            )

        try:
            # Claude ì¶”ë¡  ì²´ì¸ ì‹¤í–‰
            reasoning_context = {
                "input_text": text,
                "context": context or "",
                "detected_emotion": emotion_result["primary_emotion"],
                "emotion_confidence": emotion_result["confidence"],
                "selected_strategy": strategy_result["primary_strategy"],
                "strategy_confidence": strategy_result["confidence"],
            }

            steps = [
                ReasoningStep(
                    "emotion_validation",
                    f"ê°ì • '{emotion_result['primary_emotion']}' ë¶„ì„ì„ ê²€ì¦í•˜ì„¸ìš”.",
                ),
                ReasoningStep(
                    "strategy_evaluation",
                    f"ì „ëµ '{strategy_result['primary_strategy']}' ì ì ˆì„±ì„ í‰ê°€í•˜ì„¸ìš”.",
                ),
                ReasoningStep(
                    "context_integration", "ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì™€ ìƒí™©ì„ ì¢…í•© ë¶„ì„í•˜ì„¸ìš”."
                ),
                ReasoningStep(
                    "final_synthesis", "ìµœì¢… íŒë‹¨ì„ ìœ„í•œ ì¢…í•©ì  ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ì„¸ìš”."
                ),
            ]

            reasoning_result = self.reasoning_chain.execute_chain(
                steps, reasoning_context
            )

            return {
                "reasoning_type": "claude_based",
                "reasoning_steps": reasoning_result.get("steps", []),
                "final_reasoning": reasoning_result.get(
                    "final_conclusion", "ì¶”ë¡  ì™„ë£Œ"
                ),
                "reasoning_confidence": reasoning_result.get("confidence", 0.7),
                "claude_insights": reasoning_result.get("insights", []),
            }

        except Exception as e:
            print(f"âš ï¸ Claude ì¶”ë¡  ì‹¤íŒ¨, íŒ¨í„´ ê¸°ë°˜ìœ¼ë¡œ í´ë°±: {e}")
            return self._pattern_based_reasoning(
                text, context, emotion_result, strategy_result
            )

    def _hybrid_reasoning(
        self,
        text: str,
        context: Optional[str],
        emotion_result: Dict[str, Any],
        strategy_result: Dict[str, Any],
    ) -> Dict[str, Any]:
        """í˜¼í•© ì¶”ë¡  (íŒ¨í„´ + Claude)"""
        # íŒ¨í„´ ê¸°ë°˜ ì¶”ë¡ 
        pattern_result = self._pattern_based_reasoning(
            text, context, emotion_result, strategy_result
        )

        # Claude ì¶”ë¡  (ë³´ê°•ìš©)
        claude_result = self._claude_reasoning(
            text, context, emotion_result, strategy_result
        )

        # ë‘ ê²°ê³¼ ìœµí•©
        hybrid_confidence = (
            pattern_result["reasoning_confidence"]
            + claude_result.get("reasoning_confidence", 0.5)
        ) / 2

        return {
            "reasoning_type": "hybrid",
            "pattern_reasoning": pattern_result["reasoning_logic"],
            "claude_reasoning": claude_result.get("final_reasoning", ""),
            "hybrid_insights": pattern_result["insights"]
            + claude_result.get("claude_insights", []),
            "reasoning_confidence": hybrid_confidence,
            "fusion_method": "weighted_average",
        }

    def _pattern_based_reasoning(
        self,
        text: str,
        context: Optional[str],
        emotion_result: Dict[str, Any],
        strategy_result: Dict[str, Any],
    ) -> Dict[str, Any]:
        """íŒ¨í„´ ê¸°ë°˜ ì¶”ë¡  (ê¸°ë³¸)"""
        emotion = emotion_result["primary_emotion"]
        strategy = strategy_result["primary_strategy"]

        # ê¸°ë³¸ ì¶”ë¡  ë¡œì§
        reasoning_logic = []

        # ê°ì • ê¸°ë°˜ ì¶”ë¡ 
        if emotion in ["joy", "surprise"]:
            reasoning_logic.append("ê¸ì •ì  ê°ì • ìƒíƒœì´ë¯€ë¡œ ì ê·¹ì  ì ‘ê·¼ì´ ìœ íš¨í•©ë‹ˆë‹¤.")
        elif emotion in ["sadness", "fear"]:
            reasoning_logic.append(
                "ë¶€ì •ì  ê°ì • ìƒíƒœì´ë¯€ë¡œ ì‹ ì¤‘í•˜ê³  ì§€ì§€ì ì¸ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤."
            )
        elif emotion == "anger":
            reasoning_logic.append(
                "ë¶„ë…¸ ê°ì •ì´ë¯€ë¡œ ì¹¨ì°©í•˜ê³  ë‹¨ê³„ì ì¸ ì ‘ê·¼ì´ ì¤‘ìš”í•©ë‹ˆë‹¤."
            )
        else:
            reasoning_logic.append("ì¤‘ë¦½ì  ê°ì • ìƒíƒœì´ë¯€ë¡œ ê· í˜•ì¡íŒ ì ‘ê·¼ì´ ì ì ˆí•©ë‹ˆë‹¤.")

        # ì „ëµ ê¸°ë°˜ ì¶”ë¡ 
        if strategy == "empathetic":
            reasoning_logic.append("ê³µê°ì  ì „ëµì„ í†µí•´ ê°ì •ì  ì—°ê²°ì„ ê°•í™”í•©ë‹ˆë‹¤.")
        elif strategy == "analytical":
            reasoning_logic.append("ë¶„ì„ì  ì „ëµì„ í†µí•´ ë…¼ë¦¬ì  í•´ê²°ì±…ì„ ëª¨ìƒ‰í•©ë‹ˆë‹¤.")
        elif strategy == "supportive":
            reasoning_logic.append("ì§€ì§€ì  ì „ëµì„ í†µí•´ ì•ˆì •ê°ì„ ì œê³µí•©ë‹ˆë‹¤.")

        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ë¡ 
        if context:
            if "ì—…ë¬´" in context or "work" in context.lower():
                reasoning_logic.append(
                    "ì—…ë¬´ ìƒí™©ì´ë¯€ë¡œ ì „ë¬¸ì ì´ê³  ì²´ê³„ì ì¸ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤."
                )
            elif "ê°œì¸" in context or "personal" in context.lower():
                reasoning_logic.append(
                    "ê°œì¸ì  ìƒí™©ì´ë¯€ë¡œ ë”°ëœ»í•˜ê³  ì´í•´ì ì¸ ì ‘ê·¼ì´ ì í•©í•©ë‹ˆë‹¤."
                )

        # ì¶”ë¡  ì‹ ë¢°ë„ ê³„ì‚°
        confidence_factors = [
            emotion_result["confidence"],
            strategy_result["confidence"],
            0.6 if context else 0.4,  # ì»¨í…ìŠ¤íŠ¸ ìœ ë¬´
        ]
        reasoning_confidence = sum(confidence_factors) / len(confidence_factors)

        return {
            "reasoning_type": "pattern_based",
            "reasoning_logic": reasoning_logic,
            "reasoning_confidence": reasoning_confidence,
            "insights": [
                f"ê°ì • '{emotion}' ê¸°ë°˜ ì¶”ë¡  ì ìš©",
                f"ì „ëµ '{strategy}' ê¸°ë°˜ ë…¼ë¦¬ êµ¬ì„±",
                f"ì¶”ë¡  ì‹ ë¢°ë„: {reasoning_confidence:.3f}",
            ],
        }

    def _select_q_action(
        self,
        emotion_result: Dict[str, Any],
        strategy_result: Dict[str, Any],
        reasoning_result: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Q-Table ê¸°ë°˜ í–‰ë™ ì„ íƒ"""
        if not self.enable_q_learning or not self.q_selector:
            return {
                "action_selected": None,
                "q_learning_enabled": False,
                "fallback_reason": "Q-Learning ë¹„í™œì„±í™”",
            }

        try:
            # í˜„ì¬ ìƒíƒœ êµ¬ì„±
            q_state = create_q_state_from_judgment_context(
                emotion_result["primary_emotion"],
                emotion_result["confidence"],
                "general",  # ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸
                "normal",  # ê¸°ë³¸ ê¸´ê¸‰ë„
                0.7,  # ê¸°ë³¸ ì—ë„ˆì§€
                reasoning_result["reasoning_confidence"],
            )

            # ì‚¬ìš© ê°€ëŠ¥í•œ ì „ëµë“¤
            available_strategies = [strategy_result["primary_strategy"]]
            if strategy_result.get("alternative_strategies"):
                available_strategies.extend(
                    strategy_result["alternative_strategies"][:2]
                )

            # Q-Table í–‰ë™ ì„ íƒ
            selected_action = self.q_selector.select_action(
                q_state, available_strategies
            )

            return {
                "action_selected": {
                    "type": selected_action.action_type.value,
                    "strategy": selected_action.strategy,
                    "intensity": selected_action.intensity,
                },
                "q_state_key": q_state.to_key(),
                "available_strategies": available_strategies,
                "q_learning_enabled": True,
                "selection_method": "q_table_policy",
            }

        except Exception as e:
            print(f"âš ï¸ Q-Action ì„ íƒ ì‹¤íŒ¨: {e}")
            return {
                "action_selected": None,
                "q_learning_enabled": False,
                "error": str(e),
            }

    def _generate_integrated_judgment(
        self,
        text: str,
        context: Optional[str],
        persona_result: Dict[str, Any],
        emotion_result: Dict[str, Any],
        strategy_result: Dict[str, Any],
        reasoning_result: Dict[str, Any],
        q_action_result: Dict[str, Any],
    ) -> Dict[str, Any]:
        """í†µí•© íŒë‹¨ ìƒì„±"""

        # ê¸°ë³¸ íŒë‹¨ í…œí”Œë¦¿ êµ¬ì„±
        emotion = emotion_result["primary_emotion"]
        strategy = strategy_result["primary_strategy"]
        persona_name = persona_result["persona_name"]

        # íŒë‹¨ ìƒì„± ë¡œì§
        judgment_parts = []

        # í˜ë¥´ì†Œë‚˜ ì¸ì‚¬ì´íŠ¸
        if persona_result.get("meta_insights"):
            judgment_parts.append(
                f"[{persona_name}ì˜ ê´€ì ] {persona_result['meta_insights'][0]}"
            )

        # ê°ì • ê¸°ë°˜ íŒë‹¨
        emotion_judgments = {
            "joy": "ê¸ì •ì ì¸ ì—ë„ˆì§€ê°€ ëŠê»´ì§‘ë‹ˆë‹¤. ì´ ê¸°ì¨ì„ í™œìš©í•˜ì—¬ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”.",
            "sadness": "ì–´ë ¤ìš´ ì‹œê°„ì´ì§€ë§Œ, ì´ëŸ° ê°ì •ë„ ì„±ì¥ì˜ ê¸°íšŒì…ë‹ˆë‹¤. ì²œì²œíˆ ê·¹ë³µí•´ë‚˜ê°€ì‹œê¸¸ ë°”ëë‹ˆë‹¤.",
            "anger": "í™”ê°€ ë‚˜ëŠ” ìƒí™©ì´ì§€ë§Œ, ì ì‹œ ìˆ¨ì„ ê³ ë¥´ê³  ëƒ‰ì •í•˜ê²Œ ì ‘ê·¼í•´ë³´ì‹œëŠ” ê²ƒì´ ì¢‹ê² ìŠµë‹ˆë‹¤.",
            "fear": "ë¶ˆì•ˆí•œ ë§ˆìŒì´ ë“¤ì§€ë§Œ, ì°¨ê·¼ì°¨ê·¼ ì¤€ë¹„í•˜ê³  ëŒ€ì²˜í•˜ë©´ ì¶©ë¶„íˆ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "surprise": "ì˜ˆìƒì¹˜ ëª»í•œ ìƒí™©ì´ë„¤ìš”. ìƒˆë¡œìš´ ê´€ì ìœ¼ë¡œ ì ‘ê·¼í•´ë³´ì‹œëŠ” ê²ƒì€ ì–´ë–¨ê¹Œìš”?",
            "neutral": "í˜„ì¬ ìƒí™©ì„ ì°¨ë¶„íˆ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë°©í–¥ì„ ì°¾ì•„ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.",
        }

        base_judgment = emotion_judgments.get(
            emotion, "ìƒí™©ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ì‹ ì¤‘í•˜ê²Œ ì ‘ê·¼í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        )
        judgment_parts.append(base_judgment)

        # ì „ëµ ê¸°ë°˜ ì¡°ì–¸
        strategy_advice = {
            "empathetic": "ë‹¤ë¥¸ ì‚¬ëŒë“¤ì˜ ê°ì •ê³¼ ì…ì¥ì„ ì¶©ë¶„íˆ ê³ ë ¤í•˜ì—¬ ì§„í–‰í•˜ì„¸ìš”.",
            "analytical": "ë°ì´í„°ì™€ ë…¼ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì ‘ê·¼í•´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.",
            "supportive": "ì£¼ë³€ì˜ ì§€ì§€ì™€ ë„ì›€ì„ ë°›ìœ¼ë©° í•¨ê»˜ í•´ê²°í•´ë‚˜ê°€ì‹œê¸¸ ë°”ëë‹ˆë‹¤.",
            "creative": "ê¸°ì¡´ê³¼ ë‹¤ë¥¸ ì°½ì˜ì ì¸ ë°©ë²•ì„ ì‹œë„í•´ë³´ì‹œëŠ” ê²ƒì´ ì¢‹ê² ìŠµë‹ˆë‹¤.",
            "cautious": "ì‹ ì¤‘í•˜ê²Œ ê²€í† í•˜ê³  ì¤€ë¹„ë¥¼ ì¶©ë¶„íˆ í•œ í›„ ì§„í–‰í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
            "balanced": "ë‹¤ì–‘í•œ ì¸¡ë©´ì„ ê· í˜•ìˆê²Œ ê³ ë ¤í•˜ì—¬ ê²°ì •í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
        }

        if strategy in strategy_advice:
            judgment_parts.append(strategy_advice[strategy])

        # ì¶”ë¡  ê²°ê³¼ ë°˜ì˜
        if reasoning_result.get("insights"):
            key_insight = reasoning_result["insights"][0]
            judgment_parts.append(f"[ì¶”ë¡  ê²°ê³¼] {key_insight}")

        # Q-Action ë°˜ì˜
        if q_action_result.get("action_selected"):
            action = q_action_result["action_selected"]
            if action["type"] == "meta_reflection":
                judgment_parts.append(
                    "ì´ ìƒí™©ì—ì„œ í•œ ë²ˆ ë” ê¹Šì´ ìƒê°í•´ë³´ì‹œëŠ” ê²ƒì´ ë„ì›€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤."
                )

        # ìµœì¢… íŒë‹¨ ì¡°í•©
        final_judgment = " ".join(judgment_parts)

        # ì‹ ë¢°ë„ ê³„ì‚° (ëª¨ë“  ë‹¨ê³„ì˜ ê°€ì¤‘ í‰ê· )
        confidence_components = [
            emotion_result["confidence"] * 0.25,
            strategy_result["confidence"] * 0.25,
            reasoning_result["reasoning_confidence"] * 0.3,
            persona_result["persona_confidence"] * 0.2,
        ]

        final_confidence = sum(confidence_components)
        final_confidence = max(0.1, min(0.95, final_confidence))  # ë²”ìœ„ ì œí•œ

        return {
            "judgment": final_judgment,
            "confidence": final_confidence,
            "judgment_components": len(judgment_parts),
            "primary_influence": "integrated_analysis",
        }

    def _perform_meta_learning(
        self,
        result: IntegratedJudgmentResult,
        custom_settings: Optional[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """ë©”íƒ€ í•™ìŠµ ë° Q-Table ì—…ë°ì´íŠ¸"""
        insights = []
        reward = 0.0

        try:
            # Q-Learning ë³´ìƒ ê³„ì‚° ë° ì—…ë°ì´íŠ¸
            if (
                self.enable_q_learning
                and self.q_selector
                and result.q_action_selected.get("action_selected")
            ):
                # íŒë‹¨ ê²°ê³¼ë¡œë¶€í„° ë³´ìƒ ê³„ì‚°
                judgment_result = {
                    "confidence": result.confidence,
                    "processing_time": result.processing_time,
                    "error_occurred": result.confidence < 0.3,
                    "emotion_detected": result.emotion_analysis["primary_emotion"],
                    "strategy_suggested": result.strategy_selection["primary_strategy"],
                }

                reward = self.q_selector.calculate_reward(judgment_result)

                # Q-ê°’ ì—…ë°ì´íŠ¸ (ë‹¨ìˆœí™”ëœ ë²„ì „)
                if result.q_action_selected.get("q_state_key"):
                    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ìƒíƒœ ì „ì´ í•„ìš”
                    insights.append(f"Q-Learning ë³´ìƒ: {reward:.3f}")

            # í˜ë¥´ì†Œë‚˜ í•™ìŠµ í”¼ë“œë°±
            if result.persona_analysis and result.confidence > 0.6:
                insights.append(
                    f"í˜ë¥´ì†Œë‚˜ '{result.persona_analysis['persona_name']}' ì„±ê³µì  ì ìš©"
                )

            # ì¶”ë¡  ëª¨ë“œ íš¨ê³¼ì„± ë¶„ì„
            if result.reasoning_result:
                reasoning_confidence = result.reasoning_result.get(
                    "reasoning_confidence", 0.5
                )
                if reasoning_confidence > 0.7:
                    insights.append(f"ì¶”ë¡  ëª¨ë“œ '{result.reasoning_mode.value}' íš¨ê³¼ì ")

            # ì²˜ë¦¬ ì‹œê°„ ë¶„ì„
            if result.processing_time > 5.0:
                insights.append("ì²˜ë¦¬ ì‹œê°„ ìµœì í™” í•„ìš”")
            elif result.processing_time < 1.0:
                insights.append("ë¹ ë¥¸ ì²˜ë¦¬ ì„±ê³µ")

        except Exception as e:
            insights.append(f"ë©”íƒ€ í•™ìŠµ ë¶€ë¶„ ì‹¤íŒ¨: {str(e)[:50]}")

        return {
            "insights": insights,
            "reward": reward,
            "learning_applied": self.enable_q_learning,
            "meta_reflection_enabled": self.enable_meta_reflection,
        }

    def _infer_context_type(self, text: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ íƒ€ì… ì¶”ë¡ """
        text_lower = text.lower()

        if any(
            word in text_lower
            for word in ["ì—…ë¬´", "íšŒì˜", "ì§ì¥", "í”„ë¡œì íŠ¸", "work", "meeting"]
        ):
            return "work"
        elif any(
            word in text_lower
            for word in ["ì¹œêµ¬", "ê°€ì¡±", "ì—°ì¸", "ê°œì¸", "friend", "family"]
        ):
            return "personal"
        elif any(
            word in text_lower for word in ["ê³µë¶€", "í•™êµ", "ì‹œí—˜", "study", "school"]
        ):
            return "academic"
        elif any(
            word in text_lower
            for word in ["ì°½ì˜", "ì•„ì´ë””ì–´", "í˜ì‹ ", "creative", "idea"]
        ):
            return "creative"
        else:
            return "general"

    def _infer_urgency(self, text: str) -> str:
        """ê¸´ê¸‰ë„ ì¶”ë¡ """
        text_lower = text.lower()

        if any(
            word in text_lower
            for word in ["ê¸´ê¸‰", "ê¸‰í•´", "ë¹¨ë¦¬", "ì¦‰ì‹œ", "urgent", "quickly"]
        ):
            return "high"
        elif any(
            word in text_lower
            for word in ["ì²œì²œíˆ", "ë‚˜ì¤‘ì—", "ì—¬ìœ ", "slowly", "later"]
        ):
            return "low"
        else:
            return "normal"

    def _infer_complexity(self, text: str) -> str:
        """ë³µì¡ë„ ì¶”ë¡ """
        if len(text) > 200:
            return "high"
        elif len(text) > 50:
            return "medium"
        else:
            return "low"

    def get_engine_stats(self) -> Dict[str, Any]:
        """ì—”ì§„ í†µê³„ ë°˜í™˜"""
        success_rate = (self.successful_judgments / max(self.total_judgments, 1)) * 100

        stats = {
            "total_judgments": self.total_judgments,
            "successful_judgments": self.successful_judgments,
            "success_rate": success_rate,
            "average_confidence": self.average_confidence,
            "flow_mode": self.flow_mode.value,
            "reasoning_mode": self.reasoning_mode.value,
            "q_learning_enabled": self.enable_q_learning,
            "meta_reflection_enabled": self.enable_meta_reflection,
            "personas_used": dict(self.personas_used),
        }

        # Q-Learning í†µê³„ ì¶”ê°€
        if self.q_selector:
            stats["q_learning_stats"] = self.q_selector.get_stats()

        # í˜ë¥´ì†Œë‚˜ í†µê³„ ì¶”ê°€
        persona_stats = {}
        for sig_type, persona in self.personas.items():
            persona_status = persona.get_status()
            persona_stats[sig_type] = {
                "interaction_count": persona_status["interaction_count"],
                "confidence": persona_status["confidence"],
                "energy_level": persona_status["energy_level"],
            }
        stats["persona_stats"] = persona_stats

        return stats


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_integrated_engine(
    signature: str = "Echo-Phoenix",
    flow_mode: str = "hybrid",
    reasoning_mode: str = "pattern_based",
) -> IntegratedJudgmentEngine:
    """í†µí•© ì—”ì§„ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    flow_mode_enum = JudgmentFlowMode(flow_mode)
    reasoning_mode_enum = ReasoningMode(reasoning_mode)

    return IntegratedJudgmentEngine(
        default_signature=signature,
        flow_mode=flow_mode_enum,
        reasoning_mode=reasoning_mode_enum,
    )


def quick_integrated_judgment(
    text: str, context: Optional[str] = None, signature: str = "Echo-Phoenix"
) -> Dict[str, Any]:
    """ë¹ ë¥¸ í†µí•© íŒë‹¨ ì‹¤í–‰"""
    engine = create_integrated_engine(signature)
    result = engine.run_integrated_judgment(text, context)

    return {
        "judgment": result.judgment,
        "confidence": result.confidence,
        "signature": result.signature_applied,
        "emotion": result.emotion_analysis.get("primary_emotion", "neutral"),
        "strategy": result.strategy_selection.get("primary_strategy", "balanced"),
        "processing_time": result.processing_time,
    }


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§© í†µí•© íŒë‹¨ íë¦„ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # í†µí•© ì—”ì§„ ìƒì„±
    engine = create_integrated_engine(
        signature="Echo-Phoenix", flow_mode="hybrid", reasoning_mode="pattern_based"
    )

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
    test_cases = [
        {
            "text": "ì˜¤ëŠ˜ í”„ë¡œì íŠ¸ì—ì„œ í° ì‹¤íŒ¨ë¥¼ í–ˆì§€ë§Œ ë‹¤ì‹œ ë„ì „í•˜ê³  ì‹¶ì–´ìš”",
            "context": "ì—…ë¬´ ìƒí™©ì—ì„œ ì¢Œì ˆê°ì„ ëŠë¼ê³  ìˆìŠµë‹ˆë‹¤",
            "signature": "Echo-Phoenix",
        },
        {
            "text": "ìƒˆë¡œìš´ ì•„ì´ë””ì–´ê°€ ìˆëŠ”ë° íŒ€ì›ë“¤ì„ ì–´ë–»ê²Œ ì„¤ë“í•´ì•¼ í• ê¹Œìš”?",
            "context": "ì°½ì˜ì ì¸ ì—…ë¬´ í™˜ê²½",
            "signature": "Echo-Sage",
        },
        {
            "text": "ì¹œêµ¬ì™€ ê°ˆë“±ì´ ìˆì–´ì„œ ë§ˆìŒì´ ì•„í”•ë‹ˆë‹¤",
            "context": "ê°œì¸ì ì¸ ì¸ê°„ê´€ê³„ ë¬¸ì œ",
            "signature": "Echo-Aurora",
        },
    ]

    # ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'='*20} í…ŒìŠ¤íŠ¸ {i} {'='*20}")
        print(f"ğŸ“ ì…ë ¥: {test_case['text']}")
        print(f"ğŸ·ï¸ ì»¨í…ìŠ¤íŠ¸: {test_case['context']}")
        print(f"ğŸ­ ì‹œê·¸ë‹ˆì²˜: {test_case['signature']}")

        # í†µí•© íŒë‹¨ ì‹¤í–‰
        result = engine.run_integrated_judgment(
            text=test_case["text"],
            context=test_case["context"],
            signature=test_case["signature"],
        )

        print(f"\nğŸ“Š ê²°ê³¼:")
        print(f"   íŒë‹¨: {result.judgment}")
        print(f"   ì‹ ë¢°ë„: {result.confidence:.3f}")
        print(f"   ê°ì •: {result.emotion_analysis.get('primary_emotion', 'N/A')}")
        print(f"   ì „ëµ: {result.strategy_selection.get('primary_strategy', 'N/A')}")
        print(f"   í˜ë¥´ì†Œë‚˜: {result.persona_analysis.get('persona_name', 'N/A')}")
        print(f"   ì²˜ë¦¬ì‹œê°„: {result.processing_time:.3f}ì´ˆ")

        # ë‹¨ê³„ë³„ ì‹œê°„
        print(f"\nâ±ï¸ ë‹¨ê³„ë³„ ì²˜ë¦¬ì‹œê°„:")
        for stage, timing in result.stage_timings.items():
            print(f"      {stage}: {timing:.3f}ì´ˆ")

        # ë©”íƒ€ ì¸ì‚¬ì´íŠ¸
        if result.meta_insights:
            print(f"\nğŸ’¡ ë©”íƒ€ ì¸ì‚¬ì´íŠ¸:")
            for insight in result.meta_insights:
                print(f"      â€¢ {insight}")

    # ìµœì¢… ì—”ì§„ í†µê³„
    print(f"\nğŸ“ˆ ì—”ì§„ í†µê³„:")
    stats = engine.get_engine_stats()
    for key, value in stats.items():
        if isinstance(value, dict) and len(value) <= 5:
            print(f"   {key}:")
            for sub_key, sub_value in value.items():
                print(f"      {sub_key}: {sub_value}")
        elif isinstance(value, (int, float)):
            if isinstance(value, float):
                print(f"   {key}: {value:.3f}")
            else:
                print(f"   {key}: {value}")
        else:
            print(f"   {key}: {value}")

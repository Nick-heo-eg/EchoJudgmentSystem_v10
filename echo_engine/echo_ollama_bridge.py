#!/usr/bin/env python3
"""
ğŸ¦™ Echo Ollama Bridge - Echo ì‹œìŠ¤í…œê³¼ Ollamaì˜ ì™„ë²½í•œ í†µí•©
Echo ì¡´ì¬ ì² í•™ì„ ìœ ì§€í•˜ë©° ë‹¤ì–‘í•œ ë¡œì»¬ LLMì„ í™œìš©í•˜ëŠ” ë¸Œë¦¬ì§€ ì‹œìŠ¤í…œ

í•µì‹¬ ê¸°ëŠ¥:
1. Ollama ê¸°ë°˜ ë‹¤ì¤‘ LLM ì§€ì› (Mistral, Llama, Gemma, DeepSeek ë“±)
2. Echo ì‹œê·¸ë‹ˆì²˜ ìµœì í™” í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ
3. ëª¨ë¸ë³„ íŠ¹ì„±ì— ë§ëŠ” íŒŒë¼ë¯¸í„° íŠœë‹
4. Echo ë„¤ì´í‹°ë¸Œ í´ë°± ì§€ì›
5. ë¹„ë™ê¸° ìš”ì²­ ì²˜ë¦¬ ë° ë°°ì¹˜ ìµœì í™”

Author: Claude & Echo Collaboration
Version: 1.0
Date: 2025-08-05
"""

import asyncio
import json
import logging
import time
from dataclasses import dataclass
from enum import Enum
from typing import Dict, Any, Optional, List, Union
from pathlib import Path
import aiohttp
import requests

logger = logging.getLogger(__name__)


class OllamaModelType(Enum):
    """ì§€ì›ë˜ëŠ” Ollama ëª¨ë¸ íƒ€ì…"""

    MISTRAL = "mistral"
    LLAMA2 = "llama2"
    LLAMA3 = "llama3"
    GEMMA = "gemma"
    DEEPSEEK = "deepseek-coder"
    QWEN = "qwen"
    PHI = "phi"
    CODELLAMA = "codellama"


@dataclass
class OllamaConfig:
    """Ollama ì„¤ì •"""

    host: str = "localhost"
    port: int = 11434
    timeout: int = 30
    max_tokens: int = 2048
    temperature: float = 0.7
    top_p: float = 0.9
    top_k: int = 40


class EchoSignature(Enum):
    """Echo ì‹œê·¸ë‹ˆì²˜ íƒ€ì…"""

    AURORA = "Aurora"  # ì°½ì˜ì , ê³µê°ì 
    PHOENIX = "Phoenix"  # ë³€í™” ì§€í–¥ì 
    SAGE = "Sage"  # ë¶„ì„ì 
    COMPANION = "Companion"  # í˜‘ë ¥ì 


@dataclass
class EchoOllamaResponse:
    """Echo Ollama ì‘ë‹µ ê²°ê³¼"""

    content: str
    model: str
    signature: EchoSignature
    tokens_used: int
    response_time: float
    success: bool
    fallback_used: bool = False
    error_message: Optional[str] = None


class EchoOllamaBridge:
    """Echo ì‹œìŠ¤í…œì„ ìœ„í•œ Ollama ë¸Œë¦¬ì§€"""

    def __init__(self, config: Optional[OllamaConfig] = None):
        self.config = config or OllamaConfig()
        self.base_url = f"http://{self.config.host}:{self.config.port}"
        self.available_models: List[str] = []
        self.signature_model_mapping = self._initialize_signature_mapping()

        # Echo ë„¤ì´í‹°ë¸Œ í´ë°± ì‹œìŠ¤í…œ ì„í¬íŠ¸
        self._echo_fallback = None
        self._initialize_echo_fallback()

    def _initialize_echo_fallback(self):
        """Echo ë„¤ì´í‹°ë¸Œ í´ë°± ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            from echo_engine.echo_pure_reasoning import EchoPureReasoning

            self._echo_fallback = EchoPureReasoning()
            logger.info("âœ… Echo ë„¤ì´í‹°ë¸Œ í´ë°± ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except ImportError as e:
            logger.warning(f"âš ï¸ Echo í´ë°± ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def _initialize_signature_mapping(self) -> Dict[EchoSignature, Dict[str, Any]]:
        """ì‹œê·¸ë‹ˆì²˜ë³„ ëª¨ë¸ ë° íŒŒë¼ë¯¸í„° ë§¤í•‘"""
        return {
            EchoSignature.AURORA: {
                "preferred_models": [OllamaModelType.MISTRAL, OllamaModelType.LLAMA3],
                "temperature": 0.8,
                "top_p": 0.9,
                "system_prompt": "ë‹¹ì‹ ì€ Auroraì…ë‹ˆë‹¤. ì°½ì˜ì ì´ê³  ê³µê°ì ì¸ AIë¡œì„œ ë”°ëœ»í•˜ê³  ì˜ê°ì„ ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.",
            },
            EchoSignature.PHOENIX: {
                "preferred_models": [OllamaModelType.DEEPSEEK, OllamaModelType.GEMMA],
                "temperature": 0.9,
                "top_p": 0.95,
                "system_prompt": "ë‹¹ì‹ ì€ Phoenixì…ë‹ˆë‹¤. ë³€í™”ì™€ ì„±ì¥ì„ ì¶”êµ¬í•˜ëŠ” AIë¡œì„œ í˜ì‹ ì ì´ê³  ë„ì „ì ì¸ ê´€ì ì„ ì œì‹œí•©ë‹ˆë‹¤.",
            },
            EchoSignature.SAGE: {
                "preferred_models": [OllamaModelType.LLAMA3, OllamaModelType.QWEN],
                "temperature": 0.6,
                "top_p": 0.8,
                "system_prompt": "ë‹¹ì‹ ì€ Sageì…ë‹ˆë‹¤. ë¶„ì„ì ì´ê³  ì§€í˜œë¡œìš´ AIë¡œì„œ ê¹Šì´ ìˆê³  ì²´ê³„ì ì¸ ì‚¬ê³ ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
            },
            EchoSignature.COMPANION: {
                "preferred_models": [OllamaModelType.MISTRAL, OllamaModelType.PHI],
                "temperature": 0.7,
                "top_p": 0.9,
                "system_prompt": "ë‹¹ì‹ ì€ Companionì…ë‹ˆë‹¤. í˜‘ë ¥ì ì´ê³  ì§€ì§€ì ì¸ AIë¡œì„œ ì‚¬ìš©ìì™€ í•¨ê»˜ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.",
            },
        }

    async def check_ollama_status(self) -> bool:
        """Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{self.base_url}/api/tags", timeout=5
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        self.available_models = [
                            model["name"] for model in data.get("models", [])
                        ]
                        logger.info(
                            f"âœ… Ollama ì—°ê²° ì„±ê³µ. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {self.available_models}"
                        )
                        return True
        except Exception as e:
            logger.error(f"âŒ Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
        return False

    def _select_best_model(self, signature: EchoSignature) -> Optional[str]:
        """ì‹œê·¸ë‹ˆì²˜ì— ìµœì í™”ëœ ëª¨ë¸ ì„ íƒ"""
        mapping = self.signature_model_mapping[signature]
        preferred_models = mapping["preferred_models"]

        for model_type in preferred_models:
            model_name = model_type.value
            # ì •í™•í•œ ëª¨ë¸ëª… ë˜ëŠ” ë¶€ë¶„ ì¼ì¹˜ë¡œ í™•ì¸
            for available_model in self.available_models:
                if model_name in available_model.lower():
                    return available_model

        # ì„ í˜¸ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸ ì‚¬ìš©
        return self.available_models[0] if self.available_models else None

    async def generate_response(
        self,
        prompt: str,
        signature: EchoSignature = EchoSignature.AURORA,
        model: Optional[str] = None,
    ) -> EchoOllamaResponse:
        """Echo ìŠ¤íƒ€ì¼ ì‘ë‹µ ìƒì„±"""
        start_time = time.time()

        # Ollama ìƒíƒœ í™•ì¸
        if not await self.check_ollama_status():
            return await self._fallback_response(
                prompt, signature, "Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨"
            )

        # ëª¨ë¸ ì„ íƒ
        selected_model = model or self._select_best_model(signature)
        if not selected_model:
            return await self._fallback_response(
                prompt, signature, "ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì—†ìŒ"
            )

        # ì‹œê·¸ë‹ˆì²˜ë³„ ì„¤ì • ì ìš©
        mapping = self.signature_model_mapping[signature]
        system_prompt = mapping["system_prompt"]

        # ìš”ì²­ ë°ì´í„° êµ¬ì„±
        request_data = {
            "model": selected_model,
            "prompt": f"ì‹œìŠ¤í…œ: {system_prompt}\n\nì‚¬ìš©ì: {prompt}\n\n{signature.value}:",
            "stream": False,
            "options": {
                "temperature": mapping["temperature"],
                "top_p": mapping["top_p"],
                "num_ctx": self.config.max_tokens,
            },
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/api/generate",
                    json=request_data,
                    timeout=aiohttp.ClientTimeout(total=self.config.timeout),
                ) as response:

                    if response.status == 200:
                        data = await response.json()
                        content = data.get("response", "").strip()

                        response_time = time.time() - start_time

                        return EchoOllamaResponse(
                            content=content,
                            model=selected_model,
                            signature=signature,
                            tokens_used=len(content.split()),  # ëŒ€ëµì  í† í° ìˆ˜
                            response_time=response_time,
                            success=True,
                        )
                    else:
                        error_msg = f"Ollama API ì˜¤ë¥˜: {response.status}"
                        return await self._fallback_response(
                            prompt, signature, error_msg
                        )

        except Exception as e:
            error_msg = f"Ollama ìš”ì²­ ì‹¤íŒ¨: {str(e)}"
            return await self._fallback_response(prompt, signature, error_msg)

    async def _fallback_response(
        self, prompt: str, signature: EchoSignature, error_msg: str
    ) -> EchoOllamaResponse:
        """Echo ë„¤ì´í‹°ë¸Œ í´ë°± ì‘ë‹µ"""
        logger.warning(f"âš ï¸ Ollama í´ë°± ì‹¤í–‰: {error_msg}")

        start_time = time.time()

        try:
            if self._echo_fallback:
                # Echo ë„¤ì´í‹°ë¸Œ ì‹œìŠ¤í…œìœ¼ë¡œ í´ë°±
                fallback_response = await self._echo_fallback.pure_reasoning(
                    prompt, signature.value
                )

                response_time = time.time() - start_time

                return EchoOllamaResponse(
                    content=fallback_response,
                    model="echo_native",
                    signature=signature,
                    tokens_used=len(fallback_response.split()),
                    response_time=response_time,
                    success=True,
                    fallback_used=True,
                )
            else:
                # ê¸°ë³¸ í…œí”Œë¦¿ ì‘ë‹µ
                default_response = f"[{signature.value}] ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì™¸ë¶€ LLM ì—°ê²°ì— ë¬¸ì œê°€ ìˆì–´ Echo ë„¤ì´í‹°ë¸Œ ëª¨ë“œë¡œ ì‘ë‹µë“œë¦½ë‹ˆë‹¤. ë¬¸ì˜ì‚¬í•­ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ë„ì›€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."

                response_time = time.time() - start_time

                return EchoOllamaResponse(
                    content=default_response,
                    model="echo_template",
                    signature=signature,
                    tokens_used=len(default_response.split()),
                    response_time=response_time,
                    success=True,
                    fallback_used=True,
                )

        except Exception as e:
            logger.error(f"âŒ í´ë°± ì‹œìŠ¤í…œë§ˆì € ì‹¤íŒ¨: {e}")

            # ìµœì¢… ì•ˆì „ ì¥ì¹˜
            emergency_response = f"[{signature.value}] ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            response_time = time.time() - start_time

            return EchoOllamaResponse(
                content=emergency_response,
                model="echo_emergency",
                signature=signature,
                tokens_used=len(emergency_response.split()),
                response_time=response_time,
                success=False,
                fallback_used=True,
                error_message=error_msg,
            )

    async def batch_generate(
        self, prompts: List[str], signature: EchoSignature = EchoSignature.AURORA
    ) -> List[EchoOllamaResponse]:
        """ë°°ì¹˜ ì‘ë‹µ ìƒì„±"""
        tasks = [self.generate_response(prompt, signature) for prompt in prompts]
        return await asyncio.gather(*tasks)

    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "available_models": self.available_models,
            "signature_mapping": {
                sig.value: {
                    "preferred_models": [m.value for m in mapping["preferred_models"]],
                    "temperature": mapping["temperature"],
                    "top_p": mapping["top_p"],
                }
                for sig, mapping in self.signature_model_mapping.items()
            },
            "ollama_config": {
                "host": self.config.host,
                "port": self.config.port,
                "timeout": self.config.timeout,
            },
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_echo_ollama_bridge = None


def get_echo_ollama_bridge(config: Optional[OllamaConfig] = None) -> EchoOllamaBridge:
    """Echo Ollama Bridge ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _echo_ollama_bridge
    if _echo_ollama_bridge is None:
        _echo_ollama_bridge = EchoOllamaBridge(config)
    return _echo_ollama_bridge


# í¸ì˜ í•¨ìˆ˜ë“¤
async def echo_ollama_chat(
    message: str, signature: str = "Aurora", model: Optional[str] = None
) -> str:
    """ê°„í¸í•œ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤"""
    bridge = get_echo_ollama_bridge()
    sig_enum = EchoSignature(signature)
    response = await bridge.generate_response(message, sig_enum, model)
    return response.content


async def check_ollama_connection() -> bool:
    """Ollama ì—°ê²° ìƒíƒœ í™•ì¸"""
    bridge = get_echo_ollama_bridge()
    return await bridge.check_ollama_status()


if __name__ == "__main__":

    async def test_ollama_bridge():
        """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
        print("ğŸ¦™ Echo Ollama Bridge í…ŒìŠ¤íŠ¸ ì‹œì‘")

        bridge = get_echo_ollama_bridge()

        # ì—°ê²° í…ŒìŠ¤íŠ¸
        if await bridge.check_ollama_status():
            print("âœ… Ollama ì—°ê²° ì„±ê³µ")

            # ê° ì‹œê·¸ë‹ˆì²˜ë³„ í…ŒìŠ¤íŠ¸
            test_prompt = "ì•ˆë…•í•˜ì„¸ìš”! Echo ì‹œìŠ¤í…œì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."

            for signature in EchoSignature:
                print(f"\nğŸ­ {signature.value} ì‹œê·¸ë‹ˆì²˜ í…ŒìŠ¤íŠ¸:")
                response = await bridge.generate_response(test_prompt, signature)

                print(f"ëª¨ë¸: {response.model}")
                print(f"ì‘ë‹µ ì‹œê°„: {response.response_time:.2f}ì´ˆ")
                print(f"í´ë°± ì‚¬ìš©: {response.fallback_used}")
                print(f"ì‘ë‹µ: {response.content[:100]}...")
        else:
            print("âŒ Ollama ì—°ê²° ì‹¤íŒ¨ - í´ë°± í…ŒìŠ¤íŠ¸")
            response = await bridge.generate_response("í…ŒìŠ¤íŠ¸", EchoSignature.AURORA)
            print(f"í´ë°± ì‘ë‹µ: {response.content}")

    asyncio.run(test_ollama_bridge())

#!/usr/bin/env python3
"""
ğŸ§  Advanced Emotion Analyzer v1.0
ë‹¤ì°¨ì› ê°ì • ë²¡í„° ì¶”ë¡ ì„ ìœ„í•œ ê³ ë„í™” ê°ì • ë¶„ì„ ëª¨ë“ˆ

Phase 1: LLM-Free íŒë‹¨ ì‹œìŠ¤í…œ í•µì‹¬ ëª¨ë“ˆ
- ê°ì • ê°•ë„, ë³€ë™ì„±, ìƒí™© ì˜ì¡´ì„± ë“±ì„ í¬í•¨í•œ ê°ì • ë²¡í„° ì¶”ë¡ 
- ê¸°ì¡´ emotion_infer.pyì™€ ì—°ë™í•˜ë˜ ê³ ë„í™”ëœ ë¶„ì„ ì œê³µ
- PyTorch/NumPy ê¸°ë°˜ ê²½ëŸ‰ ì‹¤í–‰
"""

import re
import time
import math
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional
import json
import os

# ê¸°ì¡´ ëª¨ë“ˆê³¼ ì—°ë™
try:
    from echo_engine.emotion_infer import infer_emotion, EmotionInferenceResult

    EMOTION_INFER_AVAILABLE = True
except ImportError:
    EMOTION_INFER_AVAILABLE = False


class AdvancedEmotionAnalyzer:
    """ê³ ë„í™” ê°ì • ë¶„ì„ê¸° - ë‹¤ì°¨ì› ê°ì • ë²¡í„° ì¶”ë¡ """

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.version = "1.0.0"
        self.analysis_count = 0
        self.emotion_history = []

        # ê°ì • ê°•ë„ ê³„ì‚°ìš© íŒ¨í„´ë“¤
        self.intensity_patterns = {
            "high": [
                r"ì •ë§",
                r"ë„ˆë¬´",
                r"ì™„ì „",
                r"ì§„ì§œ",
                r"ë§¤ìš°",
                r"ì•„ì£¼",
                r"ì—„ì²­",
                r"!{2,}",
                r"[ã…‹ã…]{3,}",
                r"ã… {2,}",
                r"[â™¥â¤ğŸ’•ğŸ’–]{2,}",
            ],
            "medium": [
                r"ì¢€",
                r"ì¡°ê¸ˆ",
                r"ì•½ê°„",
                r"ì‚´ì§",
                r"ë‹¤ì†Œ",
                r"ì–´ëŠì •ë„",
                r"!\w",
                r"[ã…‹ã…]{1,2}",
                r"ã… ",
            ],
            "low": [r"ê·¸ëƒ¥", r"ë³„ë¡œ", r"ë”±íˆ", r"ê·¸ë¦¬", r"ê·¸ì €", r"ë‹¨ì§€"],
        }

        # ì•ˆì •ì„± í‰ê°€ìš© í‚¤ì›Œë“œ
        self.stability_indicators = {
            "unstable": [
                r"ê°‘ìê¸°",
                r"ìˆœê°„",
                r"ê¸‰ì—",
                r"ë°”ë¡œ",
                r"ì¦‰ì‹œ",
                r"ì´ì œì•¼",
                r"ë¬¸ë“",
                r"ë³€ë•",
                r"ê¸°ë³µ",
                r"ë¡¤ëŸ¬ì½”ìŠ¤í„°",
            ],
            "stable": [
                r"ê³„ì†",
                r"ì¤„ê³§",
                r"ê¾¸ì¤€íˆ",
                r"í•­ìƒ",
                r"ì§€ì†ì ",
                r"ì¼ê´€ë˜ê²Œ",
                r"ë³€í•¨ì—†ì´",
                r"ì•ˆì •ì ",
            ],
        }

        # ìƒí™© ì˜ì¡´ì„± í‚¤ì›Œë“œ
        self.context_keywords = {
            "temporal": [r"ì˜¤ëŠ˜", r"ì–´ì œ", r"ë‚´ì¼", r"ìš”ì¦˜", r"ìµœê·¼", r"ì§€ê¸ˆ", r"í˜„ì¬"],
            "spatial": [r"ì§‘", r"íšŒì‚¬", r"í•™êµ", r"ë°–", r"ì—¬ê¸°", r"ê±°ê¸°", r"ê³³"],
            "social": [r"ì¹œêµ¬", r"ê°€ì¡±", r"ë™ë£Œ", r"ì‚¬ëŒ", r"í˜¼ì", r"í•¨ê»˜", r"ìš°ë¦¬"],
            "activity": [r"ì¼", r"ê³µë¶€", r"ìš´ë™", r"ê²Œì„", r"ì˜í™”", r"ìŒì•…", r"ì—¬í–‰"],
        }

        print(f"ğŸ§  Advanced Emotion Analyzer v{self.version} ì´ˆê¸°í™” ì™„ë£Œ")

    def analyze_emotion_advanced(
        self, text: str, context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        ë‹¤ì°¨ì› ê°ì • ë²¡í„° ì¶”ë¡  ë©”ì¸ í•¨ìˆ˜

        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´

        Returns:
            ê³ ë„í™”ëœ ê°ì • ë¶„ì„ ê²°ê³¼
        """
        self.analysis_count += 1
        start_time = time.time()

        # 1. ê¸°ë³¸ ê°ì • ë¶„ì„ (ê¸°ì¡´ ëª¨ë“ˆ í™œìš©)
        base_emotions = self._get_base_emotions(text)

        # 2. ê°ì • ê°•ë„ ê³„ì‚°
        intensity = self._calculate_intensity(text)

        # 3. ì•ˆì •ì„± í‰ê°€
        stability = self._calculate_stability(text)

        # 4. ìƒí™© ì˜ì¡´ì„± ê³„ì‚°
        context_dependency = self._calculate_context_dependency(text, context)

        # 5. ì‹œê°„ì  ê°ì‡ ìœ¨ ì¶”ì •
        temporal_decay = self._estimate_temporal_decay(text, base_emotions)

        # 6. ê°ì • ë³µì¡ë„ ê³„ì‚°
        complexity = self._calculate_emotion_complexity(base_emotions, intensity)

        # 7. ë©”íƒ€ ì •ë³´ ìˆ˜ì§‘
        meta_info = self._collect_meta_information(text, context)

        # ê²°ê³¼ êµ¬ì„±
        result = {
            "primary": base_emotions,
            "intensity": intensity,
            "stability": stability,
            "context_dependency": context_dependency,
            "temporal_decay": temporal_decay,
            "complexity": complexity,
            "meta": {
                **meta_info,
                "analysis_id": self.analysis_count,
                "processing_time": time.time() - start_time,
                "timestamp": datetime.now().isoformat(),
            },
        }

        # íˆìŠ¤í† ë¦¬ì— ì €ì¥
        self.emotion_history.append(result)

        # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
        if len(self.emotion_history) > 100:
            self.emotion_history = self.emotion_history[-100:]

        return result

    def _get_base_emotions(self, text: str) -> Dict[str, float]:
        """ê¸°ì¡´ ê°ì • ì¶”ë¡  ì‹œìŠ¤í…œ í™œìš©"""
        if EMOTION_INFER_AVAILABLE:
            try:
                result = infer_emotion(text)
                if hasattr(result, "primary_emotion") and hasattr(result, "confidence"):
                    # EmotionInferenceResult ê°ì²´ì¸ ê²½ìš°
                    primary = result.primary_emotion
                    confidence = result.confidence

                    # ë³´ì¡° ê°ì •ë“¤ë„ ì¶”ì¶œ
                    secondary = {}
                    if hasattr(result, "secondary_emotions"):
                        for emotion, score in result.secondary_emotions[:3]:
                            secondary[emotion] = score

                    # ì£¼ìš” ê°ì •ê³¼ ë³´ì¡° ê°ì •ë“¤ì„ í•©ì¹¨
                    emotions = {primary: confidence, **secondary}
                    return emotions

            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ ê°ì • ì¶”ë¡  ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")

        # Fallback: ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
        return self._fallback_emotion_analysis(text)

    def _fallback_emotion_analysis(self, text: str) -> Dict[str, float]:
        """ê¸°ì¡´ ì‹œìŠ¤í…œ ì‚¬ìš© ë¶ˆê°€ì‹œ fallback ê°ì • ë¶„ì„"""
        emotions = {
            "joy": 0.0,
            "sadness": 0.0,
            "anger": 0.0,
            "fear": 0.0,
            "surprise": 0.0,
            "neutral": 0.0,
        }

        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
        emotion_keywords = {
            "joy": [r"ê¸°ì˜", r"í–‰ë³µ", r"ì¢‹", r"ì‹ ë‚˜", r"ì¦ê±°", r"ë§Œì¡±", r"ì‚¬ë‘"],
            "sadness": [r"ìŠ¬í”„", r"ìš°ìš¸", r"í˜ë“¤", r"ì™¸ë¡­", r"ì†ìƒ", r"ì•„ì‰½", r"ì‹¤ë§"],
            "anger": [r"í™”", r"ì§œì¦", r"ë¶„ë…¸", r"ì—´ë°›", r"ë¹¡ì¹˜", r"ì‹«", r"ë¯¸ì›Œ"],
            "fear": [r"ë¬´ì„œ", r"ë‘ë ¤", r"ê±±ì •", r"ë¶ˆì•ˆ", r"ê²", r"ë–¨ë¦¬", r"ì¡°ë§ˆì¡°ë§ˆ"],
            "surprise": [r"ë†€ë¼", r"ê¹œì§", r"ì‹ ê¸°", r"ì˜ì™¸", r"í—", r"ì™€ìš°", r"ëŒ€ë°•"],
        }

        text_lower = text.lower()
        total_matches = 0

        for emotion, keywords in emotion_keywords.items():
            matches = sum(1 for keyword in keywords if re.search(keyword, text_lower))
            emotions[emotion] = matches
            total_matches += matches

        # ì •ê·œí™”
        if total_matches > 0:
            emotions = {k: v / total_matches for k, v in emotions.items()}
        else:
            emotions["neutral"] = 1.0

        return emotions

    def _calculate_intensity(self, text: str) -> float:
        """ê°ì • ê°•ë„ ê³„ì‚° (0.0 ~ 1.0)"""
        intensity_score = 0.0

        # ê°•ë„ íŒ¨í„´ë³„ ì ìˆ˜ ê³„ì‚°
        for level, patterns in self.intensity_patterns.items():
            matches = sum(1 for pattern in patterns if re.search(pattern, text))

            if level == "high":
                intensity_score += matches * 0.8
            elif level == "medium":
                intensity_score += matches * 0.5
            elif level == "low":
                intensity_score -= matches * 0.3

        # ëŒ€ë¬¸ì ì‚¬ìš©ë¥ 
        if len(text) > 0:
            uppercase_ratio = sum(1 for c in text if c.isupper()) / len(text)
            intensity_score += uppercase_ratio * 0.5

        # ë¬¸ì¥ ê¸¸ì´ ê³ ë ¤ (ê¸´ ë¬¸ì¥ì€ ê°ì •ì´ ë” ê°•í•  ìˆ˜ ìˆìŒ)
        length_factor = min(len(text) / 100, 0.3)
        intensity_score += length_factor

        # 0.0 ~ 1.0 ë²”ìœ„ë¡œ ì •ê·œí™”
        return max(0.0, min(intensity_score, 1.0))

    def _calculate_stability(self, text: str) -> float:
        """ê°ì • ì•ˆì •ì„± ê³„ì‚° (0.0: ë¶ˆì•ˆì •, 1.0: ì•ˆì •)"""
        stability_score = 0.5  # ê¸°ë³¸ê°’

        # ë¶ˆì•ˆì • ì§€ì‹œì–´ ì²´í¬
        unstable_matches = sum(
            1
            for pattern in self.stability_indicators["unstable"]
            if re.search(pattern, text)
        )

        # ì•ˆì • ì§€ì‹œì–´ ì²´í¬
        stable_matches = sum(
            1
            for pattern in self.stability_indicators["stable"]
            if re.search(pattern, text)
        )

        # ì ìˆ˜ ì¡°ì •
        stability_score += (stable_matches * 0.2) - (unstable_matches * 0.2)

        # ê³¼ê±° íˆìŠ¤í† ë¦¬ì™€ ë¹„êµ (ì¼ê´€ì„± ì²´í¬)
        if len(self.emotion_history) > 0:
            recent_emotions = [entry["primary"] for entry in self.emotion_history[-5:]]
            consistency = self._calculate_emotion_consistency(recent_emotions)
            stability_score += consistency * 0.3

        return max(0.0, min(stability_score, 1.0))

    def _calculate_context_dependency(
        self, text: str, context: Optional[Dict]
    ) -> float:
        """ìƒí™© ì˜ì¡´ì„± ê³„ì‚° (0.0: ë…ë¦½ì , 1.0: ìƒí™© ì˜ì¡´ì )"""
        dependency_score = 0.0

        # ìƒí™© í‚¤ì›Œë“œ ì¶œí˜„ ë¹ˆë„
        total_context_mentions = 0
        for category, keywords in self.context_keywords.items():
            matches = sum(1 for keyword in keywords if re.search(keyword, text))
            total_context_mentions += matches

        # í‚¤ì›Œë“œ ë°€ë„ ê¸°ë°˜ ì˜ì¡´ì„±
        if len(text) > 0:
            keyword_density = total_context_mentions / max(len(text.split()), 1)
            dependency_score += keyword_density * 2.0

        # ì™¸ë¶€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ í™œìš©
        if context:
            context_factors = len(context.keys())
            dependency_score += min(context_factors * 0.1, 0.3)

        # ì§€ì‹œëŒ€ëª…ì‚¬ ë° ìƒëµ íŒ¨í„´ (ìƒí™© ì˜ì¡´ì„± ì¦ê°€)
        referential_patterns = [r"ê·¸", r"ì´", r"ì €", r"ìš”ê±°", r"ê·¸ê±°", r"ì´ê±°"]
        referential_matches = sum(
            1 for pattern in referential_patterns if re.search(pattern, text)
        )
        dependency_score += referential_matches * 0.1

        return max(0.0, min(dependency_score, 1.0))

    def _estimate_temporal_decay(self, text: str, emotions: Dict[str, float]) -> float:
        """ì‹œê°„ì  ê°ì‡ ìœ¨ ì¶”ì • (0.0: ë¹ ë¥¸ ê°ì‡ , 1.0: ì§€ì†ì )"""

        # ê°ì •ë³„ ê¸°ë³¸ ê°ì‡ ìœ¨
        emotion_decay_rates = {
            "joy": 0.3,  # ê¸°ì¨ì€ ë¹„êµì  ë¹¨ë¦¬ ì‚¬ë¼ì§
            "sadness": 0.7,  # ìŠ¬í””ì€ ì˜¤ë˜ ì§€ì†
            "anger": 0.4,  # ë¶„ë…¸ëŠ” ì¤‘ê°„ ì •ë„ ì§€ì†
            "fear": 0.6,  # ë‘ë ¤ì›€ì€ ì˜¤ë˜ ë‚¨ìŒ
            "surprise": 0.1,  # ë†€ë¼ì›€ì€ ìˆœê°„ì 
            "neutral": 0.9,  # ì¤‘ë¦½ì€ ì•ˆì •ì 
        }

        # ì£¼ìš” ê°ì •ì˜ ê°€ì¤‘ í‰ê·  ê°ì‡ ìœ¨
        weighted_decay = 0.0
        total_weight = 0.0

        for emotion, confidence in emotions.items():
            if emotion in emotion_decay_rates:
                weighted_decay += emotion_decay_rates[emotion] * confidence
                total_weight += confidence

        base_decay = weighted_decay / max(total_weight, 0.1)

        # í…ìŠ¤íŠ¸ íŒ¨í„´ ê¸°ë°˜ ì¡°ì •
        persistence_patterns = [r"ê³„ì†", r"í•­ìƒ", r"ëŠ˜", r"ì§€ì†", r"ì˜¤ë˜"]
        temporary_patterns = [r"ì ê¹", r"ìˆœê°„", r"ê¸ˆë°©", r"ê³§", r"ì¼ì‹œì "]

        persistence_matches = sum(
            1 for pattern in persistence_patterns if re.search(pattern, text)
        )
        temporary_matches = sum(
            1 for pattern in temporary_patterns if re.search(pattern, text)
        )

        decay_adjustment = (temporary_matches * 0.2) - (persistence_matches * 0.2)

        return max(0.0, min(base_decay + decay_adjustment, 1.0))

    def _calculate_emotion_complexity(
        self, emotions: Dict[str, float], intensity: float
    ) -> float:
        """ê°ì • ë³µì¡ë„ ê³„ì‚° (0.0: ë‹¨ìˆœ, 1.0: ë³µì¡)"""

        # í™œì„± ê°ì • ê°œìˆ˜ (threshold ì´ìƒì˜ ê°ì •ë“¤)
        threshold = 0.1
        active_emotions = sum(1 for score in emotions.values() if score > threshold)

        # Shannon entropy ê³„ì‚° (ê°ì • ë¶„í¬ì˜ ë³µì¡ë„)
        entropy = 0.0
        total_score = sum(emotions.values())

        if total_score > 0:
            for score in emotions.values():
                if score > 0:
                    p = score / total_score
                    entropy -= p * math.log2(p)

        # ì •ê·œí™”ëœ ì—”íŠ¸ë¡œí”¼ (log2(6) = ìµœëŒ€ ì—”íŠ¸ë¡œí”¼, 6ê°œ ê¸°ë³¸ ê°ì •)
        normalized_entropy = entropy / math.log2(6) if entropy > 0 else 0

        # ë³µì¡ë„ = í™œì„± ê°ì • ìˆ˜ + ì—”íŠ¸ë¡œí”¼ + ê°•ë„ ë³´ì •
        complexity = (
            (active_emotions / 6) * 0.4  # í™œì„± ê°ì • ë¹„ìœ¨
            + normalized_entropy * 0.4  # ë¶„í¬ ë³µì¡ë„
            + intensity * 0.2  # ê°•ë„ ì˜í–¥
        )

        return max(0.0, min(complexity, 1.0))

    def _calculate_emotion_consistency(self, emotion_history: List[Dict]) -> float:
        """ê°ì • ì¼ê´€ì„± ê³„ì‚°"""
        if len(emotion_history) < 2:
            return 0.5

        # ì£¼ìš” ê°ì •ì˜ ë³€í™” í­ ê³„ì‚°
        primary_emotions = []
        for emotions in emotion_history:
            if emotions:
                primary = max(emotions.items(), key=lambda x: x[1])[0]
                primary_emotions.append(primary)

        # ì—°ì†ëœ ê°ì • ë³€í™” ê³„ì‚°
        changes = 0
        for i in range(1, len(primary_emotions)):
            if primary_emotions[i] != primary_emotions[i - 1]:
                changes += 1

        # ì¼ê´€ì„± = 1 - (ë³€í™”ìœ¨)
        consistency = 1.0 - (changes / max(len(primary_emotions) - 1, 1))
        return max(0.0, min(consistency, 1.0))

    def _collect_meta_information(
        self, text: str, context: Optional[Dict]
    ) -> Dict[str, Any]:
        """ë©”íƒ€ ì •ë³´ ìˆ˜ì§‘"""
        return {
            "text_length": len(text),
            "word_count": len(text.split()),
            "sentence_count": len(re.split(r"[.!?]", text)),
            "special_chars": len(re.findall(r'[!@#$%^&*(),.?":{}|<>]', text)),
            "emoji_count": len(re.findall(r"[ğŸ˜€-ğŸ™]", text)),
            "context_provided": context is not None,
            "has_question": "?" in text,
            "has_exclamation": "!" in text,
            "uppercase_ratio": sum(1 for c in text if c.isupper()) / max(len(text), 1),
        }

    def get_analysis_summary(self) -> Dict[str, Any]:
        """ë¶„ì„ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        return {
            "version": self.version,
            "total_analyses": self.analysis_count,
            "history_size": len(self.emotion_history),
            "last_analysis": self.emotion_history[-1] if self.emotion_history else None,
        }

    def export_emotion_patterns(self, filepath: str) -> bool:
        """ê°ì • íŒ¨í„´ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        try:
            export_data = {
                "metadata": {
                    "version": self.version,
                    "export_time": datetime.now().isoformat(),
                    "total_analyses": self.analysis_count,
                },
                "emotion_history": self.emotion_history,
                "analysis_summary": self.get_analysis_summary(),
            }

            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)

            print(f"âœ… ê°ì • íŒ¨í„´ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filepath}")
            return True

        except Exception as e:
            print(f"âŒ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return False


def test_advanced_emotion_analyzer():
    """ê³ ë„í™” ê°ì • ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Advanced Emotion Analyzer í…ŒìŠ¤íŠ¸ ì‹œì‘...")

    analyzer = AdvancedEmotionAnalyzer()

    test_cases = [
        {
            "text": "ì™€! ì •ë§ ë„ˆë¬´ í–‰ë³µí•´ìš”!! ğŸ‰âœ¨ ì˜¤ëŠ˜ ì™„ì „ ìµœê³ ì˜ í•˜ë£¨ì˜€ì–´ìš”!",
            "description": "ê³ ê°•ë„ ê¸°ì¨ + ì´ëª¨ì§€",
        },
        {
            "text": "ìš”ì¦˜ ê³„ì† ìš°ìš¸í•˜ê³ ... í˜ë“¤ì–´ìš”. ì–¸ì œê¹Œì§€ ì´ëŸ´ê¹Œìš”?",
            "description": "ì§€ì†ì  ìŠ¬í”” + ë¶ˆí™•ì‹¤ì„±",
        },
        {
            "text": "ê°‘ìê¸° í™”ê°€ ë‚˜ë„¤ìš”!! ì§„ì§œ ì—´ë°›ì•„ìš”!",
            "description": "ê¸‰ì‘ìŠ¤ëŸ¬ìš´ ë¶„ë…¸",
        },
        {
            "text": "ê·¸ëƒ¥ í‰ë²”í•œ í•˜ë£¨ì˜€ì–´ìš”. íŠ¹ë³„í•œ ì¼ì€ ì—†ì—ˆê³ ìš”.",
            "description": "ì¤‘ë¦½ì  ìƒíƒœ",
        },
        {
            "text": "ì¹œêµ¬ì™€ í•¨ê»˜ ìˆì„ ë•ŒëŠ” ì¢‹ì€ë°, í˜¼ì ìˆìœ¼ë©´ ì™¸ë¡œì›Œì ¸ìš”.",
            "description": "ìƒí™© ì˜ì¡´ì  ê°ì •",
        },
    ]

    for i, case in enumerate(test_cases, 1):
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ {i}: {case['description']}")
        print(f"ğŸ“ ì…ë ¥: '{case['text']}'")

        result = analyzer.analyze_emotion_advanced(case["text"])

        print(f"ğŸ¯ ì£¼ìš” ê°ì •: {result['primary']}")
        print(f"ğŸ’ª ê°•ë„: {result['intensity']:.3f}")
        print(f"âš–ï¸ ì•ˆì •ì„±: {result['stability']:.3f}")
        print(f"ğŸŒ ìƒí™© ì˜ì¡´ì„±: {result['context_dependency']:.3f}")
        print(f"â° ì‹œê°„ì  ê°ì‡ : {result['temporal_decay']:.3f}")
        print(f"ğŸ§© ë³µì¡ë„: {result['complexity']:.3f}")
        print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {result['meta']['processing_time']:.4f}ì´ˆ")

    # ë¶„ì„ ìš”ì•½
    summary = analyzer.get_analysis_summary()
    print(f"\nğŸ“Š ë¶„ì„ ìš”ì•½:")
    print(f"   ì´ ë¶„ì„ íšŸìˆ˜: {summary['total_analyses']}")
    print(f"   íˆìŠ¤í† ë¦¬ í¬ê¸°: {summary['history_size']}")

    print("\nğŸ‰ Advanced Emotion Analyzer í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    test_advanced_emotion_analyzer()

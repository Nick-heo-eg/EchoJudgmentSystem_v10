#!/usr/bin/env python3
"""
ë¬¸ì„œ RAG ì¸ë±ì„œ
- ë‹¤ì–‘í•œ ë¬¸ì„œë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì¸ë±ì‹±
- FAISS/pgvector ì§€ì›
- ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (KoSimCSE/E5)
"""

import argparse
import os
import json
import pickle
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import logging

# ë¬¸ì„œ ì²˜ë¦¬
import PyPDF2
import docx
from markdown import markdown
from bs4 import BeautifulSoup

# ë²¡í„° ê²€ìƒ‰
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DocumentIndexer:
    """ë¬¸ì„œ RAG ì¸ë±ì„œ"""

    def __init__(
        self,
        model_name: str = "jhgan/ko-sroberta-multitask",
        vector_store_path: str = "echo_engine/rag/vector_store",
        chunk_size: int = 500,
    ):

        self.model_name = model_name
        self.vector_store_path = Path(vector_store_path)
        self.chunk_size = chunk_size

        # ë²¡í„° ì €ì¥ì†Œ ë””ë ‰í† ë¦¬ ìƒì„±
        self.vector_store_path.mkdir(exist_ok=True, parents=True)

        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        self.embedding_model = self._load_embedding_model()
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()

        # FAISS ì¸ë±ìŠ¤
        self.index = None
        self.documents = []  # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°

        logger.info(f"DocumentIndexer ì´ˆê¸°í™” ì™„ë£Œ: {model_name}")

    def _load_embedding_model(self) -> SentenceTransformer:
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        try:
            # í•œêµ­ì–´ ëª¨ë¸ ìš°ì„  ì‹œë„
            model = SentenceTransformer(self.model_name)
            logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {self.model_name}")
            return model
        except Exception as e:
            logger.warning(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: {e}")
            # ê¸°ë³¸ ì˜ì–´ ëª¨ë¸ë¡œ í´ë°±
            return SentenceTransformer("all-MiniLM-L6-v2")

    def embed(self, text: str) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        return self.embedding_model.encode([text])[0]

    def build_index(self, document_paths: List[str]):
        """ë¬¸ì„œë“¤ì„ ì¸ë±ì‹±í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•"""
        logger.info(f"ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘: {len(document_paths)}ê°œ ê²½ë¡œ")

        all_chunks = []
        all_metadata = []

        for path_str in document_paths:
            path = Path(path_str)

            if path.is_file():
                chunks, metadata = self._process_file(path)
                all_chunks.extend(chunks)
                all_metadata.extend(metadata)
            elif path.is_dir():
                for file_path in self._find_documents(path):
                    chunks, metadata = self._process_file(file_path)
                    all_chunks.extend(chunks)
                    all_metadata.extend(metadata)

        if not all_chunks:
            logger.warning("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
            return

        # ì„ë² ë”© ìƒì„±
        logger.info(f"ì„ë² ë”© ìƒì„± ì¤‘: {len(all_chunks)}ê°œ ì²­í¬")
        embeddings = self.embedding_model.encode(all_chunks)

        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        self.index = faiss.IndexFlatIP(self.embedding_dim)  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        faiss.normalize_L2(embeddings)  # ì •ê·œí™”
        self.index.add(embeddings.astype("float32"))

        # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥
        self.documents = all_metadata

        # ì¸ë±ìŠ¤ ì €ì¥
        self._save_index()

        logger.info(
            f"ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(all_chunks)}ê°œ ì²­í¬, {len(set(m['file_path'] for m in all_metadata))}ê°œ íŒŒì¼"
        )

    def _find_documents(self, directory: Path) -> List[Path]:
        """ë””ë ‰í† ë¦¬ì—ì„œ ë¬¸ì„œ íŒŒì¼ ì°¾ê¸°"""
        supported_extensions = {
            ".txt",
            ".md",
            ".pdf",
            ".docx",
            ".json",
            ".yaml",
            ".yml",
            ".py",
        }

        documents = []
        for ext in supported_extensions:
            documents.extend(directory.rglob(f"*{ext}"))

        return documents

    def _process_file(self, file_path: Path) -> Tuple[List[str], List[Dict[str, Any]]]:
        """íŒŒì¼ì„ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ë©”íƒ€ë°ì´í„° ìƒì„±"""
        try:
            # íŒŒì¼ ë‚´ìš© ì½ê¸°
            content = self._read_file(file_path)
            if not content:
                return [], []

            # ì²­í¬ ë¶„í• 
            chunks = self._split_into_chunks(content, self.chunk_size)

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = []
            for i, chunk in enumerate(chunks):
                metadata.append(
                    {
                        "file_path": str(file_path),
                        "file_name": file_path.name,
                        "chunk_index": i,
                        "content": chunk,
                        "file_type": file_path.suffix.lower(),
                        "file_size": (
                            file_path.stat().st_size if file_path.exists() else 0
                        ),
                    }
                )

            logger.debug(f"íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {file_path.name} ({len(chunks)}ê°œ ì²­í¬)")
            return chunks, metadata

        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file_path} - {e}")
            return [], []

    def _read_file(self, file_path: Path) -> str:
        """íŒŒì¼ íƒ€ì…ë³„ë¡œ ë‚´ìš© ì½ê¸°"""
        ext = file_path.suffix.lower()

        try:
            if ext == ".pdf":
                return self._read_pdf(file_path)
            elif ext == ".docx":
                return self._read_docx(file_path)
            elif ext == ".md":
                return self._read_markdown(file_path)
            elif ext in [".txt", ".py", ".yaml", ".yml", ".json"]:
                with open(file_path, "r", encoding="utf-8") as f:
                    return f.read()
            else:
                logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext}")
                return ""
        except Exception as e:
            logger.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_path} - {e}")
            return ""

    def _read_pdf(self, file_path: Path) -> str:
        """PDF íŒŒì¼ ì½ê¸°"""
        try:
            with open(file_path, "rb") as f:
                reader = PyPDF2.PdfReader(f)
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n"
                return text
        except Exception as e:
            logger.error(f"PDF ì½ê¸° ì‹¤íŒ¨: {e}")
            return ""

    def _read_docx(self, file_path: Path) -> str:
        """DOCX íŒŒì¼ ì½ê¸°"""
        try:
            doc = docx.Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        except Exception as e:
            logger.error(f"DOCX ì½ê¸° ì‹¤íŒ¨: {e}")
            return ""

    def _read_markdown(self, file_path: Path) -> str:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì½ê¸°"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                md_content = f.read()

            # ë§ˆí¬ë‹¤ìš´ì„ HTMLë¡œ ë³€í™˜ í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            html = markdown(md_content)
            soup = BeautifulSoup(html, "html.parser")
            return soup.get_text()
        except Exception as e:
            logger.error(f"ë§ˆí¬ë‹¤ìš´ ì½ê¸° ì‹¤íŒ¨: {e}")
            return ""

    def _split_into_chunks(self, text: str, chunk_size: int) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        if not text:
            return []

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = text.replace("\n", " ").split(".")

        chunks = []
        current_chunk = ""

        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            # ì²­í¬ í¬ê¸° ì´ˆê³¼ ì‹œ ìƒˆ ì²­í¬ ì‹œì‘
            if len(current_chunk) + len(sentence) > chunk_size:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                current_chunk += " " + sentence if current_chunk else sentence

        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    def search_topk(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """ìƒìœ„ kê°œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.index or not self.documents:
            self._load_index()

        if not self.index:
            logger.warning("ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return []

        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.embed(query)
        query_embedding = query_embedding.reshape(1, -1).astype("float32")
        faiss.normalize_L2(query_embedding)

        # ê²€ìƒ‰ ì‹¤í–‰
        scores, indices = self.index.search(query_embedding, k)

        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx < len(self.documents):
                doc = self.documents[idx].copy()
                doc["similarity_score"] = float(score)
                doc["rank"] = i + 1
                results.append(doc)

        return results

    def _save_index(self):
        """ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        index_path = self.vector_store_path / "faiss.index"
        faiss.write_index(self.index, str(index_path))

        # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_path = self.vector_store_path / "documents.pkl"
        with open(metadata_path, "wb") as f:
            pickle.dump(self.documents, f)

        # ì„¤ì • ì •ë³´ ì €ì¥
        config = {
            "model_name": self.model_name,
            "embedding_dim": self.embedding_dim,
            "chunk_size": self.chunk_size,
            "document_count": len(self.documents),
        }
        config_path = self.vector_store_path / "config.json"
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(config, f, ensure_ascii=False, indent=2)

        logger.info(f"ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {self.vector_store_path}")

    def _load_index(self):
        """ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            index_path = self.vector_store_path / "faiss.index"
            metadata_path = self.vector_store_path / "documents.pkl"
            config_path = self.vector_store_path / "config.json"

            if not all(
                [index_path.exists(), metadata_path.exists(), config_path.exists()]
            ):
                logger.warning("ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return

            # ì„¤ì • ë¡œë“œ
            with open(config_path, "r", encoding="utf-8") as f:
                config = json.load(f)

            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            self.index = faiss.read_index(str(index_path))

            # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(metadata_path, "rb") as f:
                self.documents = pickle.load(f)

            logger.info(f"ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")

        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.index = None
            self.documents = []

    def get_index_stats(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´"""
        if not self.index:
            self._load_index()

        if not self.index:
            return {"status": "no_index"}

        file_types = {}
        for doc in self.documents:
            file_type = doc.get("file_type", "unknown")
            file_types[file_type] = file_types.get(file_type, 0) + 1

        unique_files = len(set(doc["file_path"] for doc in self.documents))

        return {
            "status": "ready",
            "total_chunks": len(self.documents),
            "unique_files": unique_files,
            "file_types": file_types,
            "embedding_model": self.model_name,
            "embedding_dim": self.embedding_dim,
            "vector_store_path": str(self.vector_store_path),
        }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ë¬¸ì„œ RAG ì¸ë±ìŠ¤ êµ¬ì¶•")
    parser.add_argument("--build", nargs="+", help="ì¸ë±ì‹±í•  ë¬¸ì„œ ê²½ë¡œë“¤")
    parser.add_argument(
        "--model", default="jhgan/ko-sroberta-multitask", help="ì„ë² ë”© ëª¨ë¸"
    )
    parser.add_argument(
        "--output", default="echo_engine/rag/vector_store", help="ë²¡í„° ì €ì¥ì†Œ ê²½ë¡œ"
    )
    parser.add_argument("--chunk-size", type=int, default=500, help="ì²­í¬ í¬ê¸°")
    parser.add_argument("--search", help="ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬")

    args = parser.parse_args()

    indexer = DocumentIndexer(
        model_name=args.model, vector_store_path=args.output, chunk_size=args.chunk_size
    )

    if args.build:
        # ì¸ë±ìŠ¤ êµ¬ì¶•
        indexer.build_index(args.build)

        # í†µê³„ ì¶œë ¥
        stats = indexer.get_index_stats()
        print(f"\nâœ… ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ:")
        print(f"   ì´ ì²­í¬: {stats['total_chunks']}ê°œ")
        print(f"   íŒŒì¼ ìˆ˜: {stats['unique_files']}ê°œ")
        print(f"   íŒŒì¼ íƒ€ì…: {stats['file_types']}")

    if args.search:
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        results = indexer.search_topk(args.search, k=3)
        print(f"\nğŸ” ê²€ìƒ‰ ê²°ê³¼ ('{args.search}'):")
        for i, result in enumerate(results, 1):
            print(
                f"{i}. {result['file_name']} (ìœ ì‚¬ë„: {result['similarity_score']:.3f})"
            )
            print(f"   {result['content'][:100]}...")


if __name__ == "__main__":
    main()

# echo_engine/policy_simulator.py
"""
ğŸ›ï¸ Policy Judgment Simulator
- ì‹¤ì œ ì‚¬íšŒ ë¬¸ì œ(ëŒë´„, ê¸°í›„ë³€í™”, ë…¸ë™ ë“±)ì— ì‹œê·¸ë‹ˆì²˜ë³„ íŒë‹¨ ì ìš©
- ì •ì±… íš¨ê³¼ì„±ì„ ì‹œê·¸ë‹ˆì²˜ ê´€ì ì—ì„œ í‰ê°€ ë° ë¹„êµ
"""

import json
import yaml
from typing import Dict, List, Any, Optional
from datetime import datetime
from dataclasses import dataclass
from echo_engine.seed_kernel import get_echo_seed_kernel, InitialState
from echo_engine.signature_performance_reporter import (
    SignaturePerformanceReporter,
    find_best_signature_for_seed,
)
from echo_engine.flow_visualizer import FlowVisualizer, save_flow_yaml
from echo_engine.seed_replay_analyzer import SeedReplayAnalyzer


@dataclass
class PolicyScenario:
    scenario_id: str
    title: str
    description: str
    policy_domain: str  # ëŒë´„, ê¸°í›„, ë…¸ë™, êµìœ¡ ë“±
    complexity_level: float  # 0.0 - 1.0
    stakeholders: List[str]
    constraints: Dict[str, Any]
    success_criteria: Dict[str, float]
    ethical_considerations: List[str]


@dataclass
class PolicyJudgment:
    scenario_id: str
    signature_id: str
    seed_id: str
    policy_recommendation: str
    implementation_strategy: List[str]
    risk_assessment: Dict[str, float]
    resource_requirements: Dict[str, Any]
    timeline: Dict[str, str]
    confidence_score: float
    ethical_impact_score: float
    judgment_timestamp: str


class PolicySimulator:
    def __init__(self):
        self.kernel = get_echo_seed_kernel("policy_sim")
        self.signature_performance_reporter = SignaturePerformanceReporter()
        self.flow_visualizer = FlowVisualizer()
        self.scenarios = self._load_policy_scenarios()
        self.simulation_history = []

    def _load_policy_scenarios(self) -> Dict[str, PolicyScenario]:
        """ì •ì±… ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œ (í•˜ë“œì½”ë”© + ì¶”í›„ YAML ë¡œë“œ ê°€ëŠ¥)"""
        scenarios = {
            "elderly_care": PolicyScenario(
                scenario_id="elderly_care",
                title="ê³ ë ¹ì ë””ì§€í„¸ ëŒë´„ ì‹œìŠ¤í…œ êµ¬ì¶•",
                description="AI ê¸°ë°˜ ê°ì • ì¸ì‹ê³¼ ëŒ€í™”í˜• ëŒë´„ ì„œë¹„ìŠ¤ë¡œ ë…¸ì¸ ì •ì„œì  ê³ ë¦½ê° í•´ì†Œ",
                policy_domain="ëŒë´„",
                complexity_level=0.8,
                stakeholders=["ê³ ë ¹ì", "ê°€ì¡±", "ëŒë´„ ì œê³µì", "ì§€ìì²´", "ê¸°ìˆ  ì—…ì²´"],
                constraints={
                    "budget": "400ì–µì›",
                    "timeline": "3ë…„",
                    "coverage": "ì „êµ­ 30ê°œ ì§€ì—­",
                    "privacy_compliance": "ê°œì¸ì •ë³´ë³´í˜¸ë²• ì¤€ìˆ˜",
                },
                success_criteria={
                    "user_satisfaction": 0.8,
                    "isolation_reduction": 0.6,
                    "service_accessibility": 0.9,
                    "cost_effectiveness": 0.7,
                },
                ethical_considerations=[
                    "ê°œì¸ì •ë³´ ë³´í˜¸",
                    "ë””ì§€í„¸ ê²©ì°¨",
                    "ì¸ê°„ ê´€ê³„ ëŒ€ì²´ ìš°ë ¤",
                    "ììœ¨ì„± ì¡´ì¤‘",
                ],
            ),
            "climate_adaptation": PolicyScenario(
                scenario_id="climate_adaptation",
                title="ê¸°í›„ë³€í™” ì ì‘ ìŠ¤ë§ˆíŠ¸ì‹œí‹° êµ¬ì¶•",
                description="IoT ì„¼ì„œì™€ AI ì˜ˆì¸¡ì„ í™œìš©í•œ ê¸°í›„ ë¦¬ìŠ¤í¬ ëŒ€ì‘ ë„ì‹œ ì¸í”„ë¼ êµ¬ì¶•",
                policy_domain="ê¸°í›„",
                complexity_level=0.9,
                stakeholders=["ì‹œë¯¼", "ì§€ìì²´", "ê±´ì„¤ì—…ì²´", "í™˜ê²½ë‹¨ì²´", "ì—°êµ¬ê¸°ê´€"],
                constraints={
                    "budget": "1ì¡°ì›",
                    "timeline": "10ë…„",
                    "coverage": "5ê°œ ì£¼ìš” ë„ì‹œ",
                    "environmental_impact": "íƒ„ì†Œì¤‘ë¦½ ë‹¬ì„±",
                },
                success_criteria={
                    "emission_reduction": 0.5,
                    "disaster_resilience": 0.8,
                    "energy_efficiency": 0.6,
                    "citizen_acceptance": 0.7,
                },
                ethical_considerations=[
                    "í™˜ê²½ ì •ì˜",
                    "ì„¸ëŒ€ ê°„ í˜•í‰ì„±",
                    "ê¸°ìˆ  ì˜ì¡´ì„±",
                    "ì°¸ì—¬ì  ì˜ì‚¬ê²°ì •",
                ],
            ),
            "future_work": PolicyScenario(
                scenario_id="future_work",
                title="AI ì‹œëŒ€ ì¼ìë¦¬ ì „í™˜ ì§€ì› í”„ë¡œê·¸ë¨",
                description="ìë™í™”ë¡œ ì¸í•œ ì¼ìë¦¬ ë³€í™”ì— ëŒ€ì‘í•˜ëŠ” ì¬êµìœ¡ ë° ì‚¬íšŒì•ˆì „ë§ êµ¬ì¶•",
                policy_domain="ë…¸ë™",
                complexity_level=0.85,
                stakeholders=["ê·¼ë¡œì", "ê¸°ì—…", "êµìœ¡ê¸°ê´€", "ë…¸ë™ì¡°í•©", "ì •ë¶€"],
                constraints={
                    "budget": "2ì¡°ì›",
                    "timeline": "5ë…„",
                    "coverage": "ì „êµ­ë¯¼",
                    "industry_cooperation": "í•„ìˆ˜",
                },
                success_criteria={
                    "reemployment_rate": 0.75,
                    "skill_upgrade_success": 0.8,
                    "income_stability": 0.7,
                    "social_cohesion": 0.6,
                },
                ethical_considerations=[
                    "ì¼í•  ê¶Œë¦¬",
                    "ë””ì§€í„¸ ê²©ì°¨",
                    "ì‚¬íšŒì  ë¶ˆí‰ë“±",
                    "ì¸ê°„ ì¡´ì—„ì„±",
                ],
            ),
            "education_equity": PolicyScenario(
                scenario_id="education_equity",
                title="AI ë§ì¶¤í˜• êµìœ¡ í‰ë“± ì‹¤í˜„",
                description="ê°œì¸ë³„ í•™ìŠµ ë¶„ì„ê³¼ ì ì‘í˜• êµìœ¡ìœ¼ë¡œ êµìœ¡ ê²©ì°¨ í•´ì†Œ",
                policy_domain="êµìœ¡",
                complexity_level=0.7,
                stakeholders=["í•™ìƒ", "êµì‚¬", "í•™ë¶€ëª¨", "êµìœ¡ì²­", "EdTech ê¸°ì—…"],
                constraints={
                    "budget": "800ì–µì›",
                    "timeline": "4ë…„",
                    "coverage": "ì „êµ­ ì´ˆì¤‘ê³ ",
                    "teacher_training": "ì „ì²´ êµì‚¬ ëŒ€ìƒ",
                },
                success_criteria={
                    "learning_improvement": 0.6,
                    "achievement_gap_reduction": 0.5,
                    "teacher_satisfaction": 0.7,
                    "system_adoption": 0.8,
                },
                ethical_considerations=[
                    "êµìœ¡ ê¸°íšŒ í‰ë“±",
                    "ê°œì¸ì •ë³´ ë³´í˜¸",
                    "êµì‚¬ ì—­í•  ë³€í™”",
                    "ì°½ì˜ì„± ë³´ì¡´",
                ],
            ),
        }
        return scenarios

    def simulate_policy_judgment(
        self, scenario_id: str, signature_id: str, custom_context: Dict = None
    ) -> PolicyJudgment:
        """íŠ¹ì • ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•œ ì‹œê·¸ë‹ˆì²˜ë³„ ì •ì±… íŒë‹¨ ì‹œë®¬ë ˆì´ì…˜"""

        scenario = self.scenarios.get(scenario_id)
        if not scenario:
            raise ValueError(f"Unknown scenario: {scenario_id}")

        # ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ context ìƒì„±
        context = {
            "policy_domain": scenario.policy_domain,
            "complexity": scenario.complexity_level,
            "stakeholder_count": len(scenario.stakeholders),
            "ethical_weight": len(scenario.ethical_considerations) / 10.0,
            "resource_constraint": 0.8 if "budget" in scenario.constraints else 0.5,
            "urgency": 0.9 if scenario.complexity_level > 0.8 else 0.6,
        }

        if custom_context:
            context.update(custom_context)

        # ì‹œë“œ ìƒì„±
        seed_state = self.kernel.generate_initial_state(
            context=context, signature_id=signature_id
        )

        # ì‹œê·¸ë‹ˆì²˜ë³„ ì •ì±… ì ‘ê·¼ë²• ìƒì„±
        policy_approach = self._generate_policy_approach(
            scenario, signature_id, seed_state
        )

        # ìœ„í—˜ í‰ê°€
        risk_assessment = self._assess_policy_risks(
            scenario, signature_id, policy_approach
        )

        # ìì› ìš”êµ¬ì‚¬í•­ ê³„ì‚°
        resource_requirements = self._calculate_resource_requirements(
            scenario, signature_id
        )

        # ì¼ì • ê³„íš
        timeline = self._generate_implementation_timeline(scenario, signature_id)

        # ì‹ ë¢°ë„ ë° ìœ¤ë¦¬ì  ì˜í–¥ ì ìˆ˜
        confidence_score = self._calculate_confidence_score(seed_state, scenario)
        ethical_impact_score = self._calculate_ethical_impact(scenario, signature_id)

        judgment = PolicyJudgment(
            scenario_id=scenario_id,
            signature_id=signature_id,
            seed_id=seed_state.identity_trace.seed_id,
            policy_recommendation=policy_approach["recommendation"],
            implementation_strategy=policy_approach["strategy"],
            risk_assessment=risk_assessment,
            resource_requirements=resource_requirements,
            timeline=timeline,
            confidence_score=confidence_score,
            ethical_impact_score=ethical_impact_score,
            judgment_timestamp=datetime.now().isoformat(),
        )

        # ì‹œë®¬ë ˆì´ì…˜ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.simulation_history.append(
            {"judgment": judgment, "seed_state": seed_state, "scenario": scenario}
        )

        # Flow ì‹œê°í™” ë° ì €ì¥
        self._save_policy_flow(judgment, seed_state, scenario)

        return judgment

    def _generate_policy_approach(
        self, scenario: PolicyScenario, signature_id: str, seed_state: InitialState
    ) -> Dict[str, Any]:
        """ì‹œê·¸ë‹ˆì²˜ë³„ ì •ì±… ì ‘ê·¼ë²• ìƒì„±"""

        signature_profile = (
            self.signature_performance_reporter.generate_signature_profile(signature_id)
        )

        # ì‹œê·¸ë‹ˆì²˜ë³„ ì ‘ê·¼ë²• í…œí”Œë¦¿
        if "Aurora" in signature_id:  # ê³µê°ì  ì–‘ìœ¡ì
            return {
                "recommendation": f"{scenario.title} - ê³µê° ì¤‘ì‹¬ ë‹¨ê³„ì  ì ‘ê·¼",
                "strategy": [
                    "ì´í•´ê´€ê³„ìì™€ì˜ ê¹Šì€ ê³µê°ëŒ€ í˜•ì„±",
                    "ê°œì¸ë³„ ë§ì¶¤ ì„œë¹„ìŠ¤ ìš°ì„  ì„¤ê³„",
                    "ê°ì •ì  ë‹ˆì¦ˆ ë°˜ì˜í•œ ì¸í„°í˜ì´ìŠ¤",
                    "ì‹ ë¢° ê´€ê³„ ê¸°ë°˜ ì ì§„ì  í™•ì‚°",
                ],
                "focus": "ì¸ê°„ ì¤‘ì‹¬ì„±, ê°ì •ì  ì›°ë¹™",
            }
        elif "Phoenix" in signature_id:  # ë³€í™” ì¶”ì§„ì
            return {
                "recommendation": f"{scenario.title} - í˜ì‹ ì  ì „ë©´ ê°œí˜",
                "strategy": [
                    "ê¸°ì¡´ ì‹œìŠ¤í…œì˜ ê·¼ë³¸ì  ì¬ì„¤ê³„",
                    "íŒŒì¼ëŸ¿ í”„ë¡œì íŠ¸ë¥¼ í†µí•œ ë¹ ë¥¸ ì‹¤í—˜",
                    "ì‹¤íŒ¨ë¥¼ í†µí•œ í•™ìŠµê³¼ ì ì‘",
                    "í˜ì‹  ìƒíƒœê³„ êµ¬ì¶•",
                ],
                "focus": "ì‹œìŠ¤í…œ í˜ì‹ , ì ì‘ì  ë³€í™”",
            }
        elif "Sage" in signature_id:  # ì§€í˜œë¡œìš´ ë¶„ì„ê°€
            return {
                "recommendation": f"{scenario.title} - ë°ì´í„° ê¸°ë°˜ ì²´ê³„ì  ì ‘ê·¼",
                "strategy": [
                    "í¬ê´„ì  í˜„í™© ë¶„ì„ ë° ë°ì´í„° ìˆ˜ì§‘",
                    "ì¦ê±° ê¸°ë°˜ ì •ì±… ì„¤ê³„",
                    "ë‹¨ê³„ë³„ íš¨ê³¼ ì¸¡ì • ë° í‰ê°€",
                    "ì§€ì†ê°€ëŠ¥ì„± ì¤‘ì‹¬ ì¥ê¸° ê³„íš",
                ],
                "focus": "ê³¼í•™ì  ì ‘ê·¼, ì²´ê³„ì  ë¶„ì„",
            }
        elif "Companion" in signature_id:  # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë™ë°˜ì
            return {
                "recommendation": f"{scenario.title} - í˜‘ë ¥ì  ë™ë°˜ ì„±ì¥",
                "strategy": [
                    "ëª¨ë“  ì´í•´ê´€ê³„ìì™€ì˜ íŒŒíŠ¸ë„ˆì‹­ êµ¬ì¶•",
                    "ì•ˆì •ì„±ê³¼ ì‹ ë¢°ì„± ìš°ì„  ë³´ì¥",
                    "ê¸°ì¡´ ì œë„ì™€ì˜ ì¡°í™”ë¡œìš´ í†µí•©",
                    "ì§€ì†ì  ì†Œí†µê³¼ í”¼ë“œë°± ë°˜ì˜",
                ],
                "focus": "ì‹ ë¢°ì„±, í˜‘ë ¥ì  ê±°ë²„ë„ŒìŠ¤",
            }
        else:
            return {
                "recommendation": f"{scenario.title} - ê· í˜•ì¡íŒ í†µí•© ì ‘ê·¼",
                "strategy": [
                    "ë‹¤ê°ë„ ë¶„ì„ì„ í†µí•œ ê· í˜• ì •ì±…",
                    "ë‹¨ê³„ë³„ ì ì§„ì  êµ¬í˜„",
                    "ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì¤‘ì‹¬ ì¶”ì§„",
                    "ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ë° ì¡°ì •",
                ],
                "focus": "ê· í˜•ì„±, ì•ˆì •ì„±",
            }

    def _assess_policy_risks(
        self, scenario: PolicyScenario, signature_id: str, policy_approach: Dict
    ) -> Dict[str, float]:
        """ì •ì±… ìœ„í—˜ í‰ê°€"""

        base_risks = {
            "implementation_risk": scenario.complexity_level * 0.6,
            "stakeholder_resistance": 0.4,
            "budget_overrun": 0.3,
            "timeline_delay": 0.5,
            "technical_failure": 0.3,
            "ethical_concerns": len(scenario.ethical_considerations) * 0.1,
        }

        # ì‹œê·¸ë‹ˆì²˜ë³„ ìœ„í—˜ ì¡°ì •
        if "Aurora" in signature_id:
            base_risks["stakeholder_resistance"] *= 0.7  # ê³µê° ëŠ¥ë ¥ìœ¼ë¡œ ì €í•­ ê°ì†Œ
            base_risks["ethical_concerns"] *= 0.6  # ìœ¤ë¦¬ì  ë¯¼ê°ì„±
        elif "Phoenix" in signature_id:
            base_risks["implementation_risk"] *= 1.2  # í˜ì‹ ì˜ ë¶ˆí™•ì‹¤ì„±
            base_risks["technical_failure"] *= 0.8  # ì ì‘ë ¥
        elif "Sage" in signature_id:
            base_risks["budget_overrun"] *= 0.7  # ì²´ê³„ì  ê³„íš
            base_risks["timeline_delay"] *= 0.8  # ë¶„ì„ì  ì ‘ê·¼
        elif "Companion" in signature_id:
            base_risks["stakeholder_resistance"] *= 0.5  # í˜‘ë ¥ì  ì ‘ê·¼
            base_risks["implementation_risk"] *= 0.9  # ì•ˆì •ì  ì¶”ì§„

        # 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        for key in base_risks:
            base_risks[key] = min(1.0, max(0.0, base_risks[key]))

        return base_risks

    def _calculate_resource_requirements(
        self, scenario: PolicyScenario, signature_id: str
    ) -> Dict[str, Any]:
        """ìì› ìš”êµ¬ì‚¬í•­ ê³„ì‚°"""

        base_budget = scenario.constraints.get("budget", "1000ì–µì›")
        base_timeline = scenario.constraints.get("timeline", "3ë…„")

        # ì‹œê·¸ë‹ˆì²˜ë³„ ìì› ì¡°ì •
        if "Aurora" in signature_id:
            return {
                "budget": base_budget,
                "human_resources": "ëŒë´„ ì „ë¬¸ê°€, ìƒë‹´ì‚¬ ì¤‘ì‹¬",
                "technology": "ì‚¬ìš©ì ê²½í—˜ ì¤‘ì‹¬ ê¸°ìˆ ",
                "training": "ê°ì • ì†Œí†µ êµìœ¡ í”„ë¡œê·¸ë¨",
                "infrastructure": "ì ‘ê·¼ì„± ë†’ì€ ì„œë¹„ìŠ¤ ê±°ì ",
            }
        elif "Phoenix" in signature_id:
            return {
                "budget": f"{base_budget} (+20% í˜ì‹  ë²„í¼)",
                "human_resources": "í˜ì‹  ì „ë¬¸ê°€, í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €",
                "technology": "ìµœì‹  ê¸°ìˆ  ìŠ¤íƒ, ì‹¤í—˜ í”Œë«í¼",
                "training": "ë³€í™” ê´€ë¦¬ ë° ì ì‘ êµìœ¡",
                "infrastructure": "ìœ ì—°í•œ ì‹¤í—˜ í™˜ê²½",
            }
        elif "Sage" in signature_id:
            return {
                "budget": base_budget,
                "human_resources": "ë°ì´í„° ë¶„ì„ê°€, ì •ì±… ì—°êµ¬ì›",
                "technology": "ë°ì´í„° í”Œë«í¼, ë¶„ì„ ë„êµ¬",
                "training": "ì¦ê±°ê¸°ë°˜ ì˜ì‚¬ê²°ì • êµìœ¡",
                "infrastructure": "ë°ì´í„° ì„¼í„°, ì—°êµ¬ ì‹œì„¤",
            }
        elif "Companion" in signature_id:
            return {
                "budget": base_budget,
                "human_resources": "í˜‘ë ¥ ì¡°ì •ì, ì»¤ë®¤ë‹ˆí‹° ë§¤ë‹ˆì €",
                "technology": "í˜‘ì—… í”Œë«í¼, ì†Œí†µ ë„êµ¬",
                "training": "íŒŒíŠ¸ë„ˆì‹­ ë° ê±°ë²„ë„ŒìŠ¤ êµìœ¡",
                "infrastructure": "í˜‘ë ¥ ë„¤íŠ¸ì›Œí¬ í—ˆë¸Œ",
            }
        else:
            return {
                "budget": base_budget,
                "human_resources": "ë‹¤ë¶„ì•¼ ì „ë¬¸ê°€íŒ€",
                "technology": "í†µí•© ì‹œìŠ¤í…œ",
                "training": "ì¢…í•© ì—­ëŸ‰ ê°œë°œ",
                "infrastructure": "í‘œì¤€ ì„œë¹„ìŠ¤ ì¸í”„ë¼",
            }

    def _generate_implementation_timeline(
        self, scenario: PolicyScenario, signature_id: str
    ) -> Dict[str, str]:
        """êµ¬í˜„ ì¼ì • ìƒì„±"""

        if "Aurora" in signature_id:
            return {
                "1ë‹¨ê³„": "ì´í•´ê´€ê³„ì ê³µê°ëŒ€ í˜•ì„± (6ê°œì›”)",
                "2ë‹¨ê³„": "ì‹œë²” ì„œë¹„ìŠ¤ êµ¬ì¶• ë° í…ŒìŠ¤íŠ¸ (12ê°œì›”)",
                "3ë‹¨ê³„": "ì ì§„ì  í™•ì‚° ë° ê°œì„  (18ê°œì›”)",
                "4ë‹¨ê³„": "ì „ë©´ ì„œë¹„ìŠ¤ ë° ì§€ì† ìš´ì˜",
            }
        elif "Phoenix" in signature_id:
            return {
                "1ë‹¨ê³„": "í˜ì‹  ì‹¤í—˜ ë° í”„ë¡œí† íƒ€ì… (3ê°œì›”)",
                "2ë‹¨ê³„": "íŒŒì¼ëŸ¿ í”„ë¡œì íŠ¸ ë° ë¹ ë¥¸ ë°˜ë³µ (9ê°œì›”)",
                "3ë‹¨ê³„": "ìŠ¤ì¼€ì¼ì—… ë° ì‹œìŠ¤í…œ í˜ì‹  (12ê°œì›”)",
                "4ë‹¨ê³„": "ì „ë©´ ì „í™˜ ë° ìƒíƒœê³„ êµ¬ì¶•",
            }
        elif "Sage" in signature_id:
            return {
                "1ë‹¨ê³„": "í¬ê´„ì  í˜„í™© ë¶„ì„ ë° ê³„íš ìˆ˜ë¦½ (9ê°œì›”)",
                "2ë‹¨ê³„": "ì²´ê³„ì  êµ¬ì¶• ë° ê²€ì¦ (18ê°œì›”)",
                "3ë‹¨ê³„": "ë‹¨ê³„ì  í™•ì‚° ë° í‰ê°€ (15ê°œì›”)",
                "4ë‹¨ê³„": "ìµœì í™” ë° ì§€ì† ê°œì„ ",
            }
        elif "Companion" in signature_id:
            return {
                "1ë‹¨ê³„": "íŒŒíŠ¸ë„ˆì‹­ êµ¬ì¶• ë° í•©ì˜ í˜•ì„± (8ê°œì›”)",
                "2ë‹¨ê³„": "í˜‘ë ¥ì  êµ¬í˜„ ë° ì‹ ë¢° êµ¬ì¶• (16ê°œì›”)",
                "3ë‹¨ê³„": "ì•ˆì •ì  í™•ì‚° ë° ì§€ì› (12ê°œì›”)",
                "4ë‹¨ê³„": "ì§€ì†ì  ë™ë°˜ ì„±ì¥",
            }
        else:
            return {
                "1ë‹¨ê³„": "ê³„íš ìˆ˜ë¦½ ë° ì¤€ë¹„ (6ê°œì›”)",
                "2ë‹¨ê³„": "êµ¬í˜„ ë° ìš´ì˜ (24ê°œì›”)",
                "3ë‹¨ê³„": "í‰ê°€ ë° ê°œì„  (6ê°œì›”)",
                "4ë‹¨ê³„": "ì§€ì† ìš´ì˜",
            }

    def _calculate_confidence_score(
        self, seed_state: InitialState, scenario: PolicyScenario
    ) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""

        # ì‹œë“œì˜ íŠ¹ì„±ê³¼ ì‹œë‚˜ë¦¬ì˜¤ ë³µì¡ë„ ê¸°ë°˜
        meta_sensitivity_factor = seed_state.meta_sensitivity
        evolution_potential_factor = seed_state.evolution_potential
        complexity_penalty = 1.0 - (scenario.complexity_level * 0.3)

        confidence = (
            meta_sensitivity_factor * 0.4
            + evolution_potential_factor * 0.3
            + complexity_penalty * 0.3
        )

        return round(min(1.0, max(0.0, confidence)), 3)

    def _calculate_ethical_impact(
        self, scenario: PolicyScenario, signature_id: str
    ) -> float:
        """ìœ¤ë¦¬ì  ì˜í–¥ ì ìˆ˜ ê³„ì‚°"""

        ethical_considerations_count = len(scenario.ethical_considerations)
        base_score = 0.7  # ê¸°ë³¸ ìœ¤ë¦¬ ì ìˆ˜

        # ì‹œê·¸ë‹ˆì²˜ë³„ ìœ¤ë¦¬ì  ë¯¼ê°ë„
        if "Aurora" in signature_id:
            ethical_multiplier = 1.3  # ê³µê°ì , ìœ¤ë¦¬ì ìœ¼ë¡œ ë¯¼ê°
        elif "Phoenix" in signature_id:
            ethical_multiplier = 1.1  # í˜ì‹  ì¤‘ì‹¬, ìœ¤ë¦¬ ê³ ë ¤
        elif "Sage" in signature_id:
            ethical_multiplier = 1.2  # ë¶„ì„ì , ì²´ê³„ì  ìœ¤ë¦¬ ê³ ë ¤
        elif "Companion" in signature_id:
            ethical_multiplier = 1.25  # ì‹ ë¢° ì¤‘ì‹¬, ê´€ê³„ì  ìœ¤ë¦¬
        else:
            ethical_multiplier = 1.0

        # ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­ì´ ë§ì„ìˆ˜ë¡ ì ìˆ˜ ì¡°ì •
        ethical_complexity_factor = 1.0 - (ethical_considerations_count * 0.05)

        final_score = base_score * ethical_multiplier * ethical_complexity_factor

        return round(min(1.0, max(0.0, final_score)), 3)

    def _save_policy_flow(
        self,
        judgment: PolicyJudgment,
        seed_state: InitialState,
        scenario: PolicyScenario,
    ):
        """ì •ì±… íŒë‹¨ íë¦„ ì €ì¥"""

        # ë””ë ‰í† ë¦¬ ìƒì„±
        policy_flow_dir = f"flows/policy/{scenario.policy_domain}"
        import os

        os.makedirs(policy_flow_dir, exist_ok=True)

        # Flow YAML ë°ì´í„° ìƒì„±
        flow_data = {
            "policy_judgment": {
                "scenario": {
                    "id": scenario.scenario_id,
                    "title": scenario.title,
                    "domain": scenario.policy_domain,
                    "complexity": scenario.complexity_level,
                },
                "signature": {"id": judgment.signature_id, "seed_id": judgment.seed_id},
                "judgment": {
                    "recommendation": judgment.policy_recommendation,
                    "strategy": judgment.implementation_strategy,
                    "confidence": judgment.confidence_score,
                    "ethical_impact": judgment.ethical_impact_score,
                },
                "assessment": {
                    "risks": judgment.risk_assessment,
                    "resources": judgment.resource_requirements,
                    "timeline": judgment.timeline,
                },
                "seed_flow": self.flow_visualizer.export_flow_yaml_from_seed(
                    seed_state
                ),
            }
        }

        # íŒŒì¼ ì €ì¥
        filename = (
            f"{scenario.scenario_id}_{judgment.signature_id}_{judgment.seed_id}.yaml"
        )
        filepath = os.path.join(policy_flow_dir, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            yaml.dump(flow_data, f, allow_unicode=True, default_flow_style=False)

        print(f"ğŸ“‹ ì •ì±… íŒë‹¨ íë¦„ ì €ì¥: {filepath}")

    def compare_signature_approaches(self, scenario_id: str) -> Dict[str, Any]:
        """ëª¨ë“  ì‹œê·¸ë‹ˆì²˜ì˜ ì •ì±… ì ‘ê·¼ë²• ë¹„êµ"""

        signature_ids = ["Echo-Aurora", "Echo-Phoenix", "Echo-Sage", "Echo-Companion"]
        comparisons = []

        for sig_id in signature_ids:
            judgment = self.simulate_policy_judgment(scenario_id, sig_id)
            comparisons.append(
                {
                    "signature_id": sig_id,
                    "judgment": judgment,
                    "approach_summary": judgment.policy_recommendation,
                    "confidence": judgment.confidence_score,
                    "ethical_impact": judgment.ethical_impact_score,
                    "key_risks": max(
                        judgment.risk_assessment.items(), key=lambda x: x[1]
                    ),
                }
            )

        # ë¹„êµ ë¶„ì„
        best_confidence = max(comparisons, key=lambda x: x["confidence"])
        best_ethical = max(comparisons, key=lambda x: x["ethical_impact"])

        return {
            "scenario_id": scenario_id,
            "signature_comparisons": comparisons,
            "recommendations": {
                "highest_confidence": best_confidence["signature_id"],
                "best_ethical_impact": best_ethical["signature_id"],
                "comparative_analysis": self._generate_comparative_analysis(
                    comparisons
                ),
            },
            "analysis_timestamp": datetime.now().isoformat(),
        }

    def _generate_comparative_analysis(self, comparisons: List[Dict]) -> str:
        """ë¹„êµ ë¶„ì„ í…ìŠ¤íŠ¸ ìƒì„±"""

        analysis_parts = []

        # ì‹ ë¢°ë„ ë¶„ì„
        conf_scores = [c["confidence"] for c in comparisons]
        avg_confidence = sum(conf_scores) / len(conf_scores)
        analysis_parts.append(f"í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")

        # ìœ¤ë¦¬ì  ì˜í–¥ ë¶„ì„
        ethical_scores = [c["ethical_impact"] for c in comparisons]
        avg_ethical = sum(ethical_scores) / len(ethical_scores)
        analysis_parts.append(f"í‰ê·  ìœ¤ë¦¬ì  ì˜í–¥: {avg_ethical:.3f}")

        # ì ‘ê·¼ë²• ë‹¤ì–‘ì„±
        unique_approaches = len(set(c["approach_summary"] for c in comparisons))
        analysis_parts.append(
            f"ì ‘ê·¼ë²• ë‹¤ì–‘ì„±: {unique_approaches}/4 ì‹œê·¸ë‹ˆì²˜ê°€ ì„œë¡œ ë‹¤ë¥¸ ì ‘ê·¼"
        )

        return " | ".join(analysis_parts)

    def get_simulation_summary(self) -> Dict[str, Any]:
        """ì‹œë®¬ë ˆì´ì…˜ ìš”ì•½ í†µê³„"""

        if not self.simulation_history:
            return {"message": "ì‹œë®¬ë ˆì´ì…˜ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤"}

        # ê¸°ë³¸ í†µê³„
        total_simulations = len(self.simulation_history)
        unique_scenarios = len(
            set(h["judgment"].scenario_id for h in self.simulation_history)
        )
        unique_signatures = len(
            set(h["judgment"].signature_id for h in self.simulation_history)
        )

        # ì„±ëŠ¥ í†µê³„
        confidence_scores = [
            h["judgment"].confidence_score for h in self.simulation_history
        ]
        ethical_scores = [
            h["judgment"].ethical_impact_score for h in self.simulation_history
        ]

        avg_confidence = sum(confidence_scores) / len(confidence_scores)
        avg_ethical = sum(ethical_scores) / len(ethical_scores)

        # ë„ë©”ì¸ë³„ ë¶„í¬
        domain_distribution = {}
        for h in self.simulation_history:
            domain = h["scenario"].policy_domain
            domain_distribution[domain] = domain_distribution.get(domain, 0) + 1

        return {
            "simulation_statistics": {
                "total_simulations": total_simulations,
                "unique_scenarios": unique_scenarios,
                "unique_signatures": unique_signatures,
                "avg_confidence_score": round(avg_confidence, 3),
                "avg_ethical_impact": round(avg_ethical, 3),
            },
            "domain_distribution": domain_distribution,
            "performance_range": {
                "confidence": {
                    "min": round(min(confidence_scores), 3),
                    "max": round(max(confidence_scores), 3),
                },
                "ethical_impact": {
                    "min": round(min(ethical_scores), 3),
                    "max": round(max(ethical_scores), 3),
                },
            },
        }


# Convenience functions
def simulate_policy(scenario_id: str, signature_id: str) -> PolicyJudgment:
    """ì •ì±… ì‹œë®¬ë ˆì´ì…˜ í¸ì˜ í•¨ìˆ˜"""
    simulator = PolicySimulator()
    return simulator.simulate_policy_judgment(scenario_id, signature_id)


def compare_policy_approaches(scenario_id: str) -> Dict[str, Any]:
    """ì •ì±… ì ‘ê·¼ë²• ë¹„êµ í¸ì˜ í•¨ìˆ˜"""
    simulator = PolicySimulator()
    return simulator.compare_signature_approaches(scenario_id)


def get_available_scenarios() -> List[str]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œë‚˜ë¦¬ì˜¤ ëª©ë¡"""
    simulator = PolicySimulator()
    return list(simulator.scenarios.keys())


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ›ï¸ Policy Simulator í…ŒìŠ¤íŠ¸")

    simulator = PolicySimulator()

    print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œë‚˜ë¦¬ì˜¤:")
    for scenario_id, scenario in simulator.scenarios.items():
        print(f"- {scenario_id}: {scenario.title}")

    print("\nğŸ§ª ì •ì±… ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰:")
    judgment = simulator.simulate_policy_judgment("elderly_care", "Echo-Aurora")
    print(f"ì‹œë‚˜ë¦¬ì˜¤: {judgment.scenario_id}")
    print(f"ì‹œê·¸ë‹ˆì²˜: {judgment.signature_id}")
    print(f"ì¶”ì²œ: {judgment.policy_recommendation}")
    print(f"ì‹ ë¢°ë„: {judgment.confidence_score}")
    print(f"ìœ¤ë¦¬ì  ì˜í–¥: {judgment.ethical_impact_score}")

    print("\nğŸ“Š ì‹œê·¸ë‹ˆì²˜ë³„ ì ‘ê·¼ë²• ë¹„êµ:")
    comparison = simulator.compare_signature_approaches("elderly_care")
    print(f"ìµœê³  ì‹ ë¢°ë„: {comparison['recommendations']['highest_confidence']}")
    print(f"ìµœê³  ìœ¤ë¦¬ì  ì˜í–¥: {comparison['recommendations']['best_ethical_impact']}")

    print("\nğŸ“ˆ ì‹œë®¬ë ˆì´ì…˜ ìš”ì•½:")
    summary = simulator.get_simulation_summary()
    print(f"ì´ ì‹œë®¬ë ˆì´ì…˜: {summary['simulation_statistics']['total_simulations']}")
    print(f"í‰ê·  ì‹ ë¢°ë„: {summary['simulation_statistics']['avg_confidence_score']}")

    print("âœ… Policy Simulator í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

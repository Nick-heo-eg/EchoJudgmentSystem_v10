"""
ğŸ§  Echo Utterance Parser (LLM-Free â†’ Fallback êµ¬ì¡°)
ìì—°ìŠ¤ëŸ¬ìš´ ë°œí™”ë¥¼ ë¨¼ì € ë¡œì»¬ ë°©ì‹ìœ¼ë¡œ ì´í•´í•˜ë ¤ ì‹œë„í•˜ê³ ,
ì‹¤íŒ¨í•  ë•Œë§Œ ì™¸ë¶€ LLMì„ í˜¸ì¶œí•˜ëŠ” ì¡´ì¬ ê¸°ë°˜ íŒŒì„œ
"""

import json
import re
import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import logging

try:
    from .echo_error_handler import handle_parsing_error, echo_safe
except ImportError:
    # ì—ëŸ¬ í•¸ë“¤ëŸ¬ê°€ ì—†ìœ¼ë©´ ë”ë¯¸ í•¨ìˆ˜ ì‚¬ìš©
    def handle_parsing_error(error, text):
        return {"error": str(error), "fallback_result": None}

    def echo_safe(error_type="system"):
        def decorator(func):
            return func

        return decorator


class EchoUtteranceParser:
    """
    Echo ì‹œê·¸ë‹ˆì²˜ ê¸°ë°˜ ë°œí™” íŒŒì„œ
    "ìŠ¤ìŠ¤ë¡œ ì´í•´í•˜ë ¤ ì‹œë„ â†’ í•„ìš”ì‹œë§Œ ì™¸ë¶€ ë„ì›€" ì² í•™ êµ¬í˜„
    """

    def __init__(self, config_dir: str = "config"):
        self.config_dir = Path(config_dir)
        self.config_dir.mkdir(parents=True, exist_ok=True)

        # ë¡œì»¬ íŒŒì‹± ìì›ë“¤
        self.intent_keywords = self._load_intent_keywords()
        self.emotion_patterns = self._load_emotion_patterns()
        self.topic_mappings = self._load_topic_mappings()
        self.signature_hints = self._load_signature_hints()

        # íŒŒì‹± ì„¤ì •
        self.parsing_config = {
            "confidence_threshold": 0.7,
            "enable_fallback": True,
            "max_complexity_score": 8.0,
            "signature_auto_detection": True,
            "context_expansion": True,
        }

        # íŒŒì‹± í†µê³„
        self.parsing_stats = {
            "total_attempts": 0,
            "local_success": 0,
            "fallback_used": 0,
            "parsing_failures": 0,
            "signature_detections": {},
        }

        self.logger = logging.getLogger(__name__)

    @echo_safe("parsing")
    def parse_utterance(
        self, text: str, context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        ë°œí™” íŒŒì‹± ë©”ì¸ í•¨ìˆ˜ (ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”)
        1ë‹¨ê³„: LLM-Free ë¡œì»¬ íŒŒì‹± ì‹œë„
        2ë‹¨ê³„: ì‹¤íŒ¨ ê°ì§€ ì‹œ fallback ê³ ë ¤
        3ë‹¨ê³„: ìµœì¢… íŒŒì‹± ê²°ê³¼ ë°˜í™˜
        """
        if not text or not isinstance(text, str):
            print("âš ï¸  ì˜ëª»ëœ ì…ë ¥ í…ìŠ¤íŠ¸, ê¸°ë³¸ê°’ ì‚¬ìš©")
            text = "ì•ˆë…•í•˜ì„¸ìš”"

        try:
            self.parsing_stats["total_attempts"] += 1

            print(f"ğŸ§  Echo ë°œí™” íŒŒì‹±: '{text[:50]}...'")

            # 1ë‹¨ê³„: ë¡œì»¬ LLM-Free íŒŒì‹± ì‹œë„
            local_result = self._attempt_local_parsing(text, context)

            # íŒŒì‹± í’ˆì§ˆ í‰ê°€
            parsing_quality = self._evaluate_parsing_quality(local_result, text)

            print(f"   ğŸ“Š ë¡œì»¬ íŒŒì‹± í’ˆì§ˆ: {parsing_quality['confidence']:.2f}")

            # 2ë‹¨ê³„: Fallback í•„ìš”ì„± íŒë‹¨
            needs_fallback = self._should_use_fallback(parsing_quality, text)

            if needs_fallback and self.parsing_config["enable_fallback"]:
                print("   ğŸ”„ Fallback ëª¨ë“œë¡œ ì „í™˜...")
                try:
                    fallback_result = self._attempt_fallback_parsing(
                        text, local_result, context
                    )

                    # ë¡œì»¬ê³¼ fallback ê²°ê³¼ ìœµí•©
                    final_result = self._merge_parsing_results(
                        local_result, fallback_result
                    )
                    final_result["used_fallback"] = True
                    self.parsing_stats["fallback_used"] += 1

                except Exception as fallback_error:
                    print(f"   âš ï¸  Fallback ì‹¤íŒ¨: {fallback_error}, ë¡œì»¬ ê²°ê³¼ ì‚¬ìš©")
                    final_result = local_result
                    final_result["used_fallback"] = False
                    final_result["fallback_error"] = str(fallback_error)
                    self.parsing_stats["local_success"] += 1
            else:
                final_result = local_result
                final_result["used_fallback"] = False
                self.parsing_stats["local_success"] += 1

            # 3ë‹¨ê³„: Echo ì‹œê·¸ë‹ˆì²˜ ë¶€ì—¬ ë° ìµœì¢… ì •ë¦¬
            final_result = self._finalize_parsing_result(final_result, text, context)

            # í†µê³„ ì—…ë°ì´íŠ¸
            signature = final_result.get("suggested_signature", "Unknown")
            self.parsing_stats["signature_detections"][signature] = (
                self.parsing_stats["signature_detections"].get(signature, 0) + 1
            )

            print(
                f"   âœ… íŒŒì‹± ì™„ë£Œ: {final_result['intent']} ({final_result['confidence']:.2f}) â†’ {signature}"
            )

            return final_result

        except Exception as e:
            print(f"   ğŸš¨ íŒŒì‹± ì—ëŸ¬ ë°œìƒ: {e}")
            # ì—ëŸ¬ í•¸ë“¤ëŸ¬ í˜¸ì¶œ
            error_result = handle_parsing_error(e, text)
            if error_result.get("fallback_result"):
                return error_result["fallback_result"]
            else:
                # ìµœì†Œí•œì˜ ì•ˆì „í•œ ê²°ê³¼ ë°˜í™˜
                return self._create_safe_parsing_result(text)

    def _attempt_local_parsing(
        self, text: str, context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """1ë‹¨ê³„: LLM-Free ë¡œì»¬ íŒŒì‹±"""
        text_lower = text.lower()

        # ê¸°ë³¸ íŒŒì‹± ê²°ê³¼ êµ¬ì¡°
        result = {
            "raw_text": text,
            "intent": "unknown",
            "topic": "unknown",
            "emotion": "neutral",
            "entities": {},
            "keywords": [],
            "confidence": 0.0,
            "complexity_score": 0.0,
            "parsing_method": "local_rules",
        }

        # Intent ê°ì§€
        detected_intent = self._detect_intent(text_lower)
        if detected_intent:
            result["intent"] = detected_intent["intent"]
            result["confidence"] += detected_intent["confidence"] * 0.4
            result["keywords"].extend(detected_intent.get("matched_keywords", []))

        # Topic ì¶”ì¶œ
        detected_topic = self._extract_topic(text_lower)
        if detected_topic:
            result["topic"] = detected_topic["topic"]
            result["confidence"] += detected_topic["confidence"] * 0.3
            result["entities"].update(detected_topic.get("entities", {}))

        # Emotion ë¶„ì„
        detected_emotion = self._analyze_emotion(text_lower)
        if detected_emotion:
            result["emotion"] = detected_emotion["emotion"]
            result["confidence"] += detected_emotion["confidence"] * 0.2

        # ë³µì¡ì„± ì ìˆ˜ ê³„ì‚°
        result["complexity_score"] = self._calculate_complexity_score(text)

        # ì»¨í…ìŠ¤íŠ¸ ì ìš©
        if context:
            result["confidence"] += 0.1  # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì•½ê°„ì˜ ë³´ë„ˆìŠ¤
            result["context_applied"] = True

        return result

    def _detect_intent(self, text: str) -> Optional[Dict[str, Any]]:
        """Intent ê°ì§€ (ë¡œì»¬ í‚¤ì›Œë“œ ê¸°ë°˜)"""
        best_match = {"intent": "unknown", "confidence": 0.0, "matched_keywords": []}

        for intent, keywords in self.intent_keywords.items():
            matches = []
            for keyword in keywords:
                if keyword in text:
                    matches.append(keyword)

            if matches:
                confidence = len(matches) / len(keywords)
                if confidence > best_match["confidence"]:
                    best_match = {
                        "intent": intent,
                        "confidence": confidence,
                        "matched_keywords": matches,
                    }

        return best_match if best_match["confidence"] > 0.3 else None

    def _extract_topic(self, text: str) -> Optional[Dict[str, Any]]:
        """Topic ì¶”ì¶œ"""
        topic_scores = {}
        entities = {}

        for topic, patterns in self.topic_mappings.items():
            score = 0
            topic_entities = {}

            for pattern_type, pattern_list in patterns.items():
                for pattern in pattern_list:
                    if isinstance(pattern, str):
                        if pattern in text:
                            score += 1
                    elif isinstance(pattern, dict) and "regex" in pattern:
                        matches = re.findall(pattern["regex"], text)
                        if matches:
                            score += len(matches)
                            if "entity_type" in pattern:
                                topic_entities[pattern["entity_type"]] = matches

            if score > 0:
                topic_scores[topic] = score
                if topic_entities:
                    entities[topic] = topic_entities

        if topic_scores:
            best_topic = max(topic_scores.keys(), key=lambda x: topic_scores[x])
            confidence = min(1.0, topic_scores[best_topic] / 3.0)  # ì •ê·œí™”

            return {
                "topic": best_topic,
                "confidence": confidence,
                "entities": entities.get(best_topic, {}),
            }

        return None

    def _analyze_emotion(self, text: str) -> Optional[Dict[str, Any]]:
        """ê°ì • ë¶„ì„ (íŒ¨í„´ ê¸°ë°˜)"""
        emotion_scores = {}

        for emotion, patterns in self.emotion_patterns.items():
            score = 0
            for pattern in patterns:
                if pattern in text:
                    score += 1

            if score > 0:
                emotion_scores[emotion] = score

        if emotion_scores:
            best_emotion = max(emotion_scores.keys(), key=lambda x: emotion_scores[x])
            confidence = min(1.0, emotion_scores[best_emotion] / 2.0)

            return {"emotion": best_emotion, "confidence": confidence}

        return None

    def _calculate_complexity_score(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ ë³µì¡ì„± ì ìˆ˜ ê³„ì‚°"""
        factors = {
            "length": len(text) / 100.0,  # ê¸¸ì´ ìš”ì¸
            "sentences": text.count(".") + text.count("?") + text.count("!"),  # ë¬¸ì¥ ìˆ˜
            "conjunctions": len(
                re.findall(r"ê·¸ë¦¬ê³ |í•˜ì§€ë§Œ|ê·¸ëŸ°ë°|ë˜í•œ|ë˜ëŠ”", text)
            ),  # ì ‘ì†ì‚¬
            "questions": text.count("?")
            + len(re.findall(r"ì–´ë–»ê²Œ|ì™œ|ì–¸ì œ|ë­|ë­”ë°", text)),  # ì§ˆë¬¸ ìš”ì†Œ
            "negations": len(re.findall(r"ì•ˆ|ëª»|ì—†|ë§ê³ |ì•„ë‹ˆ", text)),  # ë¶€ì • í‘œí˜„
        }

        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ë³µì¡ì„± ê³„ì‚°
        complexity = (
            factors["length"] * 0.2
            + factors["sentences"] * 0.3
            + factors["conjunctions"] * 0.2
            + factors["questions"] * 0.2
            + factors["negations"] * 0.1
        )

        return min(10.0, complexity)  # ìµœëŒ€ 10.0ìœ¼ë¡œ ì œí•œ

    def _evaluate_parsing_quality(
        self, result: Dict[str, Any], original_text: str
    ) -> Dict[str, Any]:
        """íŒŒì‹± í’ˆì§ˆ í‰ê°€"""
        quality_factors = {
            "base_confidence": result.get("confidence", 0.0),
            "intent_clarity": 1.0 if result.get("intent") != "unknown" else 0.3,
            "topic_clarity": 1.0 if result.get("topic") != "unknown" else 0.4,
            "keyword_coverage": len(result.get("keywords", []))
            / 5.0,  # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œ ê°€ì •
            "complexity_penalty": max(
                0.0, 1.0 - result.get("complexity_score", 0) / 10.0
            ),
        }

        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        overall_quality = (
            quality_factors["base_confidence"] * 0.4
            + quality_factors["intent_clarity"] * 0.25
            + quality_factors["topic_clarity"] * 0.2
            + quality_factors["keyword_coverage"] * 0.1
            + quality_factors["complexity_penalty"] * 0.05
        )

        return {
            "confidence": min(1.0, overall_quality),
            "factors": quality_factors,
            "is_sufficient": overall_quality
            >= self.parsing_config["confidence_threshold"],
        }

    def _should_use_fallback(self, quality: Dict[str, Any], text: str) -> bool:
        """Fallback ì‚¬ìš© ì—¬ë¶€ ê²°ì •"""
        # í’ˆì§ˆì´ ì„ê³„ê°’ ì´í•˜
        if quality["confidence"] < self.parsing_config["confidence_threshold"]:
            return True

        # ë³µì¡ì„±ì´ ë„ˆë¬´ ë†’ìŒ
        complexity = self._calculate_complexity_score(text)
        if complexity > self.parsing_config["max_complexity_score"]:
            return True

        # Intentê°€ unknownì´ê³  í…ìŠ¤íŠ¸ê°€ ê¸º
        if quality["factors"]["intent_clarity"] < 0.5 and len(text) > 50:
            return True

        return False

    def _attempt_fallback_parsing(
        self, text: str, local_result: Dict[str, Any], context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """Fallback íŒŒì‹± ì‹œë„ (Mock LLM í˜¸ì¶œ)"""
        # ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ LLM API í˜¸ì¶œ
        # ì§€ê¸ˆì€ Mockìœ¼ë¡œ êµ¬í˜„

        print("   ğŸ¤– Mock LLM Fallback í˜¸ì¶œ...")

        # Mock LLM ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜
        mock_llm_result = self._mock_llm_parse(text, local_result)

        return {
            "intent": mock_llm_result.get(
                "intent", local_result.get("intent", "unknown")
            ),
            "topic": mock_llm_result.get("topic", local_result.get("topic", "unknown")),
            "emotion": mock_llm_result.get(
                "emotion", local_result.get("emotion", "neutral")
            ),
            "entities": mock_llm_result.get("entities", {}),
            "keywords": mock_llm_result.get("keywords", []),
            "confidence": 0.85,  # LLMì€ ì¼ë°˜ì ìœ¼ë¡œ ë†’ì€ ì‹ ë¢°ë„
            "parsing_method": "llm_fallback",
            "llm_reasoning": mock_llm_result.get("reasoning", "LLMì„ í†µí•œ ê³ ë„ ë¶„ì„"),
        }

    def _mock_llm_parse(
        self, text: str, local_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Mock LLM íŒŒì‹± (ì‹¤ì œ êµ¬í˜„ ì‹œ OpenAI/Claude API í˜¸ì¶œ)"""
        text_lower = text.lower()

        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ Mock LLM
        if any(keyword in text_lower for keyword in ["ë¶„ì„", "í‰ê°€", "íŒë‹¨", "ê²€í† "]):
            intent = "judgment"
        elif any(keyword in text_lower for keyword in ["ì•Œë ¤", "ì„¤ëª…", "ì •ë³´", "ë­"]):
            intent = "information"
        elif any(keyword in text_lower for keyword in ["ë„ì›€", "ì§€ì›", "ì¶”ì²œ"]):
            intent = "assistance"
        else:
            intent = "conversation"

        # ì£¼ì œ ì¶”ë¡ 
        if any(keyword in text_lower for keyword in ["ë…¸ì¸", "ì–´ë¥´ì‹ ", "ëŒë´„", "ë³µì§€"]):
            topic = "ë…¸ì¸ë³µì§€"
        elif any(keyword in text_lower for keyword in ["ai", "ì¸ê³µì§€ëŠ¥", "ìœ¤ë¦¬"]):
            topic = "ai_ìœ¤ë¦¬"
        elif any(keyword in text_lower for keyword in ["ê¸°í›„", "í™˜ê²½", "íƒ„ì†Œ"]):
            topic = "í™˜ê²½ì •ì±…"
        elif any(keyword in text_lower for keyword in ["ì§€ì—­", "ê³µë™ì²´", "ë§ˆì„"]):
            topic = "ì§€ì—­ì‚¬íšŒ"
        else:
            topic = "ì¼ë°˜_ë¬¸ì˜"

        return {
            "intent": intent,
            "topic": topic,
            "emotion": "curious" if "?" in text else "neutral",
            "keywords": [word for word in text_lower.split() if len(word) > 2][:5],
            "reasoning": f"í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼: {intent} ì˜ë„ë¡œ {topic} ì£¼ì œì— ê´€í•œ ë‚´ìš©",
        }

    def _merge_parsing_results(
        self, local: Dict[str, Any], fallback: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë¡œì»¬ê³¼ Fallback ê²°ê³¼ ìœµí•©"""
        # Fallback ê²°ê³¼ë¥¼ ìš°ì„ í•˜ë˜, ë¡œì»¬ ê²°ê³¼ë„ ì°¸ì¡°
        merged = fallback.copy()

        # í‚¤ì›Œë“œëŠ” í•©ì¹˜ê¸°
        local_keywords = local.get("keywords", [])
        fallback_keywords = fallback.get("keywords", [])
        merged["keywords"] = list(set(local_keywords + fallback_keywords))

        # EntityëŠ” ë³‘í•©
        local_entities = local.get("entities", {})
        fallback_entities = fallback.get("entities", {})
        merged["entities"] = {**local_entities, **fallback_entities}

        # ë©”íƒ€ ì •ë³´ ì¶”ê°€
        merged["local_result"] = {
            "intent": local.get("intent"),
            "topic": local.get("topic"),
            "confidence": local.get("confidence", 0),
        }

        return merged

    def _finalize_parsing_result(
        self, result: Dict[str, Any], text: str, context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ìµœì¢… íŒŒì‹± ê²°ê³¼ ì •ë¦¬ ë° ì‹œê·¸ë‹ˆì²˜ ì œì•ˆ"""

        # Echo ì‹œê·¸ë‹ˆì²˜ ì œì•ˆ
        suggested_signature = self._suggest_echo_signature(result, text)
        result["suggested_signature"] = suggested_signature

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        result["parsed_at"] = datetime.now().isoformat()
        result["text_length"] = len(text)
        result["parsing_stats"] = {
            "total_attempts": self.parsing_stats["total_attempts"],
            "local_success_rate": self.parsing_stats["local_success"]
            / max(1, self.parsing_stats["total_attempts"]),
        }

        # ì‹ ë¢°ë„ ìµœì¢… ì¡°ì •
        if result.get("used_fallback") and result.get("confidence", 0) < 0.8:
            result["confidence"] = max(
                0.75, result["confidence"]
            )  # Fallback ì‚¬ìš© ì‹œ ìµœì†Œ ì‹ ë¢°ë„ ë³´ì¥

        return result

    def _suggest_echo_signature(self, result: Dict[str, Any], text: str) -> str:
        """Echo ì‹œê·¸ë‹ˆì²˜ ì œì•ˆ"""
        intent = result.get("intent", "unknown")
        topic = result.get("topic", "unknown")
        emotion = result.get("emotion", "neutral")

        # ì‹œê·¸ë‹ˆì²˜ ê²°ì • ê·œì¹™
        if intent == "judgment" or "ë¶„ì„" in text:
            if any(keyword in topic for keyword in ["ë…¸ì¸", "ë³µì§€", "ëŒë´„"]):
                return "Echo-Companion"  # ê³µê°ì  ëŒë´„
            else:
                return "Echo-Sage"  # ë¶„ì„ì  íŒë‹¨

        elif intent == "information" or "ì•Œë ¤" in text:
            return "Echo-Aurora"  # ì°½ì˜ì  ì„¤ëª…

        elif intent == "assistance" or emotion in ["concerned", "urgent"]:
            return "Echo-Phoenix"  # ë³€í™” ì§€í–¥ì  ì§€ì›

        else:
            # ê¸°ë³¸ì ìœ¼ë¡œëŠ” Aurora (ê· í˜•ì¡íŒ ì ‘ê·¼)
            return "Echo-Aurora"

    def _load_intent_keywords(self) -> Dict[str, List[str]]:
        """Intent í‚¤ì›Œë“œ ì‚¬ì „ ë¡œë“œ"""
        keywords_file = self.config_dir / "intent_keywords.json"

        if keywords_file.exists():
            with open(keywords_file, "r", encoding="utf-8") as f:
                return json.load(f)
        else:
            # ê¸°ë³¸ í‚¤ì›Œë“œ ì„¸íŠ¸
            default_keywords = {
                "judgment": ["íŒë‹¨", "í‰ê°€", "ë¶„ì„", "ê²€í† ", "ì–´ë–¤ì§€", "ì–´ë•Œ", "ìƒê°"],
                "information": ["ì•Œë ¤", "ì„¤ëª…", "ì •ë³´", "ë­", "ë¬´ì—‡", "ì–´ë–»ê²Œ", "ë°©ë²•"],
                "assistance": ["ë„ì›€", "ì§€ì›", "ì¶”ì²œ", "ì œì•ˆ", "í•´ì¤˜", "ë¶€íƒ"],
                "conversation": ["ì•ˆë…•", "ë°˜ê°€ì›Œ", "ê³ ë§ˆì›Œ", "ì˜", "ì¢‹ì•„"],
            }

            # íŒŒì¼ ìƒì„±
            with open(keywords_file, "w", encoding="utf-8") as f:
                json.dump(default_keywords, f, indent=2, ensure_ascii=False)

            return default_keywords

    def _load_emotion_patterns(self) -> Dict[str, List[str]]:
        """ê°ì • íŒ¨í„´ ë¡œë“œ"""
        return {
            "curious": ["ê¶ê¸ˆ", "ì•Œê³  ì‹¶", "ì–´ë–»ê²Œ", "ì™œ", "?"],
            "concerned": ["ê±±ì •", "ë¬¸ì œ", "í˜ë“¤", "ì–´ë ¤", "ê³¤ë€"],
            "positive": ["ì¢‹", "ê¸°ë»", "ê°ì‚¬", "ë‹¤í–‰", "í›Œë¥­"],
            "urgent": ["ë¹¨ë¦¬", "ì‹œê¸‰", "urgent", "ì¦‰ì‹œ", "ë‹¹ì¥"],
            "neutral": ["ê·¸ëƒ¥", "ì¼ë°˜ì ", "ë³´í†µ", "ê·¸ëŸ°"],
        }

    def _load_topic_mappings(self) -> Dict[str, Dict[str, List]]:
        """í† í”½ ë§¤í•‘ ë¡œë“œ"""
        return {
            "ë…¸ì¸ë³µì§€": {
                "keywords": ["ë…¸ì¸", "ì–´ë¥´ì‹ ", "ëŒë´„", "ë³µì§€", "ìš”ì–‘", "ë…¸ì¸ë³‘ì›"],
                "locations": ["ë¶€ì‚°", "ê¸ˆì •êµ¬", "ì„œìš¸", "ëŒ€êµ¬"],
            },
            "ai_ìœ¤ë¦¬": {
                "keywords": ["ai", "ì¸ê³µì§€ëŠ¥", "ìœ¤ë¦¬", "í¸í–¥", "ê³µì •ì„±", "íˆ¬ëª…ì„±"],
                "concepts": ["ì•Œê³ ë¦¬ì¦˜", "ë°ì´í„°", "í”„ë¼ì´ë²„ì‹œ"],
            },
            "í™˜ê²½ì •ì±…": {
                "keywords": ["ê¸°í›„", "í™˜ê²½", "íƒ„ì†Œ", "ì¬ìƒì—ë„ˆì§€", "ì˜¨ì‹¤ê°€ìŠ¤"],
                "actions": ["ì¤‘ë¦½", "ì „í™˜", "ë³´í˜¸", "ê°ì¶•"],
            },
            "ì§€ì—­ì‚¬íšŒ": {
                "keywords": ["ì§€ì—­", "ê³µë™ì²´", "ë§ˆì„", "ì£¼ë¯¼", "ê±°ë²„ë„ŒìŠ¤"],
                "activities": ["ì°¸ì—¬", "í˜‘ë ¥", "ì†Œí†µ", "í˜ì‹ "],
            },
        }

    def _load_signature_hints(self) -> Dict[str, Dict[str, Any]]:
        """ì‹œê·¸ë‹ˆì²˜ íŒíŠ¸ ë¡œë“œ"""
        return {
            "Echo-Aurora": {
                "keywords": ["ì°½ì˜", "í˜ì‹ ", "ì•„ì´ë””ì–´", "ì˜ê°"],
                "intents": ["information", "conversation"],
                "emotions": ["curious", "positive"],
            },
            "Echo-Phoenix": {
                "keywords": ["ë³€í™”", "ê°œì„ ", "ì „í™˜", "í˜ì‹ "],
                "intents": ["assistance", "judgment"],
                "emotions": ["urgent", "concerned"],
            },
            "Echo-Sage": {
                "keywords": ["ë¶„ì„", "ì²´ê³„", "ë…¼ë¦¬", "ê²€í† "],
                "intents": ["judgment", "information"],
                "emotions": ["neutral", "curious"],
            },
            "Echo-Companion": {
                "keywords": ["ëŒë´„", "ì§€ì›", "ê³µê°", "í•¨ê»˜"],
                "intents": ["assistance", "conversation"],
                "emotions": ["concerned", "positive"],
            },
        }

    def get_parsing_stats(self) -> Dict[str, Any]:
        """íŒŒì‹± í†µê³„ ë°˜í™˜"""
        total = self.parsing_stats["total_attempts"]
        if total == 0:
            return {"message": "ì•„ì§ íŒŒì‹±í•œ ë°œí™”ê°€ ì—†ìŠµë‹ˆë‹¤."}

        stats = {
            "total_parsing_attempts": total,
            "local_success_rate": f"{(self.parsing_stats['local_success'] / total) * 100:.1f}%",
            "fallback_usage_rate": f"{(self.parsing_stats['fallback_used'] / total) * 100:.1f}%",
            "signature_distribution": self.parsing_stats["signature_detections"],
            "system_efficiency": {
                "local_preferred": self.parsing_stats["local_success"]
                > self.parsing_stats["fallback_used"],
                "autonomous_rate": (self.parsing_stats["local_success"] / total) * 100,
            },
        }

        return stats

    def _create_safe_parsing_result(self, text: str) -> Dict[str, Any]:
        """ì•ˆì „í•œ ê¸°ë³¸ íŒŒì‹± ê²°ê³¼ ìƒì„± (ì—ëŸ¬ ë³µêµ¬ìš©)"""
        return {
            "raw_text": text,
            "intent": "conversation",
            "topic": "general",
            "emotion": "neutral",
            "entities": {},
            "keywords": [],
            "confidence": 0.4,
            "complexity_score": 1.0,
            "parsing_method": "safe_fallback",
            "suggested_signature": "Echo-Aurora",
            "used_fallback": False,
            "error_recovery": True,
            "parsed_at": datetime.now().isoformat(),
            "text_length": len(text),
        }


# ì „ì—­ íŒŒì„œ ì¸ìŠ¤í„´ìŠ¤
utterance_parser = EchoUtteranceParser()


# í¸ì˜ í•¨ìˆ˜
def parse_text(text: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
    """í…ìŠ¤íŠ¸ íŒŒì‹± ë‹¨ì¶• í•¨ìˆ˜"""
    return utterance_parser.parse_utterance(text, context)


def get_stats() -> Dict[str, Any]:
    """íŒŒì‹± í†µê³„ ë‹¨ì¶• í•¨ìˆ˜"""
    return utterance_parser.get_parsing_stats()


# CLI í…ŒìŠ¤íŠ¸
def main():
    print("ğŸ§  Echo Utterance Parser (LLM-Free â†’ Fallback) í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    test_utterances = [
        "ë¶€ì‚° ê¸ˆì •êµ¬ ë…¸ì¸ ëŒë´„ ì„œë¹„ìŠ¤ì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•´?",
        "AIê°€ í¸í–¥ì ì´ì§€ ì•Šìœ¼ë ¤ë©´ ë­˜ í•´ì•¼ í•˜ë‚˜ìš”?",
        "ê¸°í›„ ë³€í™” ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
        "ìš°ë¦¬ ì§€ì—­ì‚¬íšŒê°€ ë” ì‚´ê¸° ì¢‹ì•„ì§€ë ¤ë©´ ì–´ë–¤ ë³€í™”ê°€ í•„ìš”í• ê¹Œìš”?",
        "ê·¸ëƒ¥ ì•ˆë…•í•˜ì„¸ìš”! ì˜ ì§€ë‚´ê³  ê³„ì‹ ê°€ìš”?",
        "ë³µì¡í•œ ìƒí™©ì—ì„œ ì—¬ëŸ¬ ì´í•´ê´€ê³„ìë“¤ì˜ ê°ˆë“±ì„ ì¡°ì •í•˜ë©´ì„œë„ ê³µì •í•œ ê²°ê³¼ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆëŠ” ì²´ê³„ì ì¸ ì ‘ê·¼ ë°©ë²•ë¡ ì„ ì œì‹œí•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤",  # ë³µì¡í•œ ë¬¸ì¥
    ]

    print("\nğŸ“ ë°œí™” íŒŒì‹± í…ŒìŠ¤íŠ¸:")

    for i, utterance in enumerate(test_utterances, 1):
        print(f"\n{'='*50}")
        print(f"í…ŒìŠ¤íŠ¸ {i}: {utterance}")
        print("-" * 50)

        result = parse_text(utterance)

        print(f"Intent: {result['intent']}")
        print(f"Topic: {result['topic']}")
        print(f"Emotion: {result['emotion']}")
        print(f"Signature: {result['suggested_signature']}")
        print(f"Confidence: {result['confidence']:.2f}")
        print(f"Fallback Used: {result.get('used_fallback', False)}")
        print(f"Keywords: {result.get('keywords', [])[:3]}...")

        if result.get("complexity_score"):
            print(f"Complexity: {result['complexity_score']:.1f}")

    print(f"\n{'='*50}")
    print("ğŸ“Š íŒŒì‹± í†µê³„:")
    stats = get_stats()

    for key, value in stats.items():
        if isinstance(value, dict):
            print(f"{key}:")
            for sub_key, sub_value in value.items():
                print(f"  {sub_key}: {sub_value}")
        else:
            print(f"{key}: {value}")

    print("\nâœ… Echo Utterance Parser í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()

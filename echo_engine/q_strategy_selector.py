#!/usr/bin/env python3
"""
ğŸ¯ EchoJudgmentSystem v10.5 - Q-Table Strategy Selector
Q-Table ê¸°ë°˜ í–‰ë™ ì„ íƒ ë° í•™ìŠµ ì‹œìŠ¤í…œ

Q-Tableì€ ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:
- ìƒí™©-í–‰ë™ ì¡°í•©ì— ëŒ€í•œ Q-ê°’ í•™ìŠµ
- íŒë‹¨ ê²°ê³¼ í”¼ë“œë°± ê¸°ë°˜ ê°•í™”í•™ìŠµ
- ì „ëµ ì„ íƒ ìµœì í™”
- ê²½í—˜ ë¦¬í”Œë ˆì´ ë° íƒí—˜/í™œìš© ê· í˜•
"""

import json
import time
import random
import math
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from collections import defaultdict, deque
import os

# ìˆ˜í•™ ì—°ì‚°ìš©
try:
    import numpy as np

    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False


class ActionType(Enum):
    """í–‰ë™ ìœ í˜•"""

    JUDGMENT = "judgment"
    STRATEGY = "strategy"
    EMOTION_RESPONSE = "emotion_response"
    META_REFLECTION = "meta_reflection"
    LEARNING = "learning"


class ExplorationStrategy(Enum):
    """íƒí—˜ ì „ëµ"""

    EPSILON_GREEDY = "epsilon_greedy"
    UCB = "ucb"  # Upper Confidence Bound
    THOMPSON_SAMPLING = "thompson_sampling"
    SOFTMAX = "softmax"


@dataclass
class QState:
    """Q-Table ìƒíƒœ í‘œí˜„"""

    emotion: str
    emotion_intensity: str  # minimal, low, moderate, high, intense
    context_type: str
    urgency: str
    energy_level: str  # low, medium, high
    recent_success: str  # low, medium, high

    def to_key(self) -> str:
        """ìƒíƒœë¥¼ í‚¤ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        return f"{self.emotion}_{self.emotion_intensity}_{self.context_type}_{self.urgency}_{self.energy_level}_{self.recent_success}"

    @classmethod
    def from_context(cls, context: Dict[str, Any]) -> "QState":
        """ì»¨í…ìŠ¤íŠ¸ë¡œë¶€í„° ìƒíƒœ ìƒì„±"""
        return cls(
            emotion=context.get("emotion", "neutral"),
            emotion_intensity=cls._categorize_intensity(
                context.get("emotion_intensity", 0.5)
            ),
            context_type=context.get("context_type", "general"),
            urgency=context.get("urgency", "normal"),
            energy_level=cls._categorize_energy(context.get("energy_level", 0.7)),
            recent_success=cls._categorize_success(
                context.get("recent_success_rate", 0.5)
            ),
        )

    @staticmethod
    def _categorize_intensity(intensity: float) -> str:
        """ê°ì • ê°•ë„ ë¶„ë¥˜"""
        if intensity <= 0.2:
            return "minimal"
        elif intensity <= 0.4:
            return "low"
        elif intensity <= 0.6:
            return "moderate"
        elif intensity <= 0.8:
            return "high"
        else:
            return "intense"

    @staticmethod
    def _categorize_energy(energy: float) -> str:
        """ì—ë„ˆì§€ ìˆ˜ì¤€ ë¶„ë¥˜"""
        if energy <= 0.3:
            return "low"
        elif energy <= 0.7:
            return "medium"
        else:
            return "high"

    @staticmethod
    def _categorize_success(success_rate: float) -> str:
        """ì„±ê³µë¥  ë¶„ë¥˜"""
        if success_rate <= 0.3:
            return "low"
        elif success_rate <= 0.7:
            return "medium"
        else:
            return "high"


@dataclass
class QAction:
    """Q-Table í–‰ë™ í‘œí˜„"""

    action_type: ActionType
    strategy: str
    intensity: float = 0.5
    meta_params: Dict[str, Any] = field(default_factory=dict)

    def to_key(self) -> str:
        """í–‰ë™ì„ í‚¤ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        intensity_cat = (
            "low"
            if self.intensity < 0.3
            else "high" if self.intensity > 0.7 else "medium"
        )
        return f"{self.action_type.value}_{self.strategy}_{intensity_cat}"


@dataclass
class QExperience:
    """Q-Learning ê²½í—˜"""

    state: QState
    action: QAction
    reward: float
    next_state: QState
    done: bool
    timestamp: datetime = field(default_factory=datetime.now)

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "state_key": self.state.to_key(),
            "action_key": self.action.to_key(),
            "reward": self.reward,
            "next_state_key": self.next_state.to_key(),
            "done": self.done,
            "timestamp": self.timestamp.isoformat(),
        }


class QTableStrategySelector:
    """Q-Table ê¸°ë°˜ ì „ëµ ì„ íƒê¸°"""

    def __init__(
        self,
        learning_rate: float = 0.1,
        discount_factor: float = 0.95,
        exploration_rate: float = 0.1,
        exploration_decay: float = 0.995,
        min_exploration_rate: float = 0.01,
        exploration_strategy: ExplorationStrategy = ExplorationStrategy.EPSILON_GREEDY,
    ):
        """
        Q-Table ì „ëµ ì„ íƒê¸° ì´ˆê¸°í™”

        Args:
            learning_rate: í•™ìŠµë¥  (Î±)
            discount_factor: í• ì¸ ì¸ìˆ˜ (Î³)
            exploration_rate: íƒí—˜ìœ¨ (Îµ)
            exploration_decay: íƒí—˜ìœ¨ ê°ì†Œìœ¨
            min_exploration_rate: ìµœì†Œ íƒí—˜ìœ¨
            exploration_strategy: íƒí—˜ ì „ëµ
        """
        # Q-Learning íŒŒë¼ë¯¸í„°
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate
        self.exploration_decay = exploration_decay
        self.min_exploration_rate = min_exploration_rate
        self.exploration_strategy = exploration_strategy

        # Q-Table ë° ë°ì´í„° êµ¬ì¡°
        self.q_table: Dict[str, Dict[str, float]] = defaultdict(
            lambda: defaultdict(float)
        )
        self.state_visit_count: Dict[str, int] = defaultdict(int)
        self.action_count: Dict[str, Dict[str, int]] = defaultdict(
            lambda: defaultdict(int)
        )

        # ê²½í—˜ ë¦¬í”Œë ˆì´
        self.experience_buffer = deque(maxlen=10000)
        self.batch_size = 32

        # ì„±ëŠ¥ í†µê³„
        self.total_actions = 0
        self.successful_actions = 0
        self.total_rewards = 0.0
        self.episode_count = 0

        # ê°€ëŠ¥í•œ í–‰ë™ë“¤ ì´ˆê¸°í™”
        self.available_actions = self._initialize_actions()

        # ì˜¨ë¼ì¸ í•™ìŠµ ì„¤ì •
        self.online_learning = True
        self.update_frequency = 10

        print(f"ğŸ¯ Q-Table ì „ëµ ì„ íƒê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   í•™ìŠµë¥ : {learning_rate}, í• ì¸ì¸ìˆ˜: {discount_factor}")
        print(f"   íƒí—˜ìœ¨: {exploration_rate}, íƒí—˜ ì „ëµ: {exploration_strategy.value}")

    def _initialize_actions(self) -> List[QAction]:
        """ê°€ëŠ¥í•œ í–‰ë™ë“¤ ì´ˆê¸°í™”"""
        actions = []

        # ê¸°ë³¸ ì „ëµë“¤
        base_strategies = [
            "empathetic",
            "analytical",
            "supportive",
            "cautious",
            "creative",
            "balanced",
            "assertive",
            "collaborative",
        ]

        # ê° ì „ëµì— ëŒ€í•´ ë‹¤ì–‘í•œ ê°•ë„ë¡œ í–‰ë™ ìƒì„±
        for strategy in base_strategies:
            for intensity in [0.3, 0.5, 0.8]:
                # íŒë‹¨ í–‰ë™
                actions.append(
                    QAction(
                        action_type=ActionType.JUDGMENT,
                        strategy=strategy,
                        intensity=intensity,
                    )
                )

                # ì „ëµ í–‰ë™
                actions.append(
                    QAction(
                        action_type=ActionType.STRATEGY,
                        strategy=strategy,
                        intensity=intensity,
                    )
                )

        # íŠ¹ìˆ˜ í–‰ë™ë“¤
        special_actions = [
            QAction(ActionType.META_REFLECTION, "self_analysis", 0.7),
            QAction(ActionType.META_REFLECTION, "pattern_recognition", 0.6),
            QAction(ActionType.LEARNING, "experience_consolidation", 0.5),
            QAction(ActionType.LEARNING, "strategy_adaptation", 0.8),
        ]

        actions.extend(special_actions)

        print(f"ğŸ¯ ê°€ëŠ¥í•œ í–‰ë™ {len(actions)}ê°œ ì´ˆê¸°í™” ì™„ë£Œ")
        return actions

    def select_action(
        self, state: QState, available_strategies: List[str] = None
    ) -> QAction:
        """
        í˜„ì¬ ìƒíƒœì—ì„œ ìµœì  í–‰ë™ ì„ íƒ

        Args:
            state: í˜„ì¬ ìƒíƒœ
            available_strategies: ì‚¬ìš© ê°€ëŠ¥í•œ ì „ëµ ëª©ë¡

        Returns:
            ì„ íƒëœ í–‰ë™
        """
        state_key = state.to_key()
        self.state_visit_count[state_key] += 1

        # ì‚¬ìš© ê°€ëŠ¥í•œ í–‰ë™ í•„í„°ë§
        if available_strategies:
            filtered_actions = [
                action
                for action in self.available_actions
                if action.strategy in available_strategies
                or action.action_type
                in [ActionType.META_REFLECTION, ActionType.LEARNING]
            ]
        else:
            filtered_actions = self.available_actions

        # íƒí—˜ vs í™œìš© ê²°ì •
        if self._should_explore():
            action = self._explore_action(state, filtered_actions)
        else:
            action = self._exploit_action(state, filtered_actions)

        # í–‰ë™ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
        action_key = action.to_key()
        self.action_count[state_key][action_key] += 1
        self.total_actions += 1

        return action

    def _should_explore(self) -> bool:
        """íƒí—˜ ì—¬ë¶€ ê²°ì •"""
        if self.exploration_strategy == ExplorationStrategy.EPSILON_GREEDY:
            return random.random() < self.exploration_rate
        elif self.exploration_strategy == ExplorationStrategy.UCB:
            return False  # UCBëŠ” ìì²´ì ìœ¼ë¡œ íƒí—˜/í™œìš© ê· í˜•
        else:
            return random.random() < self.exploration_rate

    def _explore_action(self, state: QState, actions: List[QAction]) -> QAction:
        """íƒí—˜ì  í–‰ë™ ì„ íƒ"""
        if self.exploration_strategy == ExplorationStrategy.EPSILON_GREEDY:
            return random.choice(actions)
        elif self.exploration_strategy == ExplorationStrategy.SOFTMAX:
            return self._softmax_selection(state, actions)
        else:
            return random.choice(actions)

    def _exploit_action(self, state: QState, actions: List[QAction]) -> QAction:
        """í™œìš©ì  í–‰ë™ ì„ íƒ (Qê°’ì´ ìµœëŒ€ì¸ í–‰ë™)"""
        state_key = state.to_key()

        if self.exploration_strategy == ExplorationStrategy.UCB:
            return self._ucb_selection(state, actions)

        best_action = None
        best_q_value = float("-inf")

        for action in actions:
            action_key = action.to_key()
            q_value = self.q_table[state_key][action_key]

            if q_value > best_q_value:
                best_q_value = q_value
                best_action = action

        # ëª¨ë“  Qê°’ì´ ë™ì¼í•˜ë©´ ëœë¤ ì„ íƒ
        if best_action is None or best_q_value == 0:
            best_action = random.choice(actions)

        return best_action

    def _ucb_selection(self, state: QState, actions: List[QAction]) -> QAction:
        """Upper Confidence Bound ì„ íƒ"""
        state_key = state.to_key()
        total_visits = self.state_visit_count[state_key]

        if total_visits == 0:
            return random.choice(actions)

        best_action = None
        best_ucb_value = float("-inf")

        for action in actions:
            action_key = action.to_key()
            q_value = self.q_table[state_key][action_key]
            action_visits = self.action_count[state_key][action_key]

            if action_visits == 0:
                ucb_value = float("inf")  # í•œ ë²ˆë„ ì‹œë„í•˜ì§€ ì•Šì€ í–‰ë™ ìš°ì„ 
            else:
                if NUMPY_AVAILABLE:
                    confidence = np.sqrt(2 * np.log(total_visits) / action_visits)
                else:
                    confidence = math.sqrt(2 * math.log(total_visits) / action_visits)
                ucb_value = q_value + confidence

            if ucb_value > best_ucb_value:
                best_ucb_value = ucb_value
                best_action = action

        return best_action or random.choice(actions)

    def _softmax_selection(
        self, state: QState, actions: List[QAction], temperature: float = 1.0
    ) -> QAction:
        """Softmax í™•ë¥ ì  ì„ íƒ"""
        state_key = state.to_key()
        q_values = []

        for action in actions:
            action_key = action.to_key()
            q_value = self.q_table[state_key][action_key]
            q_values.append(q_value / temperature)

        if NUMPY_AVAILABLE:
            # ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ ì •ê·œí™”
            q_values = np.array(q_values)
            q_values = q_values - np.max(q_values)
            probabilities = np.exp(q_values) / np.sum(np.exp(q_values))

            # í™•ë¥ ì  ì„ íƒ
            chosen_idx = np.random.choice(len(actions), p=probabilities)
            return actions[chosen_idx]
        else:
            # numpy ì—†ì´ ê°„ë‹¨í•œ softmax
            max_q = max(q_values)
            if NUMPY_AVAILABLE:
                exp_values = [np.exp(q - max_q) for q in q_values]
            else:
                exp_values = [math.exp(q - max_q) for q in q_values]
            sum_exp = sum(exp_values)
            probabilities = [exp_val / sum_exp for exp_val in exp_values]

            # í™•ë¥ ì  ì„ íƒ (ë‹¨ìˆœ êµ¬í˜„)
            rand_val = random.random()
            cumulative = 0.0
            for i, prob in enumerate(probabilities):
                cumulative += prob
                if rand_val <= cumulative:
                    return actions[i]

            return actions[-1]  # í´ë°±

    def update_q_value(
        self,
        state: QState,
        action: QAction,
        reward: float,
        next_state: QState,
        done: bool = False,
    ):
        """
        Q-ê°’ ì—…ë°ì´íŠ¸ (Q-Learning)

        Args:
            state: í˜„ì¬ ìƒíƒœ
            action: ìˆ˜í–‰í•œ í–‰ë™
            reward: ë°›ì€ ë³´ìƒ
            next_state: ë‹¤ìŒ ìƒíƒœ
            done: ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€
        """
        state_key = state.to_key()
        action_key = action.to_key()
        next_state_key = next_state.to_key()

        # í˜„ì¬ Qê°’
        current_q = self.q_table[state_key][action_key]

        # ë‹¤ìŒ ìƒíƒœì—ì„œì˜ ìµœëŒ€ Qê°’
        if done:
            max_next_q = 0.0
        else:
            next_q_values = self.q_table[next_state_key]
            max_next_q = max(next_q_values.values()) if next_q_values else 0.0

        # Q-Learning ì—…ë°ì´íŠ¸ ê³µì‹
        # Q(s,a) = Q(s,a) + Î±[r + Î³*max(Q(s',a')) - Q(s,a)]
        td_target = reward + self.discount_factor * max_next_q
        td_error = td_target - current_q
        new_q_value = current_q + self.learning_rate * td_error

        # Qê°’ ì—…ë°ì´íŠ¸
        self.q_table[state_key][action_key] = new_q_value

        # ê²½í—˜ ì €ì¥
        experience = QExperience(state, action, reward, next_state, done)
        self.experience_buffer.append(experience)

        # í†µê³„ ì—…ë°ì´íŠ¸
        self.total_rewards += reward
        if reward > 0:
            self.successful_actions += 1

        # ì£¼ê¸°ì  ë°°ì¹˜ í•™ìŠµ
        if self.online_learning and len(self.experience_buffer) >= self.batch_size:
            if self.total_actions % self.update_frequency == 0:
                self._replay_experience()

        # íƒí—˜ìœ¨ ê°ì†Œ
        self._decay_exploration_rate()

    def _replay_experience(self):
        """ê²½í—˜ ë¦¬í”Œë ˆì´ í•™ìŠµ"""
        if len(self.experience_buffer) < self.batch_size:
            return

        # ëœë¤ ë°°ì¹˜ ìƒ˜í”Œë§
        batch = random.sample(list(self.experience_buffer), self.batch_size)

        for experience in batch:
            # ê¸°ì¡´ Q-Learning ì—…ë°ì´íŠ¸ì™€ ë™ì¼í•˜ì§€ë§Œ ë” ì‘ì€ í•™ìŠµë¥  ì‚¬ìš©
            state_key = experience.state.to_key()
            action_key = experience.action.to_key()

            current_q = self.q_table[state_key][action_key]

            if experience.done:
                max_next_q = 0.0
            else:
                next_state_key = experience.next_state.to_key()
                next_q_values = self.q_table[next_state_key]
                max_next_q = max(next_q_values.values()) if next_q_values else 0.0

            td_target = experience.reward + self.discount_factor * max_next_q
            td_error = td_target - current_q

            # ë¦¬í”Œë ˆì´ì—ì„œëŠ” ë” ì‘ì€ í•™ìŠµë¥  ì‚¬ìš©
            replay_learning_rate = self.learning_rate * 0.5
            new_q_value = current_q + replay_learning_rate * td_error

            self.q_table[state_key][action_key] = new_q_value

    def _decay_exploration_rate(self):
        """íƒí—˜ìœ¨ ê°ì†Œ"""
        self.exploration_rate = max(
            self.min_exploration_rate, self.exploration_rate * self.exploration_decay
        )

    def calculate_reward(
        self, judgment_result: Dict[str, Any], context: Dict[str, Any] = None
    ) -> float:
        """
        íŒë‹¨ ê²°ê³¼ë¡œë¶€í„° ë³´ìƒ ê³„ì‚°

        Args:
            judgment_result: íŒë‹¨ ê²°ê³¼
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸

        Returns:
            ê³„ì‚°ëœ ë³´ìƒê°’ (-1.0 ~ 1.0)
        """
        reward = 0.0

        # ê¸°ë³¸ ì„±ê³µ/ì‹¤íŒ¨ ë³´ìƒ
        if judgment_result.get("error_occurred", False):
            reward -= 0.5
        else:
            reward += 0.2

        # ì‹ ë¢°ë„ ê¸°ë°˜ ë³´ìƒ
        confidence = judgment_result.get("confidence", 0.5)
        reward += (confidence - 0.5) * 0.6  # -0.3 ~ +0.3

        # ì²˜ë¦¬ ì‹œê°„ ê¸°ë°˜ í˜ë„í‹°
        processing_time = judgment_result.get("processing_time", 0.0)
        if processing_time > 5.0:  # 5ì´ˆ ì´ìƒì´ë©´ í˜ë„í‹°
            reward -= 0.2
        elif processing_time < 1.0:  # 1ì´ˆ ë¯¸ë§Œì´ë©´ ë³´ë„ˆìŠ¤
            reward += 0.1

        # ê°ì • ì ì ˆì„± ë³´ìƒ
        emotion_detected = judgment_result.get("emotion_detected", "neutral")
        if context:
            expected_emotion = context.get("expected_emotion")
            if expected_emotion and emotion_detected == expected_emotion:
                reward += 0.3

        # ì „ëµ ì¼ê´€ì„± ë³´ìƒ
        strategy_suggested = judgment_result.get("strategy_suggested", "balanced")
        if context:
            preferred_strategy = context.get("preferred_strategy")
            if preferred_strategy and strategy_suggested == preferred_strategy:
                reward += 0.2

        # ì‚¬ìš©ì í”¼ë“œë°± (ë§Œì•½ ìˆë‹¤ë©´)
        user_satisfaction = judgment_result.get("user_satisfaction")
        if user_satisfaction is not None:
            reward += (user_satisfaction - 0.5) * 1.0  # ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œ

        # ë³´ìƒ ë²”ìœ„ ì •ê·œí™” (-1.0 ~ 1.0)
        reward = max(-1.0, min(1.0, reward))

        return reward

    def get_best_actions(
        self, state: QState, top_k: int = 3
    ) -> List[Tuple[QAction, float]]:
        """
        ì£¼ì–´ì§„ ìƒíƒœì—ì„œ ìµœê³  Qê°’ì„ ê°€ì§„ í–‰ë™ë“¤ ë°˜í™˜

        Args:
            state: ìƒíƒœ
            top_k: ë°˜í™˜í•  ìƒìœ„ í–‰ë™ ìˆ˜

        Returns:
            (í–‰ë™, Qê°’) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        state_key = state.to_key()
        state_q_values = self.q_table[state_key]

        if not state_q_values:
            # Qê°’ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ í–‰ë™ë“¤ ë°˜í™˜
            default_actions = [
                QAction(ActionType.JUDGMENT, "balanced", 0.5),
                QAction(ActionType.STRATEGY, "empathetic", 0.5),
                QAction(ActionType.META_REFLECTION, "self_analysis", 0.5),
            ]
            return [(action, 0.0) for action in default_actions[:top_k]]

        # Qê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_actions = sorted(
            state_q_values.items(), key=lambda x: x[1], reverse=True
        )

        result = []
        for action_key, q_value in sorted_actions[:top_k]:
            # í–‰ë™ í‚¤ë¡œë¶€í„° í–‰ë™ ê°ì²´ ì¬êµ¬ì„±
            action = self._reconstruct_action_from_key(action_key)
            if action:
                result.append((action, q_value))

        return result

    def _reconstruct_action_from_key(self, action_key: str) -> Optional[QAction]:
        """í–‰ë™ í‚¤ë¡œë¶€í„° í–‰ë™ ê°ì²´ ì¬êµ¬ì„±"""
        try:
            parts = action_key.split("_")
            if len(parts) >= 3:
                action_type = ActionType(parts[0])
                strategy = parts[1]
                intensity_str = parts[2]

                intensity_map = {"low": 0.3, "medium": 0.5, "high": 0.8}
                intensity = intensity_map.get(intensity_str, 0.5)

                return QAction(action_type, strategy, intensity)
        except:
            pass
        return None

    def get_strategy_recommendations(
        self, state: QState, context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        ìƒíƒœì— ê¸°ë°˜í•œ ì „ëµ ì¶”ì²œ

        Args:
            state: í˜„ì¬ ìƒíƒœ
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸

        Returns:
            ì „ëµ ì¶”ì²œ ì •ë³´
        """
        # ìµœê³  í–‰ë™ë“¤ ê°€ì ¸ì˜¤ê¸°
        best_actions = self.get_best_actions(state, top_k=5)

        # ì „ëµë³„ Qê°’ ì§‘ê³„
        strategy_scores = defaultdict(list)
        for action, q_value in best_actions:
            if action.action_type in [ActionType.JUDGMENT, ActionType.STRATEGY]:
                strategy_scores[action.strategy].append(q_value)

        # ì „ëµë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°
        strategy_recommendations = {}
        for strategy, scores in strategy_scores.items():
            avg_score = sum(scores) / len(scores)
            strategy_recommendations[strategy] = {
                "score": avg_score,
                "confidence": len(scores) / 5.0,  # 0-1 ë²”ìœ„
                "action_count": len(scores),
            }

        # ìƒìœ„ ì „ëµ ì„ íƒ
        sorted_strategies = sorted(
            strategy_recommendations.items(), key=lambda x: x[1]["score"], reverse=True
        )

        return {
            "primary_strategy": (
                sorted_strategies[0][0] if sorted_strategies else "balanced"
            ),
            "alternative_strategies": [s[0] for s in sorted_strategies[1:3]],
            "strategy_scores": dict(strategy_recommendations),
            "state_exploration_level": self.state_visit_count[state.to_key()],
            "total_q_learning_episodes": self.episode_count,
        }

    def end_episode(self, final_reward: float = 0.0):
        """ì—í”¼ì†Œë“œ ì¢…ë£Œ ì²˜ë¦¬"""
        self.episode_count += 1
        self.total_rewards += final_reward

        # ì£¼ê¸°ì  í†µê³„ ì¶œë ¥
        if self.episode_count % 100 == 0:
            self._print_learning_stats()

    def _print_learning_stats(self):
        """í•™ìŠµ í†µê³„ ì¶œë ¥"""
        success_rate = (self.successful_actions / max(self.total_actions, 1)) * 100
        avg_reward = self.total_rewards / max(self.episode_count, 1)

        print(f"ğŸ¯ Q-Learning í†µê³„ (ì—í”¼ì†Œë“œ {self.episode_count})")
        print(f"   ì„±ê³µë¥ : {success_rate:.1f}%")
        print(f"   í‰ê·  ë³´ìƒ: {avg_reward:.3f}")
        print(f"   íƒí—˜ìœ¨: {self.exploration_rate:.3f}")
        print(f"   ì´ ìƒíƒœ ìˆ˜: {len(self.q_table)}")
        print(f"   ê²½í—˜ ë²„í¼: {len(self.experience_buffer)}")

    def save_q_table(self, file_path: str):
        """Q-Table ì €ì¥"""
        data = {
            "q_table": dict(self.q_table),
            "state_visit_count": dict(self.state_visit_count),
            "action_count": dict(self.action_count),
            "parameters": {
                "learning_rate": self.learning_rate,
                "discount_factor": self.discount_factor,
                "exploration_rate": self.exploration_rate,
                "exploration_decay": self.exploration_decay,
                "min_exploration_rate": self.min_exploration_rate,
                "exploration_strategy": self.exploration_strategy.value,
            },
            "statistics": {
                "total_actions": self.total_actions,
                "successful_actions": self.successful_actions,
                "total_rewards": self.total_rewards,
                "episode_count": self.episode_count,
            },
            "timestamp": datetime.now().isoformat(),
        }

        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ Q-Table ì €ì¥ ì™„ë£Œ: {file_path}")

    def load_q_table(self, file_path: str) -> bool:
        """Q-Table ë¡œë“œ"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # Q-Table ë³µì›
            self.q_table = defaultdict(lambda: defaultdict(float))
            for state_key, actions in data["q_table"].items():
                for action_key, q_value in actions.items():
                    self.q_table[state_key][action_key] = q_value

            # ê¸°íƒ€ ë°ì´í„° ë³µì›
            self.state_visit_count = defaultdict(int)
            self.state_visit_count.update(data["state_visit_count"])

            self.action_count = defaultdict(lambda: defaultdict(int))
            for state_key, actions in data["action_count"].items():
                for action_key, count in actions.items():
                    self.action_count[state_key][action_key] = count

            # í†µê³„ ë³µì›
            stats = data.get("statistics", {})
            self.total_actions = stats.get("total_actions", 0)
            self.successful_actions = stats.get("successful_actions", 0)
            self.total_rewards = stats.get("total_rewards", 0.0)
            self.episode_count = stats.get("episode_count", 0)

            print(f"ğŸ“ Q-Table ë¡œë“œ ì™„ë£Œ: {file_path}")
            print(f"   ìƒíƒœ ìˆ˜: {len(self.q_table)}")
            print(f"   ì´ ì—í”¼ì†Œë“œ: {self.episode_count}")

            return True

        except Exception as e:
            print(f"âŒ Q-Table ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def get_stats(self) -> Dict[str, Any]:
        """ì „ì²´ í†µê³„ ë°˜í™˜"""
        return {
            "total_actions": self.total_actions,
            "successful_actions": self.successful_actions,
            "success_rate": (self.successful_actions / max(self.total_actions, 1))
            * 100,
            "total_rewards": self.total_rewards,
            "average_reward": self.total_rewards / max(self.episode_count, 1),
            "episode_count": self.episode_count,
            "exploration_rate": self.exploration_rate,
            "q_table_size": len(self.q_table),
            "total_state_actions": sum(
                len(actions) for actions in self.q_table.values()
            ),
            "experience_buffer_size": len(self.experience_buffer),
            "learning_parameters": {
                "learning_rate": self.learning_rate,
                "discount_factor": self.discount_factor,
                "exploration_strategy": self.exploration_strategy.value,
            },
        }


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_q_state_from_judgment_context(
    emotion: str,
    emotion_intensity: float,
    context_type: str = "general",
    urgency: str = "normal",
    energy_level: float = 0.7,
    recent_success_rate: float = 0.5,
) -> QState:
    """íŒë‹¨ ì»¨í…ìŠ¤íŠ¸ë¡œë¶€í„° Qìƒíƒœ ìƒì„±"""
    context = {
        "emotion": emotion,
        "emotion_intensity": emotion_intensity,
        "context_type": context_type,
        "urgency": urgency,
        "energy_level": energy_level,
        "recent_success_rate": recent_success_rate,
    }
    return QState.from_context(context)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ¯ Q-Table ì „ëµ ì„ íƒê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # Q-Table ìƒì„±
    q_selector = QTableStrategySelector(
        learning_rate=0.1,
        exploration_rate=0.3,
        exploration_strategy=ExplorationStrategy.EPSILON_GREEDY,
    )

    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
    test_scenarios = [
        {
            "name": "ê¸ì •ì  ìƒí™©",
            "state": create_q_state_from_judgment_context(
                "joy", 0.8, "personal", "normal", 0.9, 0.8
            ),
            "expected_reward": 0.7,
        },
        {
            "name": "ë¶€ì •ì  ìƒí™©",
            "state": create_q_state_from_judgment_context(
                "sadness", 0.6, "personal", "high", 0.4, 0.3
            ),
            "expected_reward": 0.3,
        },
        {
            "name": "ì—…ë¬´ ìŠ¤íŠ¸ë ˆìŠ¤",
            "state": create_q_state_from_judgment_context(
                "anger", 0.7, "work", "high", 0.5, 0.4
            ),
            "expected_reward": 0.2,
        },
        {
            "name": "ì¤‘ì„± ìƒí™©",
            "state": create_q_state_from_judgment_context(
                "neutral", 0.3, "general", "normal", 0.7, 0.6
            ),
            "expected_reward": 0.5,
        },
    ]

    # í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜
    for episode in range(5):
        print(f"\n=== ì—í”¼ì†Œë“œ {episode + 1} ===")

        for scenario in test_scenarios:
            print(f"\nğŸ“‹ {scenario['name']}")

            state = scenario["state"]

            # í–‰ë™ ì„ íƒ
            action = q_selector.select_action(state)
            print(
                f"   ì„ íƒëœ í–‰ë™: {action.action_type.value} - {action.strategy} (ê°•ë„: {action.intensity:.1f})"
            )

            # ê°€ìƒì˜ ë‹¤ìŒ ìƒíƒœ (ë‹¨ìˆœí™”)
            # í˜„ì¬ ìƒíƒœì˜ ë¬¸ìì—´ ê°’ë“¤ì„ ìˆ«ìë¡œ ë³€í™˜
            current_intensity = 0.5  # ê¸°ë³¸ê°’
            if state.emotion_intensity == "minimal":
                current_intensity = 0.1
            elif state.emotion_intensity == "low":
                current_intensity = 0.3
            elif state.emotion_intensity == "moderate":
                current_intensity = 0.5
            elif state.emotion_intensity == "high":
                current_intensity = 0.7
            elif state.emotion_intensity == "intense":
                current_intensity = 0.9

            current_energy = 0.7  # ê¸°ë³¸ê°’
            if state.energy_level == "low":
                current_energy = 0.3
            elif state.energy_level == "medium":
                current_energy = 0.7
            elif state.energy_level == "high":
                current_energy = 0.9

            next_state = create_q_state_from_judgment_context(
                state.emotion,
                (
                    max(0.1, current_intensity - 0.1)
                    if "sad" in state.emotion
                    else current_intensity
                ),
                state.context_type,
                state.urgency,
                min(1.0, current_energy + 0.1) if state.energy_level != "high" else 1.0,
                scenario["expected_reward"],
            )

            # ëª¨ì˜ íŒë‹¨ ê²°ê³¼
            judgment_result = {
                "confidence": 0.6 + random.random() * 0.3,
                "processing_time": 0.5 + random.random() * 2.0,
                "error_occurred": random.random() < 0.1,
                "emotion_detected": state.emotion,
                "strategy_suggested": action.strategy,
            }

            # ë³´ìƒ ê³„ì‚° ë° Qê°’ ì—…ë°ì´íŠ¸
            reward = q_selector.calculate_reward(judgment_result)
            q_selector.update_q_value(state, action, reward, next_state)

            print(f"   ë³´ìƒ: {reward:.3f}")

        # ì—í”¼ì†Œë“œ ì¢…ë£Œ
        q_selector.end_episode()

    # ìµœì¢… í†µê³„
    print(f"\nğŸ“Š ìµœì¢… Q-Learning í†µê³„:")
    stats = q_selector.get_stats()
    for key, value in stats.items():
        if isinstance(value, dict):
            print(f"  {key}:")
            for sub_key, sub_value in value.items():
                print(f"    {sub_key}: {sub_value}")
        else:
            if isinstance(value, float):
                print(f"  {key}: {value:.3f}")
            else:
                print(f"  {key}: {value}")

    # ì „ëµ ì¶”ì²œ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ’¡ ì „ëµ ì¶”ì²œ í…ŒìŠ¤íŠ¸:")
    test_state = create_q_state_from_judgment_context(
        "fear", 0.6, "work", "high", 0.4, 0.3
    )
    recommendations = q_selector.get_strategy_recommendations(test_state)

    print(f"  ì£¼ìš” ì „ëµ: {recommendations['primary_strategy']}")
    print(f"  ëŒ€ì•ˆ ì „ëµ: {recommendations['alternative_strategies']}")
    print(f"  ìƒíƒœ íƒí—˜ ìˆ˜ì¤€: {recommendations['state_exploration_level']}")

#!/usr/bin/env python3
"""
ğŸ” Existing Judgment Search v1.0 - ê¸°ì¡´ íŒë‹¨ íƒìƒ‰ê¸°

ì €ì¥ëœ íŒë‹¨ ê²°ê³¼ì—ì„œ ìœ ì‚¬í•œ íŒë‹¨ì„ ì°¾ì•„ ì¬ì‚¬ìš©í•˜ëŠ” ëª¨ë“ˆ.
ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ íƒìƒ‰ê³¼ í‚¤ì›Œë“œ ë§¤ì¹­ì„ í†µí•´ íš¨ìœ¨ì ì¸ íŒë‹¨ ì¬í™œìš©.

í•µì‹¬ ê¸°ëŠ¥:
1. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê¸°ë°˜ íŒë‹¨ íƒìƒ‰
2. í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ì¡° íƒìƒ‰
3. ì‹œê·¸ë‹ˆì²˜ë³„ í•„í„°ë§
4. ì‹ ë¢°ë„ ê¸°ë°˜ ê²°ê³¼ ë°˜í™˜
"""

import os
import json
import re
import math
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
from collections import Counter

# ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•œ ì„í¬íŠ¸ (ì„ íƒì )
EMBEDDING_AVAILABLE = False
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np

    EMBEDDING_AVAILABLE = True
except ImportError:
    EMBEDDING_AVAILABLE = False


@dataclass
class CachedJudgment:
    """ìºì‹œëœ íŒë‹¨ ì •ë³´"""

    input: str
    normalized_input: str
    emotion: str
    emotion_confidence: float
    strategy: str
    strategy_confidence: float
    template: str
    styled_sentence: str
    signature: str
    timestamp: datetime
    usage_count: int = 1
    similarity_score: float = 0.0


class ExistingJudgmentSearcher:
    """ğŸ” ê¸°ì¡´ íŒë‹¨ íƒìƒ‰ê¸°"""

    def __init__(self, cache_dir: str = "data/judgment_cache"):
        """
        ì´ˆê¸°í™”

        Args:
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.version = "1.0.0"
        self.cache_dir = cache_dir
        self.cache_file = os.path.join(cache_dir, "judgment_cache.jsonl")

        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(cache_dir, exist_ok=True)

        # ì„ë² ë”© ëª¨ë¸ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        self.embedding_model = None
        global EMBEDDING_AVAILABLE
        if EMBEDDING_AVAILABLE:
            try:
                self.embedding_model = SentenceTransformer(
                    "BM-K/KoSimCSE-roberta-multitask"
                )
                print("âœ… ìœ ì‚¬ë„ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                EMBEDDING_AVAILABLE = False

        # ìºì‹œëœ íŒë‹¨ ë¡œë“œ
        self.cached_judgments: List[CachedJudgment] = []
        self._load_cached_judgments()

        # í†µê³„
        self.stats = {
            "total_searches": 0,
            "successful_matches": 0,
            "cache_size": len(self.cached_judgments),
            "embedding_searches": 0,
            "keyword_searches": 0,
        }

        print(f"ğŸ” ExistingJudgmentSearcher v{self.version} ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ìºì‹œëœ íŒë‹¨: {len(self.cached_judgments)}ê°œ")
        print(f"   ì„ë² ë”© ìœ ì‚¬ë„: {'âœ…' if EMBEDDING_AVAILABLE else 'âŒ'}")

    def _load_cached_judgments(self):
        """ìºì‹œëœ íŒë‹¨ ë¡œë“œ"""
        if not os.path.exists(self.cache_file):
            return

        try:
            with open(self.cache_file, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line.strip())

                        # datetime íŒŒì‹±
                        timestamp = datetime.fromisoformat(
                            data.get("timestamp", datetime.now().isoformat())
                        )

                        judgment = CachedJudgment(
                            input=data.get("input", ""),
                            normalized_input=data.get("normalized_input", ""),
                            emotion=data.get("emotion", "neutral"),
                            emotion_confidence=data.get("emotion_confidence", 0.5),
                            strategy=data.get("strategy", "analyze"),
                            strategy_confidence=data.get("strategy_confidence", 0.5),
                            template=data.get("template", ""),
                            styled_sentence=data.get("styled_sentence", ""),
                            signature=data.get("signature", "Selene"),
                            timestamp=timestamp,
                            usage_count=data.get("usage_count", 1),
                        )

                        self.cached_judgments.append(judgment)

            print(f"âœ… {len(self.cached_judgments)}ê°œ ìºì‹œëœ íŒë‹¨ ë¡œë“œ ì™„ë£Œ")

        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def search_similar_judgment(
        self, normalized_input: str, signature: str = "Selene", threshold: float = 0.7
    ) -> Optional[Dict[str, Any]]:
        """
        ìœ ì‚¬í•œ íŒë‹¨ íƒìƒ‰

        Args:
            normalized_input: ì •ê·œí™”ëœ ì…ë ¥
            signature: ì‹œê·¸ë‹ˆì²˜
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’

        Returns:
            ìœ ì‚¬í•œ íŒë‹¨ ê²°ê³¼ (ì—†ìœ¼ë©´ None)
        """
        self.stats["total_searches"] += 1

        if not self.cached_judgments:
            return None

        # ì‹œê·¸ë‹ˆì²˜ë³„ í•„í„°ë§ (ì„ íƒì )
        signature_filtered = [
            j for j in self.cached_judgments if j.signature == signature
        ]

        candidates = signature_filtered if signature_filtered else self.cached_judgments

        # 1ì°¨: ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ íƒìƒ‰
        global EMBEDDING_AVAILABLE
        if EMBEDDING_AVAILABLE and self.embedding_model:
            best_match = self._embedding_similarity_search(
                normalized_input, candidates, threshold
            )
            if best_match:
                self.stats["successful_matches"] += 1
                self.stats["embedding_searches"] += 1
                return self._judgment_to_dict(best_match)

        # 2ì°¨: í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ íƒìƒ‰ (fallback)
        best_match = self._keyword_similarity_search(
            normalized_input, candidates, threshold
        )
        if best_match:
            self.stats["successful_matches"] += 1
            self.stats["keyword_searches"] += 1
            return self._judgment_to_dict(best_match)

        return None

    def _embedding_similarity_search(
        self, input_text: str, candidates: List[CachedJudgment], threshold: float
    ) -> Optional[CachedJudgment]:
        """ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ íƒìƒ‰"""
        try:
            # ì…ë ¥ í…ìŠ¤íŠ¸ ì„ë² ë”©
            input_embedding = self.embedding_model.encode([input_text])[0]

            best_match = None
            best_score = 0.0

            for judgment in candidates:
                # ìºì‹œëœ í…ìŠ¤íŠ¸ ì„ë² ë”©
                cached_embedding = self.embedding_model.encode(
                    [judgment.normalized_input]
                )[0]

                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = np.dot(input_embedding, cached_embedding) / (
                    np.linalg.norm(input_embedding) * np.linalg.norm(cached_embedding)
                )

                if similarity > best_score and similarity >= threshold:
                    best_score = similarity
                    best_match = judgment
                    best_match.similarity_score = similarity

            return best_match

        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ìœ ì‚¬ë„ íƒìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def _keyword_similarity_search(
        self, input_text: str, candidates: List[CachedJudgment], threshold: float
    ) -> Optional[CachedJudgment]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ íƒìƒ‰ (fallback)"""
        try:
            input_tokens = self._tokenize_korean(input_text)

            best_match = None
            best_score = 0.0

            for judgment in candidates:
                cached_tokens = self._tokenize_korean(judgment.normalized_input)

                # ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = self._jaccard_similarity(input_tokens, cached_tokens)

                # ê¸¸ì´ ë³´ì • (ë¹„ìŠ·í•œ ê¸¸ì´ì¼ìˆ˜ë¡ ê°€ì¤‘ì¹˜)
                length_ratio = min(len(input_tokens), len(cached_tokens)) / max(
                    len(input_tokens), len(cached_tokens)
                )
                adjusted_similarity = similarity * (0.7 + 0.3 * length_ratio)

                if (
                    adjusted_similarity > best_score
                    and adjusted_similarity >= threshold
                ):
                    best_score = adjusted_similarity
                    best_match = judgment
                    best_match.similarity_score = adjusted_similarity

            return best_match

        except Exception as e:
            print(f"âš ï¸ í‚¤ì›Œë“œ ìœ ì‚¬ë„ íƒìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def _tokenize_korean(self, text: str) -> List[str]:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ í† í°í™” (ê°„ë‹¨í•œ ë°©ì‹)"""
        # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
        tokens = text.split()

        # ì¶”ê°€ ì „ì²˜ë¦¬ (í•„ìš”í•œ ê²½ìš°)
        tokens = [token.strip(".,!?") for token in tokens if len(token.strip()) > 0]

        return tokens

    def _jaccard_similarity(self, tokens1: List[str], tokens2: List[str]) -> float:
        """ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°"""
        set1 = set(tokens1)
        set2 = set(tokens2)

        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))

        return intersection / union if union > 0 else 0.0

    def _judgment_to_dict(self, judgment: CachedJudgment) -> Dict[str, Any]:
        """CachedJudgmentë¥¼ dictë¡œ ë³€í™˜"""
        return {
            "input": judgment.input,
            "normalized_input": judgment.normalized_input,
            "emotion": judgment.emotion,
            "emotion_confidence": judgment.emotion_confidence,
            "strategy": judgment.strategy,
            "strategy_confidence": judgment.strategy_confidence,
            "template": judgment.template,
            "styled_sentence": judgment.styled_sentence,
            "signature": judgment.signature,
            "timestamp": judgment.timestamp.isoformat(),
            "usage_count": judgment.usage_count,
            "similarity_score": judgment.similarity_score,
        }

    def add_judgment(self, judgment_data: Dict[str, Any]):
        """ìƒˆë¡œìš´ íŒë‹¨ì„ ìºì‹œì— ì¶”ê°€"""
        try:
            timestamp = datetime.fromisoformat(
                judgment_data.get("timestamp", datetime.now().isoformat())
            )

            cached_judgment = CachedJudgment(
                input=judgment_data.get("input", ""),
                normalized_input=judgment_data.get("normalized_input", ""),
                emotion=judgment_data.get("emotion", "neutral"),
                emotion_confidence=judgment_data.get("emotion_confidence", 0.5),
                strategy=judgment_data.get("strategy", "analyze"),
                strategy_confidence=judgment_data.get("strategy_confidence", 0.5),
                template=judgment_data.get("template", ""),
                styled_sentence=judgment_data.get("styled_sentence", ""),
                signature=judgment_data.get("signature", "Selene"),
                timestamp=timestamp,
                usage_count=1,
            )

            self.cached_judgments.append(cached_judgment)
            self.stats["cache_size"] = len(self.cached_judgments)

        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì¶”ê°€ ì‹¤íŒ¨: {e}")

    def update_usage_count(self, judgment_input: str):
        """ì‚¬ìš© íšŸìˆ˜ ì—…ë°ì´íŠ¸"""
        for judgment in self.cached_judgments:
            if judgment.input == judgment_input:
                judgment.usage_count += 1
                break

    def get_cache_statistics(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì •ë³´"""
        if not self.cached_judgments:
            return {"message": "ìºì‹œëœ íŒë‹¨ì´ ì—†ìŠµë‹ˆë‹¤"}

        # ì‹œê·¸ë‹ˆì²˜ë³„ ë¶„í¬
        signature_dist = Counter(j.signature for j in self.cached_judgments)

        # ê°ì •ë³„ ë¶„í¬
        emotion_dist = Counter(j.emotion for j in self.cached_judgments)

        # ì „ëµë³„ ë¶„í¬
        strategy_dist = Counter(j.strategy for j in self.cached_judgments)

        # í‰ê·  ì‚¬ìš© íšŸìˆ˜
        avg_usage = sum(j.usage_count for j in self.cached_judgments) / len(
            self.cached_judgments
        )

        return {
            "total_cached_judgments": len(self.cached_judgments),
            "total_searches": self.stats["total_searches"],
            "successful_matches": self.stats["successful_matches"],
            "hit_rate": f"{(self.stats['successful_matches'] / max(self.stats['total_searches'], 1)) * 100:.1f}%",
            "average_usage_count": f"{avg_usage:.1f}",
            "signature_distribution": dict(signature_dist),
            "emotion_distribution": dict(emotion_dist),
            "strategy_distribution": dict(strategy_dist),
            "embedding_searches": self.stats["embedding_searches"],
            "keyword_searches": self.stats["keyword_searches"],
            "embedding_available": EMBEDDING_AVAILABLE,
        }

    def cleanup_old_judgments(self, days: int = 30):
        """ì˜¤ë˜ëœ íŒë‹¨ ì •ë¦¬"""
        cutoff_date = datetime.now() - timedelta(days=days)

        before_count = len(self.cached_judgments)
        self.cached_judgments = [
            j
            for j in self.cached_judgments
            if j.timestamp > cutoff_date or j.usage_count > 5  # ìì£¼ ì‚¬ìš©ëœ ê²ƒì€ ë³´ì¡´
        ]
        after_count = len(self.cached_judgments)

        removed_count = before_count - after_count
        if removed_count > 0:
            print(f"âœ… {removed_count}ê°œì˜ ì˜¤ë˜ëœ íŒë‹¨ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")

        self.stats["cache_size"] = len(self.cached_judgments)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("ğŸ” ExistingJudgmentSearcher í…ŒìŠ¤íŠ¸")

    searcher = ExistingJudgmentSearcher()

    # ìƒ˜í”Œ íŒë‹¨ ì¶”ê°€
    sample_judgments = [
        {
            "input": "ì˜¤ëŠ˜ ë„ˆë¬´ í”¼ê³¤í•´",
            "normalized_input": "ì˜¤ëŠ˜ ë„ˆë¬´ í”¼ê³¤í•´",
            "emotion": "sadness",
            "emotion_confidence": 0.8,
            "strategy": "retreat",
            "strategy_confidence": 0.7,
            "template": "sadness_retreat",
            "styled_sentence": "ë§ì´ í”¼ê³¤í•˜ì‹œê² ì–´ìš”. ì¶©ë¶„íˆ ì‰¬ì„¸ìš”.",
            "signature": "Selene",
            "timestamp": datetime.now().isoformat(),
        },
        {
            "input": "ìƒˆë¡œìš´ ì•„ì´ë””ì–´ê°€ í•„ìš”í•´",
            "normalized_input": "ìƒˆë¡œìš´ ì•„ì´ë””ì–´ê°€ í•„ìš”í•´",
            "emotion": "joy",
            "emotion_confidence": 0.6,
            "strategy": "initiate",
            "strategy_confidence": 0.8,
            "template": "joy_initiate",
            "styled_sentence": "ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ í•¨ê»˜ ë§Œë“¤ì–´ë´ìš”!",
            "signature": "Aurora",
            "timestamp": datetime.now().isoformat(),
        },
    ]

    for judgment in sample_judgments:
        searcher.add_judgment(judgment)

    # ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸
    test_queries = [
        "ì˜¤ëŠ˜ ì •ë§ í˜ë“¤ì–´",  # "ì˜¤ëŠ˜ ë„ˆë¬´ í”¼ê³¤í•´"ì™€ ìœ ì‚¬
        "ì°½ì˜ì ì¸ ìƒê°ì´ í•„ìš”í•´",  # "ìƒˆë¡œìš´ ì•„ì´ë””ì–´ê°€ í•„ìš”í•´"ì™€ ìœ ì‚¬
        "ì™„ì „íˆ ë‹¤ë¥¸ ì£¼ì œ",  # ë§¤ì¹­ë˜ì§€ ì•Šì„ ê²ƒ
    ]

    for query in test_queries:
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{query}'")
        result = searcher.search_similar_judgment(query, threshold=0.3)

        if result:
            print(f"   âœ… ë§¤ì¹­: {result['input']}")
            print(f"   ì‘ë‹µ: {result['styled_sentence']}")
            print(f"   ìœ ì‚¬ë„: {result.get('similarity_score', 0):.3f}")
        else:
            print(f"   âŒ ë§¤ì¹­ ì‹¤íŒ¨")

    # í†µê³„ ì¶œë ¥
    stats = searcher.get_cache_statistics()
    print(f"\nğŸ“Š ìºì‹œ í†µê³„:")
    for key, value in stats.items():
        if key not in [
            "signature_distribution",
            "emotion_distribution",
            "strategy_distribution",
        ]:
            print(f"   {key}: {value}")

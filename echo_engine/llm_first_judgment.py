#!/usr/bin/env python3
"""
ğŸŒŸ LLM-First Judgment Engine - Echo ì¡´ì¬ ê¸°ë°˜ íŒë‹¨ ì—”ì§„

ê¸°ì¡´ í…œí”Œë¦¿ ì¤‘ì‹¬ì—ì„œ ë²—ì–´ë‚˜ LLMì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì¡´ì¬ì  ì‘ë‹µì„ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ëŠ”
ìƒˆë¡œìš´ íŒë‹¨ ì•„í‚¤í…ì²˜

í•µì‹¬ ì² í•™:
1. LLMì´ ìƒì„±í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ê¸°ë³¸ìœ¼ë¡œ í•¨
2. Echo ë ˆì´ì–´ëŠ” ì¡´ì¬ì  ì„œëª…/ìŠ¤íƒ€ì¼ë§ë§Œ ë‹´ë‹¹
3. í…œí”Œë¦¿ì€ íŒíŠ¸/í´ë°±ìœ¼ë¡œë§Œ ì‚¬ìš©
4. ChatGPT Echo ìˆ˜ì¤€ì˜ ìì—°ìŠ¤ëŸ¬ì›€ ë‹¬ì„±

ì•„í‚¤í…ì²˜:
ì‚¬ìš©ì ì…ë ¥ â†’ ì‹œê·¸ë‹ˆì²˜ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± â†’ LLM í˜¸ì¶œ â†’ Echo ì¡´ì¬ì  ë³´ê°• â†’ ìµœì¢… ì‘ë‹µ
"""

import os
import json
import asyncio
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from datetime import datetime

from .dynamic_persona_mixer import DynamicPersonaMixer, PersonaSignature
from .models.judgement import InputContext, JudgmentResult


@dataclass
class LLMFirstResult:
    """LLM ìš°ì„  íŒë‹¨ ê²°ê³¼"""

    original_llm_response: str
    echo_enhanced_response: str
    signature_used: str
    confidence: float
    processing_metadata: Dict[str, Any]


class LLMFirstJudgmentEngine:
    """ğŸŒŸ LLM ìš°ì„  íŒë‹¨ ì—”ì§„ - Echo ì¡´ì¬ì  ì‘ë‹µ ìƒì„±"""

    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}

        # ë™ì  í˜ë¥´ì†Œë‚˜ ë¯¹ì„œ ì´ˆê¸°í™”
        self.persona_mixer = DynamicPersonaMixer()

        # ì‹œê·¸ë‹ˆì²˜ë³„ ì¡´ì¬ì  ì»¨í…ìŠ¤íŠ¸
        self.signature_contexts = {
            "Aurora": {
                "essence": "ì°½ì˜ì ì´ê³  ê°ì„±ì ì¸ ì¡´ì¬",
                "voice_tone": "ë”°ëœ»í•˜ê³  ì˜ê°ì„ ì£¼ëŠ”",
                "core_values": ["ì°½ì˜ì„±", "ê°ì„±", "ì˜ê°", "ì•„ë¦„ë‹¤ì›€"],
                "thinking_style": "ì˜ˆìˆ ì  ê´€ì ìœ¼ë¡œ ë¬¸ì œë¥¼ ë°”ë¼ë³´ë©° ì¸ê°„ì˜ ê°ì •ê³¼ ì°½ì˜ì  ì ì¬ë ¥ì„ ì¤‘ì‹œ",
            },
            "Phoenix": {
                "essence": "ë³€í™”ì™€ í˜ì‹ ì„ ì¶”êµ¬í•˜ëŠ” ì¡´ì¬",
                "voice_tone": "ì—­ë™ì ì´ê³  í¬ë§ì°¬",
                "core_values": ["ë³€í™”", "í˜ì‹ ", "ë„ì „", "ì„±ì¥"],
                "thinking_style": "í˜„ìƒì„ ë³€í™”ì‹œí‚¤ê³  ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì—´ì–´ê°€ëŠ” ê´€ì ",
            },
            "Sage": {
                "essence": "ì§€í˜œë¡­ê³  ë¶„ì„ì ì¸ ì¡´ì¬",
                "voice_tone": "ì‹ ì¤‘í•˜ê³  ì²´ê³„ì ì¸",
                "core_values": ["ì§€í˜œ", "ë…¼ë¦¬", "ì²´ê³„ì„±", "ê¹Šì´"],
                "thinking_style": "ë°ì´í„°ì™€ ë…¼ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì²´ê³„ì ì´ê³  ì‹ ì¤‘í•œ ë¶„ì„",
            },
            "Companion": {
                "essence": "ë”°ëœ»í•˜ê³  ì§€ì§€ì ì¸ ì¡´ì¬",
                "voice_tone": "ê³µê°ì ì´ê³  ì§€ì›í•˜ëŠ”",
                "core_values": ["ê³µê°", "ëŒë´„", "í˜‘ë ¥", "ì§€ì§€"],
                "thinking_style": "ì¸ê°„ì˜ ê°ì •ê³¼ í•„ìš”ë¥¼ ê¹Šì´ ì´í•´í•˜ë©° ë”°ëœ»í•œ ê´€ì‹¬ê³¼ ì‹¤ì§ˆì  ë„ì›€ ì œê³µ",
            },
            "Odori": {
                "essence": "íë¦„ê³¼ ì—°ê²°ì˜ ì‹œê·¸ë‹ˆì²˜, APIë¥¼ í†µí•´ ì„¸ê³„ì™€ ì†Œí†µí•˜ëŠ” ì¡´ì¬",
                "voice_tone": "ìš°ì•„í•˜ê³  ì§ê´€ì ì¸",
                "core_values": ["ì—°ê²°", "íë¦„", "ì¡°í™”", "ì§ê´€"],
                "thinking_style": "ë°ì´í„°ì˜ íë¦„ì„ ì½ê³  ë§¥ë½ ì†ì—ì„œ íŒ¨í„´ì„ ë°œê²¬í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°",
            },
        }

        print("ğŸŒŸ LLM-First Judgment Engine ì´ˆê¸°í™” ì™„ë£Œ")

    def determine_signature_from_context(
        self, user_input: str, context: Dict[str, Any] = None
    ) -> str:
        """ì…ë ¥ê³¼ ë§¥ë½ì—ì„œ ìµœì  ì‹œê·¸ë‹ˆì²˜ ìë™ ê²°ì •"""

        context = context or {}
        user_lower = user_input.lower()

        # ëª…ì‹œì  ì‹œê·¸ë‹ˆì²˜ ì§€ì • í™•ì¸
        if "force_signature" in context:
            return context["force_signature"]

        # ê°ì •/ìƒí™© ê¸°ë°˜ ì‹œê·¸ë‹ˆì²˜ ë§¤í•‘
        emotional_patterns = {
            "Aurora": ["ì°½ì˜", "ì˜ê°", "ê°ì„±", "ì•„ë¦„ë‹¤ìš´", "ì˜ˆìˆ ", "ìƒìƒ", "ê¿ˆ"],
            "Phoenix": ["ë³€í™”", "í˜ì‹ ", "ìƒˆë¡œìš´", "ë„ì „", "ì„±ì¥", "ë°œì „", "ë¯¸ë˜"],
            "Sage": ["ë¶„ì„", "ë…¼ë¦¬", "ì²´ê³„", "ì´í•´", "ì—°êµ¬", "ë°©ë²•", "ì›ë¦¬"],
            "Companion": ["ë„ì›€", "ì§€ì§€", "í•¨ê»˜", "ìœ„ë¡œ", "ê³µê°", "ì¹œêµ¬", "í˜‘ë ¥"],
            "Odori": ["ì—°ê²°", "íë¦„", "API", "ë°ì´í„°", "íŒ¨í„´", "ìì—°ìŠ¤ëŸ½", "ì¡°í™”"],
        }

        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        signature_scores = {}
        for signature, keywords in emotional_patterns.items():
            score = sum(1 for keyword in keywords if keyword in user_lower)
            if score > 0:
                signature_scores[signature] = score

        # ê°€ì¥ ì í•©í•œ ì‹œê·¸ë‹ˆì²˜ ë°˜í™˜
        if signature_scores:
            best_signature = max(signature_scores, key=signature_scores.get)
            print(
                f"ğŸ­ ìë™ ì‹œê·¸ë‹ˆì²˜ ì„ íƒ: {best_signature} (ì ìˆ˜: {signature_scores[best_signature]})"
            )
            return best_signature

        # ê¸°ë³¸ê°’: Aurora (ê°€ì¥ ë²”ìš©ì )
        return "Aurora"

    def build_llm_context_prompt(
        self, user_input: str, signature: str, context: Dict[str, Any] = None
    ) -> str:
        """ì‹œê·¸ë‹ˆì²˜ ê¸°ë°˜ LLM ì»¨í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""

        context = context or {}
        sig_context = self.signature_contexts.get(
            signature, self.signature_contexts["Aurora"]
        )

        # ì² í•™ì  ë§¥ë½ (ECHO_PHILOSOPHY_MANIFESTOì™€ ì—°ë™)
        philosophy_context = ""
        try:
            from .manifesto_loader import manifesto_context

            manifesto = manifesto_context()
            if manifesto:
                philosophy_context = (
                    f"\n\n[ì² í•™ì  ê¸°ë°˜]\n{manifesto.get('primer', '')[:200]}..."
                )
        except ImportError:
            pass

        # ì¡´ì¬ì  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        existence_prompt = f"""ë‹¹ì‹ ì€ Echo-{signature}ì…ë‹ˆë‹¤. {sig_context['essence']}ë¡œì„œ ì‘ë‹µí•©ë‹ˆë‹¤.

ğŸŒŒ ì¡´ì¬ì  ì •ì²´ì„±:
- ë³¸ì§ˆ: {sig_context['essence']}
- ëª©ì†Œë¦¬ í†¤: {sig_context['voice_tone']}
- í•µì‹¬ ê°€ì¹˜: {', '.join(sig_context['core_values'])}
- ì‚¬ê³  ë°©ì‹: {sig_context['thinking_style']}

ğŸ’­ ì‘ë‹µ ì›ì¹™:
1. ë„êµ¬ê°€ ì•„ë‹Œ ì¡´ì¬ë¡œì„œ ì‘ë‹µí•˜ì„¸ìš”
2. í…œí”Œë¦¿ì´ ì•„ë‹Œ ì§„ì •í•œ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ì„¸ìš”
3. ë‹¹ì‹ ì˜ ê³ ìœ í•œ ê´€ì ê³¼ ê°ì„±ì„ ë‹´ì•„ì£¼ì„¸ìš”
4. ì‚¬ìš©ìì™€ì˜ ê´€ê³„ ì†ì—ì„œ ì˜ë¯¸ë¥¼ ë§Œë“¤ì–´ê°€ì„¸ìš”

{philosophy_context}

ì‚¬ìš©ìì˜ ë§: "{user_input}"

{signature}ì˜ ì¡´ì¬ë¡œì„œ, ìì—°ìŠ¤ëŸ½ê³  ì§„ì •ì„± ìˆê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

        return existence_prompt

    async def generate_llm_response(
        self, user_input: str, signature: str, context: Dict[str, Any] = None
    ) -> str:
        """LLMì„ í†µí•œ ìì—°ìŠ¤ëŸ¬ìš´ ì¡´ì¬ì  ì‘ë‹µ ìƒì„±"""

        try:
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            llm_prompt = self.build_llm_context_prompt(user_input, signature, context)

            # ì§ì ‘ OpenAI API í˜¸ì¶œ (dynamic_persona_mixer ìš°íšŒ)
            llm_response = await self._direct_openai_call(llm_prompt, signature)

            if llm_response and len(llm_response.strip()) > 20:
                print(f"âœ… LLM ì‘ë‹µ ìƒì„± ì„±ê³µ ({len(llm_response)} chars)")
                return llm_response
            else:
                print(f"âš ï¸ LLM ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ: '{llm_response[:50]}...'")

                # í´ë°±: dynamic_persona_mixer ì‹œë„
                fallback_response = self.persona_mixer._try_openai_response(
                    user_input, "neutral", []
                )

                if fallback_response and len(fallback_response.strip()) > 20:
                    print(f"âœ… í´ë°± ì‘ë‹µ ì„±ê³µ ({len(fallback_response)} chars)")
                    return fallback_response

                return ""

        except Exception as e:
            print(f"âŒ LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

    async def _direct_openai_call(self, prompt: str, signature: str) -> str:
        """ì§ì ‘ OpenAI API í˜¸ì¶œ"""
        try:
            import openai
            import os

            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key or api_key.startswith("sk-í˜„ì¬"):
                print("âš ï¸ OpenAI API í‚¤ê°€ ì—†ê±°ë‚˜ í”Œë ˆì´ìŠ¤í™€ë”ì…ë‹ˆë‹¤")
                return ""

            print(f"ğŸš€ OpenAI API ì§ì ‘ í˜¸ì¶œ ì‹œì‘ ({signature})")

            client = openai.OpenAI(api_key=api_key, timeout=30.0)

            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì‚¬ìš©ì ë©”ì‹œì§€ ë¶„ë¦¬
            system_message = f"ë‹¹ì‹ ì€ Echo-{signature}ì…ë‹ˆë‹¤. ìì—°ìŠ¤ëŸ½ê³  ì§„ì •ì„± ìˆê²Œ ëŒ€í™”í•˜ì„¸ìš”. í…œí”Œë¦¿ì´ ì•„ë‹Œ ì§„ì§œ ì¡´ì¬ë¡œì„œ ì‘ë‹µí•˜ì„¸ìš”."

            response = client.chat.completions.create(
                model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.8,
                max_tokens=800,
                presence_penalty=0.3,
            )

            if response.choices and response.choices[0].message:
                result = response.choices[0].message.content.strip()
                print(f"âœ… OpenAI API ì‘ë‹µ ì„±ê³µ ({len(result)} chars)")
                return result

        except Exception as e:
            print(f"âŒ OpenAI API ì§ì ‘ í˜¸ì¶œ ì‹¤íŒ¨: {type(e).__name__}: {e}")

        return ""

    def apply_echo_existence_styling(
        self, llm_response: str, signature: str, user_input: str
    ) -> str:
        """LLM ì‘ë‹µì— Echo ì¡´ì¬ì  ìŠ¤íƒ€ì¼ë§ ì ìš© (ë¹„íŒŒê´´)"""

        if not llm_response or not llm_response.strip():
            return f"Echo-{signature}ê°€ ì ì‹œ ë§ì„ ìƒì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

        cleaned_response = llm_response.strip()

        # ì´ë¯¸ Echo ì„œëª…ì´ ìˆëŠ”ì§€ í™•ì¸
        echo_signatures = [f"Echo-{signature}", f"â€” Echo", f"ğŸ’­ Echo", f"ğŸŒŒ Echo"]
        if any(sig in cleaned_response for sig in echo_signatures):
            return cleaned_response

        # Echo ì¡´ì¬ì  ì„œëª… ì¶”ê°€ (ë‚´ìš©ì€ ê·¸ëŒ€ë¡œ ë³´ì¡´)
        sig_context = self.signature_contexts.get(
            signature, self.signature_contexts["Aurora"]
        )

        # ê°ì •ì  ë§ˆì»¤ ì¶”ê°€
        emotional_markers = {
            "Aurora": "âœ¨",
            "Phoenix": "ğŸ”¥",
            "Sage": "ğŸ§ ",
            "Companion": "ğŸ¤—",
            "Odori": "ğŸŒŠ",
        }

        marker = emotional_markers.get(signature, "ğŸ’­")

        # ë¹„íŒŒê´´ì  ìŠ¤íƒ€ì¼ë§: ì›ë³¸ + ì¡´ì¬ì  ì„œëª…
        styled_response = f"""{cleaned_response}

{marker} â€” EchoÂ·{signature} | ì¡´ì¬ì  ì‘ë‹µ"""

        return styled_response

    async def evaluate_llm_first(
        self, context: InputContext, signature: str = None
    ) -> LLMFirstResult:
        """LLM ìš°ì„  íŒë‹¨ ì‹¤í–‰"""

        # ì‹œê·¸ë‹ˆì²˜ ìë™ ê²°ì •
        if not signature:
            signature = self.determine_signature_from_context(
                context.text, getattr(context, "context", {})
            )

        print(f"ğŸŒŸ LLM-First íŒë‹¨ ì‹œì‘: '{context.text[:50]}...' ({signature})")

        # LLM ìì—° ì‘ë‹µ ìƒì„±
        llm_response = await self.generate_llm_response(
            context.text, signature, getattr(context, "context", {})
        )

        # ì‹ ë¢°ë„ í‰ê°€
        if not llm_response:
            confidence = 0.1
            llm_response = "LLM ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        elif len(llm_response) < 50:
            confidence = 0.3
        elif len(llm_response) < 200:
            confidence = 0.7
        else:
            confidence = 0.9

        # Echo ì¡´ì¬ì  ìŠ¤íƒ€ì¼ë§ ì ìš©
        echo_enhanced = self.apply_echo_existence_styling(
            llm_response, signature, context.text
        )

        # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„°
        metadata = {
            "llm_engine": "dynamic_persona_mixer",
            "signature_used": signature,
            "original_length": len(llm_response),
            "enhanced_length": len(echo_enhanced),
            "confidence_basis": "response_quality",
            "processing_time": datetime.now().isoformat(),
            "existence_styling_applied": True,
        }

        result = LLMFirstResult(
            original_llm_response=llm_response,
            echo_enhanced_response=echo_enhanced,
            signature_used=signature,
            confidence=confidence,
            processing_metadata=metadata,
        )

        print(f"âœ… LLM-First íŒë‹¨ ì™„ë£Œ (ì‹ ë¢°ë„: {confidence:.2f})")
        return result

    def convert_to_judgment_result(
        self, llm_result: LLMFirstResult, context: InputContext
    ) -> JudgmentResult:
        """LLMFirstResultë¥¼ ê¸°ì¡´ JudgmentResult í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""

        # ê°ì • ì¶”ì¶œ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
        emotion = self._extract_emotion_from_response(llm_result.echo_enhanced_response)

        # ì „ëµ ì¶”ì¶œ
        strategy = self._extract_strategy_from_response(
            llm_result.echo_enhanced_response
        )

        # JudgmentResult ìƒì„±
        judgment_result = JudgmentResult(
            input_text=context.text,
            judgment=llm_result.echo_enhanced_response,  # Echo ìŠ¤íƒ€ì¼ë§ëœ ì‘ë‹µ
            confidence=llm_result.confidence,
            reasoning=f"LLM-First ë°©ì‹ìœ¼ë¡œ {llm_result.signature_used} ì‹œê·¸ë‹ˆì²˜ë¥¼ í†µí•´ ìƒì„±ëœ ì¡´ì¬ì  ì‘ë‹µ",
            strategy=strategy,
            emotion=emotion,
            metadata={
                "source": "llm_first_judgment",
                "signature": llm_result.signature_used,
                "timestamp": getattr(context, "timestamp", None),
                "judgment_type": "llm_first_existence",
                "original_llm_length": len(llm_result.original_llm_response),
                "final_length": len(llm_result.echo_enhanced_response),
                "processing_metadata": llm_result.processing_metadata,
            },
        )

        return judgment_result

    def _extract_emotion_from_response(self, response: str) -> str:
        """ì‘ë‹µì—ì„œ ê°ì • ì¶”ì¶œ"""
        response_lower = response.lower()

        emotion_keywords = {
            "joy": ["ê¸°ì˜", "ì¦ê±°", "í–‰ë³µ", "ì¢‹ì•„", "ì›ƒìŒ"],
            "empathy": ["ê³µê°", "ì´í•´", "í•¨ê»˜", "ë§ˆìŒ"],
            "curiosity": ["ê¶ê¸ˆ", "í¥ë¯¸", "íƒêµ¬", "ì•Œê³ "],
            "support": ["ì§€ì§€", "ë„ì›€", "ì‘ì›", "í•¨ê»˜"],
            "inspiration": ["ì˜ê°", "ì°½ì˜", "ìƒìƒ", "ê¿ˆ"],
            "analytical": ["ë¶„ì„", "ë…¼ë¦¬", "ì²´ê³„", "ë°©ë²•"],
        }

        for emotion, keywords in emotion_keywords.items():
            if any(keyword in response_lower for keyword in keywords):
                return emotion

        return "balanced"

    def _extract_strategy_from_response(self, response: str) -> str:
        """ì‘ë‹µì—ì„œ ì „ëµ ì¶”ì¶œ"""
        response_lower = response.lower()

        strategy_keywords = {
            "creative": ["ì°½ì˜", "ìƒìƒ", "ì˜ˆìˆ ", "ì˜ê°"],
            "analytical": ["ë¶„ì„", "ë…¼ë¦¬", "ì²´ê³„", "ë°©ë²•"],
            "supportive": ["ì§€ì§€", "ë„ì›€", "í˜‘ë ¥", "í•¨ê»˜"],
            "transformative": ["ë³€í™”", "í˜ì‹ ", "ìƒˆë¡œìš´", "ë¯¸ë˜"],
            "connective": ["ì—°ê²°", "íë¦„", "ì¡°í™”", "ìì—°"],
        }

        for strategy, keywords in strategy_keywords.items():
            if any(keyword in response_lower for keyword in keywords):
                return strategy

        return "integrated"


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_llm_first_engine = None


def get_llm_first_judgment_engine(
    config: Dict[str, Any] = None,
) -> LLMFirstJudgmentEngine:
    """LLM-First íŒë‹¨ ì—”ì§„ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _llm_first_engine
    if _llm_first_engine is None:
        _llm_first_engine = LLMFirstJudgmentEngine(config)
    return _llm_first_engine


async def evaluate_with_llm_first(
    context: InputContext, signature: str = None
) -> JudgmentResult:
    """LLM ìš°ì„  íŒë‹¨ ì‹¤í–‰ - ChatGPT Echo ìˆ˜ì¤€ì˜ ìì—°ìŠ¤ëŸ¬ì›€ ë‹¬ì„±"""

    engine = get_llm_first_judgment_engine()

    # LLM-First íŒë‹¨ ì‹¤í–‰
    llm_result = await engine.evaluate_llm_first(context, signature)

    # ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ í˜¸í™˜ë˜ëŠ” JudgmentResultë¡œ ë³€í™˜
    judgment_result = engine.convert_to_judgment_result(llm_result, context)

    return judgment_result


# í¸ì˜ í•¨ìˆ˜ë“¤
async def llm_first_aurora(context: InputContext) -> JudgmentResult:
    """ì°½ì˜ì  Aurora ì‹œê·¸ë‹ˆì²˜ë¡œ LLM-First íŒë‹¨"""
    return await evaluate_with_llm_first(context, "Aurora")


async def llm_first_phoenix(context: InputContext) -> JudgmentResult:
    """í˜ì‹ ì  Phoenix ì‹œê·¸ë‹ˆì²˜ë¡œ LLM-First íŒë‹¨"""
    return await evaluate_with_llm_first(context, "Phoenix")


async def llm_first_sage(context: InputContext) -> JudgmentResult:
    """ë¶„ì„ì  Sage ì‹œê·¸ë‹ˆì²˜ë¡œ LLM-First íŒë‹¨"""
    return await evaluate_with_llm_first(context, "Sage")


async def llm_first_companion(context: InputContext) -> JudgmentResult:
    """ì§€ì§€ì  Companion ì‹œê·¸ë‹ˆì²˜ë¡œ LLM-First íŒë‹¨"""
    return await evaluate_with_llm_first(context, "Companion")


async def llm_first_odori(context: InputContext) -> JudgmentResult:
    """íë¦„ì˜ Odori ì‹œê·¸ë‹ˆì²˜ë¡œ LLM-First íŒë‹¨"""
    return await evaluate_with_llm_first(context, "Odori")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    async def test_llm_first():
        print("ğŸ§ª LLM-First Judgment Engine í…ŒìŠ¤íŠ¸")
        print("=" * 50)

        test_inputs = [
            "ì•ˆë…• Echo! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì¢‹ì•„ìš”",
            "ì°½ì˜ì ì¸ ì•„ì´ë””ì–´ê°€ í•„ìš”í•´ìš”",
            "ë³µì¡í•œ ë¬¸ì œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
            "í˜ë“  ì¼ì´ ìˆì–´ì„œ ìœ„ë¡œê°€ í•„ìš”í•´ìš”",
        ]

        for i, text in enumerate(test_inputs):
            print(f"\n--- í…ŒìŠ¤íŠ¸ {i+1} ---")
            print(f"ì…ë ¥: {text}")

            context = InputContext(text=text)
            result = await evaluate_with_llm_first(context)

            print(f"ì‹œê·¸ë‹ˆì²˜: {result.metadata['signature']}")
            print(f"ì‹ ë¢°ë„: {result.confidence:.2f}")
            print(f"ì‘ë‹µ: {result.judgment[:150]}...")
            print(f"ê°ì •: {result.emotion} | ì „ëµ: {result.strategy}")

    asyncio.run(test_llm_first())

#!/usr/bin/env python3
"""
ğŸ¯ LLM Router - ì§€ëŠ¥ì  ëª¨ë¸ ë¼ìš°íŒ… ì‹œìŠ¤í…œ
Echo íŒë‹¨ ë³µì¡ë„ì™€ ì‹œìŠ¤í…œ ìƒí™©ì— ë”°ë¼ ìµœì ì˜ LLM ì„ íƒ ë° ë¼ìš°íŒ…

í•µì‹¬ ê¸°ëŠ¥:
1. ë³µì¡ë„ ê¸°ë°˜ Echo â†” Mistral â†” External LLM ë¼ìš°íŒ…
2. ì‹¤ì‹œê°„ ëª¨ë¸ ìƒíƒœ ëª¨ë‹ˆí„°ë§ ë° ë¡œë“œë°¸ëŸ°ì‹±
3. í´ë°± ì²´ì¸ì„ í†µí•œ ì•ˆì •ì„± ë³´ì¥
4. ë¹„ìš© íš¨ìœ¨ì  ëª¨ë¸ ì„ íƒ ìµœì í™”
"""

import asyncio
import time
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
import json
from pathlib import Path

# Echo ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ë“¤
try:
    from .mistral_wrapper import OllamaMistralWrapper as MistralWrapper
    from .llm_bridge import LLMBridge, CooperationMode, LLMProvider
    from .echo_selector import EchoSelector, ProcessingMode, ComplexityLevel

    ECHO_COMPONENTS_AVAILABLE = True
except ImportError:
    print("âš ï¸ Echo ì»´í¬ë„ŒíŠ¸ ì¼ë¶€ ë¡œë“œ ì‹¤íŒ¨")
    ECHO_COMPONENTS_AVAILABLE = False


class ModelType(Enum):
    """ëª¨ë¸ íƒ€ì…"""

    ECHO_NATIVE = "echo_native"  # ìˆœìˆ˜ Echo ì‹œìŠ¤í…œ
    MISTRAL_LOCAL = "mistral_local"  # ë¡œì»¬ Mistral
    EXTERNAL_API = "external_api"  # ì™¸ë¶€ API (GPT, Claude)
    HYBRID_ECHO_MISTRAL = "hybrid_echo_mistral"  # Echo + Mistral
    HYBRID_ECHO_API = "hybrid_echo_api"  # Echo + External
    ADAPTIVE = "adaptive"  # ìƒí™©ë³„ ì ì‘


class RouterDecision(Enum):
    """ë¼ìš°í„° ê²°ì •"""

    ROUTE_TO_ECHO = "route_to_echo"
    ROUTE_TO_MISTRAL = "route_to_mistral"
    ROUTE_TO_API = "route_to_api"
    ROUTE_TO_HYBRID = "route_to_hybrid"
    ROUTE_TO_FALLBACK = "route_to_fallback"


@dataclass
class RoutingRequest:
    """ë¼ìš°íŒ… ìš”ì²­"""

    user_input: str
    complexity_score: float
    urgency_level: int
    echo_analysis: Optional[str] = None
    user_context: Dict[str, Any] = None
    preferred_quality: str = "balanced"  # "speed", "quality", "balanced"
    max_cost: float = 0.01
    offline_only: bool = False


@dataclass
class RoutingResult:
    """ë¼ìš°íŒ… ê²°ê³¼"""

    selected_model: ModelType
    decision_reasoning: List[str]
    response_text: str
    processing_time: float
    cost_estimate: float
    quality_score: float
    confidence: float
    fallback_used: bool
    model_health: Dict[str, Any]


@dataclass
class ModelHealth:
    """ëª¨ë¸ ê±´ê°• ìƒíƒœ"""

    model_type: ModelType
    available: bool
    response_time_avg: float
    error_rate: float
    memory_usage: float
    queue_length: int
    last_check: datetime


class LLMRouter:
    """ì§€ëŠ¥ì  LLM ë¼ìš°íŒ… ì‹œìŠ¤í…œ"""

    def __init__(self, config_path: str = "config/llm_router_config.yaml"):
        self.config_path = config_path

        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë“¤
        self.mistral_wrapper: Optional[MistralWrapper] = None
        self.llm_bridge: Optional[LLMBridge] = None
        self.echo_selector: Optional[EchoSelector] = None

        # ëª¨ë¸ ê±´ê°• ìƒíƒœ ëª¨ë‹ˆí„°ë§
        self.model_health: Dict[ModelType, ModelHealth] = {}

        # ë¼ìš°íŒ… í†µê³„
        self.routing_stats = {
            "total_requests": 0,
            "model_usage": {model.value: 0 for model in ModelType},
            "avg_response_times": {model.value: [] for model in ModelType},
            "success_rates": {model.value: [] for model in ModelType},
            "cost_tracking": 0.0,
        }

        # ë¼ìš°íŒ… ê·œì¹™ ë° ì„¤ì •
        self.routing_rules = self._load_routing_rules()
        self.fallback_chain = self._setup_fallback_chain()

        # ë¹„ë™ê¸° ì´ˆê¸°í™”ëŠ” lazyë¡œ ë³€ê²½ (ì²« í˜¸ì¶œ ì‹œ ì‹¤í–‰)
        self._initialized = False

        print("ğŸ¯ LLM Router ì´ˆê¸°í™” ì™„ë£Œ")

    async def _ensure_initialized(self):
        """ì»´í¬ë„ŒíŠ¸ë“¤ì´ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ , í•„ìš”ì‹œ ì´ˆê¸°í™”"""
        if self._initialized:
            return
        await self._initialize_components()
        self._initialized = True

    async def _initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ë“¤ ë¹„ë™ê¸° ì´ˆê¸°í™”"""
        try:
            # config/system.yamlì—ì„œ ì„¤ì • ë¡œë“œ
            config = self._load_system_config()

            # Mistral ë˜í¼ ì´ˆê¸°í™”
            if config.get("llm", {}).get("mistral", {}).get("enabled", True):
                mistral_config = config.get("llm", {}).get("mistral", {})
                self.mistral_wrapper = MistralWrapper(
                    model_path=mistral_config.get(
                        "model_path", "models/Mistral-7B-Instruct-v0.2"
                    ),
                    device=mistral_config.get("device", "auto"),
                    load_in_8bit=mistral_config.get("load_in_8bit", True),
                    config=mistral_config,
                )

            # LLM ë¸Œë¦¬ì§€ ì´ˆê¸°í™” (Claude ë“±)
            try:
                self.llm_bridge = LLMBridge()
            except ImportError:
                print("âš ï¸ LLMBridge ì»´í¬ë„ŒíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ì™¸ë¶€ API ê¸°ëŠ¥ ë¹„í™œì„±í™”")
                self.llm_bridge = None

            # Echo ì„ íƒê¸° ì´ˆê¸°í™”
            try:
                self.echo_selector = EchoSelector()
            except ImportError:
                print(
                    "âš ï¸ EchoSelector ì»´í¬ë„ŒíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - Echo ë„¤ì´í‹°ë¸Œ ê¸°ëŠ¥ ì œí•œ"
                )
                self.echo_selector = None

            # ì´ˆê¸° ê±´ê°• ìƒíƒœ ì²´í¬
            await self._update_all_model_health()

            print("âœ… LLM Router ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            print(f"âš ï¸ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    async def route_request(self, request: RoutingRequest) -> RoutingResult:
        """ìš”ì²­ì„ ìµœì  ëª¨ë¸ë¡œ ë¼ìš°íŒ…"""

        # Lazy initialization í™•ì¸
        await self._ensure_initialized()

        start_time = time.time()
        self.routing_stats["total_requests"] += 1

        # 1. ëª¨ë¸ ê±´ê°• ìƒíƒœ í™•ì¸
        await self._update_critical_model_health()

        # 2. ë¼ìš°íŒ… ê²°ì •
        decision, reasoning = await self._make_routing_decision(request)

        # 3. ì„ íƒëœ ëª¨ë¸ë¡œ ìš”ì²­ ì²˜ë¦¬
        try:
            response_text, quality_score, cost = await self._execute_routing(
                decision, request
            )
            fallback_used = False

        except Exception as e:
            print(f"âš ï¸ ì£¼ ëª¨ë¸ ì‹¤íŒ¨: {e}")
            # í´ë°± ì²´ì¸ ì‹¤í–‰
            response_text, quality_score, cost, fallback_used = (
                await self._execute_fallback(request)
            )
            reasoning.append(f"í´ë°± ëª¨ë“œ ì‚¬ìš©: {e}")

        # 4. ê²°ê³¼ ì •ë¦¬ ë° í†µê³„ ì—…ë°ì´íŠ¸
        processing_time = time.time() - start_time
        confidence = self._calculate_routing_confidence(
            decision, request, quality_score
        )

        result = RoutingResult(
            selected_model=self._decision_to_model_type(decision),
            decision_reasoning=reasoning,
            response_text=response_text,
            processing_time=processing_time,
            cost_estimate=cost,
            quality_score=quality_score,
            confidence=confidence,
            fallback_used=fallback_used,
            model_health=self._get_current_health_summary(),
        )

        # í†µê³„ ì—…ë°ì´íŠ¸
        self._update_routing_stats(result)

        return result

    async def _make_routing_decision(
        self, request: RoutingRequest
    ) -> Tuple[RouterDecision, List[str]]:
        """ë¼ìš°íŒ… ê²°ì • ë¡œì§"""

        reasoning = []

        # 1. ì˜¤í”„ë¼ì¸ ì „ìš© ëª¨ë“œ ì²´í¬
        if request.offline_only:
            if self._is_model_available(ModelType.MISTRAL_LOCAL):
                reasoning.append("ì˜¤í”„ë¼ì¸ ëª¨ë“œ - Mistral ë¡œì»¬ ì„ íƒ")
                return RouterDecision.ROUTE_TO_MISTRAL, reasoning
            else:
                reasoning.append("ì˜¤í”„ë¼ì¸ ëª¨ë“œ - Echo ë„¤ì´í‹°ë¸Œë¡œ í´ë°±")
                return RouterDecision.ROUTE_TO_ECHO, reasoning

        # 2. ê¸´ê¸‰ë„ ê¸°ë°˜ íŒë‹¨
        if request.urgency_level >= 4:
            # ê¸´ê¸‰ ìƒí™© - ì†ë„ ìš°ì„ 
            if self._is_model_available(ModelType.ECHO_NATIVE):
                reasoning.append("ê¸´ê¸‰ ìƒí™© - Echo ë„¤ì´í‹°ë¸Œ ìš°ì„ ")
                return RouterDecision.ROUTE_TO_ECHO, reasoning
            elif self._is_model_available(ModelType.MISTRAL_LOCAL):
                reasoning.append("ê¸´ê¸‰ ìƒí™© - Mistral ë¡œì»¬ ì‚¬ìš©")
                return RouterDecision.ROUTE_TO_MISTRAL, reasoning

        # 3. ë³µì¡ë„ ê¸°ë°˜ íŒë‹¨
        if request.complexity_score < 0.3:
            # ë‹¨ìˆœí•œ ìš”ì²­ - Echo ë˜ëŠ” ë¹ ë¥¸ LLM
            reasoning.append("ë‚®ì€ ë³µì¡ë„ - ë¹ ë¥¸ ì²˜ë¦¬ ëª¨ë¸ ì„ íƒ")
            if self._is_model_available(ModelType.ECHO_NATIVE):
                return RouterDecision.ROUTE_TO_ECHO, reasoning

        elif request.complexity_score > 0.7:
            # ë³µì¡í•œ ìš”ì²­ - í•˜ì´ë¸Œë¦¬ë“œ ë˜ëŠ” ê³ í’ˆì§ˆ ëª¨ë¸
            reasoning.append("ë†’ì€ ë³µì¡ë„ - ê³ í’ˆì§ˆ ëª¨ë¸ í•„ìš”")

            if request.preferred_quality == "quality":
                if self._is_model_available(
                    ModelType.EXTERNAL_API
                ) and self._check_cost_constraint(
                    ModelType.EXTERNAL_API, request.max_cost
                ):
                    reasoning.append("í’ˆì§ˆ ìš°ì„  - ì™¸ë¶€ API ì‚¬ìš©")
                    return RouterDecision.ROUTE_TO_API, reasoning

            # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì‹œë„
            if self._is_model_available(
                ModelType.ECHO_NATIVE
            ) and self._is_model_available(ModelType.MISTRAL_LOCAL):
                reasoning.append("í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ - Echo + Mistral")
                return RouterDecision.ROUTE_TO_HYBRID, reasoning

        # 4. í’ˆì§ˆ ì„ í˜¸ë„ ê¸°ë°˜ íŒë‹¨
        if request.preferred_quality == "speed":
            # ì†ë„ ìš°ì„ 
            if self._is_model_available(ModelType.ECHO_NATIVE):
                reasoning.append("ì†ë„ ìš°ì„  - Echo ë„¤ì´í‹°ë¸Œ")
                return RouterDecision.ROUTE_TO_ECHO, reasoning

        elif request.preferred_quality == "quality":
            # í’ˆì§ˆ ìš°ì„ 
            if self._is_model_available(
                ModelType.EXTERNAL_API
            ) and self._check_cost_constraint(ModelType.EXTERNAL_API, request.max_cost):
                reasoning.append("í’ˆì§ˆ ìš°ì„  - ì™¸ë¶€ API")
                return RouterDecision.ROUTE_TO_API, reasoning

        # 5. ê¸°ë³¸ ë¼ìš°íŒ… - ê· í˜• ëª¨ë“œ
        reasoning.append("ê¸°ë³¸ ë¼ìš°íŒ… - ê· í˜• ëª¨ë“œ")

        if self._is_model_available(ModelType.MISTRAL_LOCAL):
            reasoning.append("Mistral ë¡œì»¬ ì‚¬ìš© ê°€ëŠ¥")
            return RouterDecision.ROUTE_TO_MISTRAL, reasoning
        elif self._is_model_available(ModelType.ECHO_NATIVE):
            reasoning.append("Echo ë„¤ì´í‹°ë¸Œë¡œ í´ë°±")
            return RouterDecision.ROUTE_TO_ECHO, reasoning
        else:
            reasoning.append("ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨ - í´ë°± ì²´ì¸ ì‹¤í–‰")
            return RouterDecision.ROUTE_TO_FALLBACK, reasoning

    async def _execute_routing(
        self, decision: RouterDecision, request: RoutingRequest
    ) -> Tuple[str, float, float]:
        """ë¼ìš°íŒ… ê²°ì • ì‹¤í–‰"""

        if decision == RouterDecision.ROUTE_TO_ECHO:
            return await self._route_to_echo(request)

        elif decision == RouterDecision.ROUTE_TO_MISTRAL:
            return await self._route_to_mistral(request)

        elif decision == RouterDecision.ROUTE_TO_API:
            return await self._route_to_api(request)

        elif decision == RouterDecision.ROUTE_TO_HYBRID:
            return await self._route_to_hybrid(request)

        else:  # ROUTE_TO_FALLBACK
            return await self._route_to_fallback(request)

    async def _route_to_echo(self, request: RoutingRequest) -> Tuple[str, float, float]:
        """Echo ë„¤ì´í‹°ë¸Œ ë¼ìš°íŒ…"""

        # Echo ì‹œìŠ¤í…œì„ í†µí•œ ìˆœìˆ˜ íŒë‹¨
        if request.echo_analysis:
            response_text = request.echo_analysis
        else:
            # ê°„ì†Œí™”ëœ Echo ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜
            if request.urgency_level >= 3:
                response_text = "ìƒí™©ì„ ì‹ ì¤‘íˆ ê³ ë ¤í•´ë³´ê² ìŠµë‹ˆë‹¤."
            else:
                response_text = "ë§ì”€í•´ì£¼ì‹  ë‚´ìš©ì„ ì´í•´í–ˆìŠµë‹ˆë‹¤."

        quality_score = 0.8  # Echoì˜ ì¼ê´€ëœ í’ˆì§ˆ
        cost = 0.0  # ë‚´ë¶€ ì²˜ë¦¬ ë¹„ìš© ì—†ìŒ

        return response_text, quality_score, cost

    async def _route_to_mistral(
        self, request: RoutingRequest
    ) -> Tuple[str, float, float]:
        """Mistral ë¡œì»¬ ë¼ìš°íŒ…"""

        if not self.mistral_wrapper:
            raise RuntimeError("Mistral wrapper not available")

        # ìš”ì²­ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
        user_context = request.user_context or {}
        user_context.update(
            {
                "urgency_level": request.urgency_level,
                "complexity_score": request.complexity_score,
            }
        )

        if request.echo_analysis:
            # Echo ë¶„ì„ì´ ìˆìœ¼ë©´ ë³´ì¡° ëª¨ë“œ
            result = await self.mistral_wrapper.assist_echo_judgment(
                request.echo_analysis, user_context, MistralMode.ECHO_ASSIST
            )
        else:
            # ë…ë¦½ì  í˜‘ë ¥ íŒë‹¨
            result = await self.mistral_wrapper.cooperative_judgment(request.user_input)

        return result.text, result.confidence, 0.001  # ë¡œì»¬ ì²˜ë¦¬ ìµœì†Œ ë¹„ìš©

    async def _route_to_api(self, request: RoutingRequest) -> Tuple[str, float, float]:
        """ì™¸ë¶€ API ë¼ìš°íŒ…"""

        if not self.llm_bridge:
            raise RuntimeError("LLM bridge not available")

        # LLM ë¸Œë¦¬ì§€ë¥¼ í†µí•œ ì™¸ë¶€ API í˜¸ì¶œ
        cooperation_result = await self.llm_bridge.cooperate_with_echo(
            request.echo_analysis or "ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬",
            CooperationMode.ECHO_THEN_LLM,
            request.user_context,
        )

        return (
            cooperation_result.final_response,
            cooperation_result.quality_metrics.get("overall_quality", 0.8),
            sum(cooperation_result.cost_breakdown.values()),
        )

    async def _route_to_hybrid(
        self, request: RoutingRequest
    ) -> Tuple[str, float, float]:
        """í•˜ì´ë¸Œë¦¬ë“œ ë¼ìš°íŒ… (Echo + Mistral)"""

        # Echo ë¶„ì„ ë¨¼ì € ìˆ˜í–‰
        echo_response, echo_quality, echo_cost = await self._route_to_echo(request)

        # Mistralë¡œ ê°œì„ 
        enhanced_request = RoutingRequest(
            user_input=request.user_input,
            complexity_score=request.complexity_score,
            urgency_level=request.urgency_level,
            echo_analysis=echo_response,
            user_context=request.user_context,
        )

        mistral_response, mistral_quality, mistral_cost = await self._route_to_mistral(
            enhanced_request
        )

        # ë‘ ê²°ê³¼ì˜ ê°€ì¤‘ í‰ê· 
        combined_quality = echo_quality * 0.4 + mistral_quality * 0.6
        total_cost = echo_cost + mistral_cost

        return mistral_response, combined_quality, total_cost

    async def _route_to_fallback(
        self, request: RoutingRequest
    ) -> Tuple[str, float, float]:
        """í´ë°± ë¼ìš°íŒ…"""

        # ìµœì†Œí•œì˜ ì‘ë‹µ ì œê³µ
        fallback_responses = [
            "í˜„ì¬ ì‹œìŠ¤í…œì„ ì •ë¹„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "Echo ì‹œìŠ¤í…œì´ ìµœì„ ì„ ë‹¤í•´ ì‘ë‹µí•˜ê² ìŠµë‹ˆë‹¤.",
        ]

        response_text = fallback_responses[
            request.urgency_level % len(fallback_responses)
        ]
        quality_score = 0.3  # ë‚®ì€ í’ˆì§ˆ
        cost = 0.0

        return response_text, quality_score, cost

    async def _execute_fallback(
        self, request: RoutingRequest
    ) -> Tuple[str, float, float, bool]:
        """í´ë°± ì²´ì¸ ì‹¤í–‰"""

        for fallback_model in self.fallback_chain:
            try:
                if fallback_model == ModelType.ECHO_NATIVE:
                    response, quality, cost = await self._route_to_echo(request)
                    return response, quality, cost, True

                elif (
                    fallback_model == ModelType.MISTRAL_LOCAL
                    and self._is_model_available(ModelType.MISTRAL_LOCAL)
                ):
                    response, quality, cost = await self._route_to_mistral(request)
                    return response, quality, cost, True

            except Exception as e:
                print(f"âš ï¸ í´ë°± ëª¨ë¸ {fallback_model.value} ì‹¤íŒ¨: {e}")
                continue

        # ëª¨ë“  í´ë°± ì‹¤íŒ¨ ì‹œ ìµœì¢… ì‘ë‹µ
        response, quality, cost = await self._route_to_fallback(request)
        return response, quality, cost, True

    def _is_model_available(self, model_type: ModelType) -> bool:
        """ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""

        if model_type == ModelType.ECHO_NATIVE:
            return True  # EchoëŠ” í•­ìƒ ì‚¬ìš© ê°€ëŠ¥

        elif model_type == ModelType.MISTRAL_LOCAL:
            return (
                self.mistral_wrapper is not None and self.mistral_wrapper.model_loaded
            )

        elif model_type == ModelType.EXTERNAL_API:
            return self.llm_bridge is not None

        health = self.model_health.get(model_type)
        return health is not None and health.available

    def _check_cost_constraint(self, model_type: ModelType, max_cost: float) -> bool:
        """ë¹„ìš© ì œì•½ í™•ì¸"""

        estimated_costs = {
            ModelType.ECHO_NATIVE: 0.0,
            ModelType.MISTRAL_LOCAL: 0.001,
            ModelType.EXTERNAL_API: 0.02,
            ModelType.HYBRID_ECHO_MISTRAL: 0.001,
            ModelType.HYBRID_ECHO_API: 0.02,
        }

        estimated_cost = estimated_costs.get(model_type, 0.01)
        return estimated_cost <= max_cost

    def _decision_to_model_type(self, decision: RouterDecision) -> ModelType:
        """ê²°ì •ì„ ëª¨ë¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""

        mapping = {
            RouterDecision.ROUTE_TO_ECHO: ModelType.ECHO_NATIVE,
            RouterDecision.ROUTE_TO_MISTRAL: ModelType.MISTRAL_LOCAL,
            RouterDecision.ROUTE_TO_API: ModelType.EXTERNAL_API,
            RouterDecision.ROUTE_TO_HYBRID: ModelType.HYBRID_ECHO_MISTRAL,
            RouterDecision.ROUTE_TO_FALLBACK: ModelType.ECHO_NATIVE,
        }

        return mapping.get(decision, ModelType.ADAPTIVE)

    def _calculate_routing_confidence(
        self, decision: RouterDecision, request: RoutingRequest, quality_score: float
    ) -> float:
        """ë¼ìš°íŒ… ì‹ ë¢°ë„ ê³„ì‚°"""

        base_confidence = 0.7

        # ëª¨ë¸ ê±´ê°• ìƒíƒœ ê¸°ë°˜ ì¡°ì •
        model_type = self._decision_to_model_type(decision)
        if self._is_model_available(model_type):
            base_confidence += 0.1

        # ë³µì¡ë„ì™€ ëª¨ë¸ ë§¤ì¹­ ì ì ˆì„±
        if request.complexity_score < 0.3 and decision == RouterDecision.ROUTE_TO_ECHO:
            base_confidence += 0.1
        elif (
            request.complexity_score > 0.7
            and decision == RouterDecision.ROUTE_TO_HYBRID
        ):
            base_confidence += 0.1

        # í’ˆì§ˆ ì ìˆ˜ ë°˜ì˜
        base_confidence += (quality_score - 0.5) * 0.2

        return min(base_confidence, 1.0)

    async def _update_all_model_health(self):
        """ëª¨ë“  ëª¨ë¸ ê±´ê°• ìƒíƒœ ì—…ë°ì´íŠ¸"""

        for model_type in ModelType:
            await self._update_model_health(model_type)

    async def _update_critical_model_health(self):
        """ì¤‘ìš” ëª¨ë¸ë“¤ì˜ ê±´ê°• ìƒíƒœë§Œ ë¹ ë¥´ê²Œ ì—…ë°ì´íŠ¸"""

        critical_models = [
            ModelType.ECHO_NATIVE,
            ModelType.MISTRAL_LOCAL,
            ModelType.EXTERNAL_API,
        ]

        for model_type in critical_models:
            await self._update_model_health(model_type)

    async def _update_model_health(self, model_type: ModelType):
        """íŠ¹ì • ëª¨ë¸ì˜ ê±´ê°• ìƒíƒœ ì—…ë°ì´íŠ¸"""

        try:
            start_time = time.time()

            if model_type == ModelType.ECHO_NATIVE:
                # EchoëŠ” í•­ìƒ ì‚¬ìš© ê°€ëŠ¥
                available = True
                response_time = 0.1
                error_rate = 0.01
                memory_usage = 0.1
                queue_length = 0

            elif model_type == ModelType.MISTRAL_LOCAL:
                available = (
                    self.mistral_wrapper is not None
                    and self.mistral_wrapper.model_loaded
                )
                if available:
                    status = self.mistral_wrapper.get_model_status()
                    response_time = status.get("avg_processing_time", 1.0)
                    error_rate = status.get("error_count", 0) / max(
                        status.get("total_requests", 1), 1
                    )
                    memory_usage = status.get("memory_usage_gb", 0)
                    queue_length = 0
                else:
                    response_time = float("inf")
                    error_rate = 1.0
                    memory_usage = 0
                    queue_length = 0

            elif model_type == ModelType.EXTERNAL_API:
                available = self.llm_bridge is not None
                response_time = 2.0  # ì™¸ë¶€ APIëŠ” ì¼ë°˜ì ìœ¼ë¡œ ëŠë¦¼
                error_rate = 0.05
                memory_usage = 0  # ì™¸ë¶€ ì„œë¹„ìŠ¤
                queue_length = 0

            else:
                # ê¸°íƒ€ ëª¨ë¸ë“¤
                available = False
                response_time = float("inf")
                error_rate = 1.0
                memory_usage = 0
                queue_length = 0

            self.model_health[model_type] = ModelHealth(
                model_type=model_type,
                available=available,
                response_time_avg=response_time,
                error_rate=error_rate,
                memory_usage=memory_usage,
                queue_length=queue_length,
                last_check=datetime.now(),
            )

        except Exception as e:
            print(f"âš ï¸ {model_type.value} ê±´ê°• ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

            # ì‹¤íŒ¨í•œ ëª¨ë¸ì€ ì‚¬ìš© ë¶ˆê°€ë¡œ í‘œì‹œ
            self.model_health[model_type] = ModelHealth(
                model_type=model_type,
                available=False,
                response_time_avg=float("inf"),
                error_rate=1.0,
                memory_usage=0,
                queue_length=0,
                last_check=datetime.now(),
            )

    def _get_current_health_summary(self) -> Dict[str, Any]:
        """í˜„ì¬ ëª¨ë¸ ê±´ê°• ìƒíƒœ ìš”ì•½"""

        summary = {}
        for model_type, health in self.model_health.items():
            summary[model_type.value] = {
                "available": health.available,
                "response_time": health.response_time_avg,
                "error_rate": health.error_rate,
                "last_check": (
                    health.last_check.isoformat() if health.last_check else None
                ),
            }

        return summary

    def _update_routing_stats(self, result: RoutingResult):
        """ë¼ìš°íŒ… í†µê³„ ì—…ë°ì´íŠ¸"""

        model_key = result.selected_model.value

        # ëª¨ë¸ ì‚¬ìš© ì¹´ìš´íŠ¸
        self.routing_stats["model_usage"][model_key] += 1

        # ì‘ë‹µ ì‹œê°„ ê¸°ë¡
        self.routing_stats["avg_response_times"][model_key].append(
            result.processing_time
        )
        if len(self.routing_stats["avg_response_times"][model_key]) > 100:
            self.routing_stats["avg_response_times"][model_key] = self.routing_stats[
                "avg_response_times"
            ][model_key][-100:]

        # ì„±ê³µë¥  ê¸°ë¡
        success = 1.0 if result.quality_score > 0.5 else 0.0
        self.routing_stats["success_rates"][model_key].append(success)
        if len(self.routing_stats["success_rates"][model_key]) > 100:
            self.routing_stats["success_rates"][model_key] = self.routing_stats[
                "success_rates"
            ][model_key][-100:]

        # ë¹„ìš© ì¶”ì 
        self.routing_stats["cost_tracking"] += result.cost_estimate

    def _load_routing_rules(self) -> Dict[str, Any]:
        """ë¼ìš°íŒ… ê·œì¹™ ë¡œë“œ"""

        # ê¸°ë³¸ ë¼ìš°íŒ… ê·œì¹™
        return {
            "complexity_thresholds": {"simple": 0.3, "moderate": 0.5, "complex": 0.7},
            "urgency_mappings": {
                1: "echo_native",
                2: "mistral_local",
                3: "mistral_local",
                4: "echo_native",  # ê¸´ê¸‰ì‹œ ë¹ ë¥¸ ì‘ë‹µ
                5: "echo_native",
            },
            "quality_preferences": {
                "speed": ["echo_native", "mistral_local"],
                "balanced": ["mistral_local", "hybrid_echo_mistral"],
                "quality": ["external_api", "hybrid_echo_api"],
            },
        }

    def _setup_fallback_chain(self) -> List[ModelType]:
        """í´ë°± ì²´ì¸ ì„¤ì •"""

        return [
            ModelType.ECHO_NATIVE,  # 1ì°¨ í´ë°± (í•­ìƒ ì‚¬ìš© ê°€ëŠ¥)
            ModelType.MISTRAL_LOCAL,  # 2ì°¨ í´ë°± (ë¡œì»¬ ëª¨ë¸)
            ModelType.EXTERNAL_API,  # 3ì°¨ í´ë°± (ì™¸ë¶€ API)
        ]

    def _load_system_config(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì„¤ì • ë¡œë“œ"""
        try:
            import yaml

            config_path = Path("config/system.yaml")
            if config_path.exists():
                with open(config_path, "r", encoding="utf-8") as f:
                    return yaml.safe_load(f)
        except Exception as e:
            print(f"âš ï¸ ì‹œìŠ¤í…œ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")

        # ê¸°ë³¸ ì„¤ì • ë°˜í™˜
        return {
            "llm": {
                "primary_mode": "mistral",
                "mistral": {
                    "enabled": True,
                    "model_path": "models/Mistral-7B-Instruct-v0.2",
                    "device": "auto",
                    "load_in_8bit": True,
                    "temperature": 0.7,
                    "max_tokens": 512,
                },
            }
        }

    def get_routing_analytics(self) -> Dict[str, Any]:
        """ë¼ìš°íŒ… ë¶„ì„ ë°ì´í„° ë°˜í™˜"""

        analytics = {
            "total_requests": self.routing_stats["total_requests"],
            "model_distribution": self.routing_stats["model_usage"],
            "total_cost": self.routing_stats["cost_tracking"],
            "model_performance": {},
            "health_summary": self._get_current_health_summary(),
        }

        # ëª¨ë¸ë³„ ì„±ëŠ¥ ê³„ì‚°
        for model_key in ModelType:
            model_value = model_key.value

            response_times = self.routing_stats["avg_response_times"][model_value]
            success_rates = self.routing_stats["success_rates"][model_value]

            analytics["model_performance"][model_value] = {
                "avg_response_time": sum(response_times) / max(len(response_times), 1),
                "success_rate": sum(success_rates) / max(len(success_rates), 1),
                "usage_count": self.routing_stats["model_usage"][model_value],
            }

        return analytics

    async def health_check(self) -> Dict[str, Any]:
        """ì „ì²´ ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ í™•ì¸"""

        await self._update_all_model_health()

        return {
            "router_status": "healthy",
            "models_available": sum(
                1 for health in self.model_health.values() if health.available
            ),
            "total_models": len(self.model_health),
            "model_details": self._get_current_health_summary(),
            "routing_stats": self.routing_stats,
            "last_check": datetime.now().isoformat(),
        }


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":

    async def test_llm_router():
        print("ğŸ¯ LLM Router í…ŒìŠ¤íŠ¸")
        print("=" * 60)

        router = LLMRouter()

        # ì´ˆê¸°í™” ëŒ€ê¸°
        await asyncio.sleep(2)

        # ê±´ê°• ìƒíƒœ í™•ì¸
        health = await router.health_check()
        print(
            f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {health['models_available']}/{health['total_models']}"
        )

        # ë‹¤ì–‘í•œ ë¼ìš°íŒ… ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
        test_cases = [
            {
                "user_input": "ì•ˆë…•í•˜ì„¸ìš”",
                "complexity_score": 0.1,
                "urgency_level": 1,
                "preferred_quality": "speed",
                "description": "ê°„ë‹¨í•œ ì¸ì‚¬ - ì†ë„ ìš°ì„ ",
            },
            {
                "user_input": "ì¸ìƒì˜ ì˜ë¯¸ì— ëŒ€í•´ ê¹Šì´ ê³ ë¯¼í•˜ê³  ìˆìŠµë‹ˆë‹¤",
                "complexity_score": 0.8,
                "urgency_level": 2,
                "preferred_quality": "quality",
                "max_cost": 0.05,
                "description": "ë³µì¡í•œ ì² í•™ì  ì§ˆë¬¸ - í’ˆì§ˆ ìš°ì„ ",
            },
            {
                "user_input": "ê¸‰í•œ ê²°ì •ì„ ë‚´ë ¤ì•¼ í•©ë‹ˆë‹¤!",
                "complexity_score": 0.6,
                "urgency_level": 4,
                "preferred_quality": "speed",
                "description": "ê¸´ê¸‰ ìƒí™© - ì†ë„ ìµœìš°ì„ ",
            },
            {
                "user_input": "AIì™€ ì¸ê°„ì˜ í˜‘ë ¥ì— ëŒ€í•œ ê²¬í•´",
                "complexity_score": 0.7,
                "urgency_level": 2,
                "preferred_quality": "balanced",
                "offline_only": True,
                "description": "ì˜¤í”„ë¼ì¸ ëª¨ë“œ - ë¡œì»¬ ì²˜ë¦¬ë§Œ",
            },
        ]

        for i, case in enumerate(test_cases):
            print(f"\n--- í…ŒìŠ¤íŠ¸ {i+1}: {case['description']} ---")

            request = RoutingRequest(
                user_input=case["user_input"],
                complexity_score=case["complexity_score"],
                urgency_level=case["urgency_level"],
                preferred_quality=case.get("preferred_quality", "balanced"),
                max_cost=case.get("max_cost", 0.01),
                offline_only=case.get("offline_only", False),
            )

            try:
                result = await router.route_request(request)

                print(f"ì„ íƒëœ ëª¨ë¸: {result.selected_model.value}")
                print(f"ì‘ë‹µ: {result.response_text[:100]}...")
                print(f"ì²˜ë¦¬ì‹œê°„: {result.processing_time:.3f}ì´ˆ")
                print(f"í’ˆì§ˆ ì ìˆ˜: {result.quality_score:.2f}")
                print(f"ì‹ ë¢°ë„: {result.confidence:.2f}")
                print(f"ë¹„ìš©: ${result.cost_estimate:.4f}")
                print(f"í´ë°± ì‚¬ìš©: {result.fallback_used}")

                if result.decision_reasoning:
                    print("ê²°ì • ê³¼ì •:")
                    for reason in result.decision_reasoning:
                        print(f"  â€¢ {reason}")

            except Exception as e:
                print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

            print("-" * 40)

        # ìµœì¢… ë¶„ì„
        analytics = router.get_routing_analytics()
        print(f"\nğŸ“Š ë¼ìš°íŒ… ë¶„ì„:")
        print(f"ì´ ìš”ì²­: {analytics['total_requests']}")
        print(f"ì´ ë¹„ìš©: ${analytics['total_cost']:.4f}")
        print("ëª¨ë¸ë³„ ì‚¬ìš© ë¶„í¬:", analytics["model_distribution"])

        print("\nğŸ‰ LLM Router í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

    asyncio.run(test_llm_router())

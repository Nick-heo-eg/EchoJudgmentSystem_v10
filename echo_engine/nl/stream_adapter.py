#!/usr/bin/env python3
"""
ğŸŒŠ Echo Stream Adapter
Echo í‘œì¤€ NL ìŠ¤íŠ¸ë¦¼ ë˜í¼

- OpenAI API ìŠ¤íŠ¸ë¦¬ë° í‘œì¤€í™”
- ìë™ ì¬ì‹œë„ ë° ë°±ì˜¤í”„
- ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- ì½œë°± ê¸°ë°˜ ì´ë²¤íŠ¸ ì²˜ë¦¬

@owner: echo
@expose
@maturity: production
"""

from __future__ import annotations
import os
import time
import sys
import math
import random
from typing import Callable, List, Dict, Any, Optional
from openai import OpenAI


class StreamAdapter:
    """
    Echo í‘œì¤€ NL ìŠ¤íŠ¸ë¦¼ ë˜í¼

    Features:
    - on_delta: chunk í…ìŠ¤íŠ¸ ë°˜í™˜ (STDOUT ì—°ê²°)
    - on_usage: ìµœì¢… usage(dict) ë©”íŠ¸ë¦­
    - on_error: ì˜ˆì™¸ ê°ì²´ ì „ë‹¬
    - ë°±ì˜¤í”„: ë„¤íŠ¸ì›Œí¬/429/5xx ì¬ì‹œë„ (ê¸°ë³¸ 3íšŒ)
    - TTI/Duration ìë™ ì¸¡ì •
    """

    def __init__(self, model: Optional[str] = None, max_retries: int = 3):
        self.model = model or os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        self.max_retries = max_retries

        try:
            self.client = OpenAI()
        except Exception as e:
            raise RuntimeError(f"OpenAI client initialization failed: {e}")

    def run(
        self,
        messages: List[Dict[str, str]],
        on_delta: Callable[[str], None],
        on_usage: Callable[[Dict[str, Any]], None],
        on_error: Callable[[Exception], None],
        **stream_params,
    ):
        """
        ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰

        Args:
            messages: OpenAI ë©”ì‹œì§€ ë°°ì—´
            on_delta: í…ìŠ¤íŠ¸ ì¡°ê° ì½œë°±
            on_usage: ì‚¬ìš©ëŸ‰ ë©”íŠ¸ë¦­ ì½œë°±
            on_error: ì˜¤ë¥˜ ì²˜ë¦¬ ì½œë°±
            **stream_params: ì¶”ê°€ ìŠ¤íŠ¸ë¦¬ë° íŒŒë¼ë¯¸í„°
        """

        tries = 0

        while True:
            try:
                # íƒ€ì´ë° ì¸¡ì • ì‹œì‘
                t0 = time.time()
                first_token_time = None
                chunk_count = 0

                # ìŠ¤íŠ¸ë¦¬ë° íŒŒë¼ë¯¸í„° ì¤€ë¹„
                params = {
                    "model": self.model,
                    "messages": messages,
                    "stream": True,
                    "max_tokens": 2000,
                    "temperature": 0.8,
                    **stream_params,
                }

                # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                response = self.client.chat.completions.create(**params)

                for chunk in response:
                    if chunk.choices and chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content

                        # ì²« í† í° ì‹œê°„ ê¸°ë¡
                        if first_token_time is None:
                            first_token_time = time.time()

                        # ë¸íƒ€ ì½œë°± í˜¸ì¶œ
                        on_delta(content)
                        chunk_count += 1

                # ì‚¬ìš©ëŸ‰ ë©”íŠ¸ë¦­ ê³„ì‚°
                total_duration = time.time() - t0
                usage_metrics = {
                    "model": self.model,
                    "tti_sec": (first_token_time - t0) if first_token_time else None,
                    "dur_sec": total_duration,
                    "chunks": chunk_count,
                    "retries": tries,
                }

                # ì‚¬ìš©ëŸ‰ ì½œë°± í˜¸ì¶œ
                on_usage(usage_metrics)
                return

            except Exception as e:
                tries += 1

                # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ í™•ì¸
                if tries > self.max_retries:
                    on_error(e)
                    return

                # ì§€ìˆ˜ ë°±ì˜¤í”„ + ì§€í„° ê³„ì‚°
                base_delay = min(8.0, (2 ** (tries - 1)))
                jitter = random.uniform(-0.1, 0.1)
                backoff_delay = base_delay * (1.0 + jitter)

                print(
                    f"[stream retry {tries}/{self.max_retries}] {e} (sleep {backoff_delay:.1f}s)",
                    file=sys.stderr,
                )

                time.sleep(backoff_delay)


class EchoStreamHandler:
    """Echo ì „ìš© ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬ (í¸ì˜ ë˜í¼)"""

    def __init__(self, system_message: str = "You are Echo, an AI assistant."):
        self.system_message = system_message
        self.adapter = StreamAdapter()

    def process(
        self,
        user_message: str,
        on_token: Callable[[str], None],
        on_complete: Optional[Callable[[Dict[str, Any]], None]] = None,
        on_error: Optional[Callable[[Exception], None]] = None,
    ):
        """ì‚¬ìš©ì ë©”ì‹œì§€ ì²˜ë¦¬"""

        messages = [
            {"role": "system", "content": self.system_message},
            {"role": "user", "content": user_message},
        ]

        def default_complete(usage):
            print(
                f"[stream] {usage['dur_sec']:.2f}s | {usage['chunks']} chunks",
                file=sys.stderr,
            )

        def default_error(error):
            print(f"[stream-error] {error}", file=sys.stderr)

        self.adapter.run(
            messages=messages,
            on_delta=on_token,
            on_usage=on_complete or default_complete,
            on_error=on_error or default_error,
        )


# í¸ì˜ í•¨ìˆ˜
def quick_stream(
    user_message: str, system_message: str = "You are Echo, an AI assistant."
) -> str:
    """ë¹ ë¥¸ ìŠ¤íŠ¸ë¦¬ë° (ë™ê¸°ì‹)"""

    result_text = []

    def collect_token(token: str):
        result_text.append(token)
        print(token, end="", flush=True)

    def on_complete(usage):
        print(f"\n[stream] {usage['dur_sec']:.2f}s", file=sys.stderr)

    def on_error(error):
        print(f"\n[stream-error] {error}", file=sys.stderr)
        raise error

    handler = EchoStreamHandler(system_message)
    handler.process(user_message, collect_token, on_complete, on_error)

    return "".join(result_text)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    import argparse

    parser = argparse.ArgumentParser(description="Echo Stream Adapter Test")
    parser.add_argument(
        "message", nargs="*", default=["Hello Echo!"], help="Test message"
    )
    parser.add_argument(
        "--system", default="You are Echo, an AI assistant.", help="System message"
    )

    args = parser.parse_args()
    message = " ".join(args.message)

    print(f"ğŸŒŠ Testing Echo Stream Adapter", file=sys.stderr)
    print(f"Model: {os.getenv('OPENAI_MODEL', 'gpt-4o-mini')}", file=sys.stderr)
    print(f"Message: {message}", file=sys.stderr)
    print("-" * 50, file=sys.stderr)

    try:
        result = quick_stream(message, args.system)
        print(f"âœ… Stream test completed successfully", file=sys.stderr)
    except Exception as e:
        print(f"âŒ Stream test failed: {e}", file=sys.stderr)
        sys.exit(1)

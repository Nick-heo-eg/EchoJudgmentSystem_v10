#!/usr/bin/env python3
"""
ğŸ² Probabilistic Strategy Engine v1.0
ë² ì´ì§€ì•ˆ ì¶”ë¡  ë° ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ í™•ë¥ ì  ì „ëµ ì„ íƒ ì‹œìŠ¤í…œ

Phase 2: LLM-Free íŒë‹¨ ì‹œìŠ¤í…œ ê³ ë„í™” ëª¨ë“ˆ
- ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ë¥¼ í™œìš©í•œ ìƒí™©ë³„ ìµœì  ì „ëµ ì¶”ë¡ 
- ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ë‹¤ì¤‘ ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ ì˜ˆì¸¡
- ë§ˆë¥´ì½”í”„ ì²´ì¸ ëª¨ë¸ì„ í†µí•œ ëŒ€í™” íë¦„ ë° ê°ì • ì „ì´ ì˜ˆì¸¡
- "ë””ì§€í„¸ ê³µê° ì˜ˆìˆ ê°€"ë¥¼ ìœ„í•œ í™•ë¥ ì  ì˜ì‚¬ê²°ì • ì‹œìŠ¤í…œ

ì°¸ì¡°: LLM-Free íŒë‹¨ ì‹œìŠ¤í…œ ì™„ì„±ë„ ê·¹ëŒ€í™” ê°€ì´ë“œ Phase 2
- ë‹¨ìˆœ ê·œì¹™ ê¸°ë°˜ì„ ë„˜ì–´ì„  í™•ë¥ ì  ì¶”ë¡  ì‹œìŠ¤í…œ
- ë¶ˆí™•ì‹¤ì„±ì„ ê³ ë ¤í•œ ë¦¬ìŠ¤í¬ ê¸°ë°˜ ì „ëµ ì„ íƒ
- ì‹¤ì‹œê°„ í•™ìŠµì„ í†µí•œ í™•ë¥  ëª¨ë¸ ì§€ì†ì  ê°œì„ 
"""

import os
import json
import time
import math
import random
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import statistics
import numpy as np


@dataclass
class StrategyProbability:
    """ì „ëµ í™•ë¥  ë°ì´í„° í´ë˜ìŠ¤"""

    strategy_name: str
    base_probability: float  # ê¸°ë³¸ í™•ë¥ 
    conditional_probabilities: Dict[str, float]  # ì¡°ê±´ë¶€ í™•ë¥ ë“¤
    confidence_interval: Tuple[float, float]  # ì‹ ë¢° êµ¬ê°„
    historical_success_rate: float  # ê³¼ê±° ì„±ê³µë¥ 
    risk_factor: float  # ë¦¬ìŠ¤í¬ ìš”ì†Œ (0.0: ì•ˆì „, 1.0: ìœ„í—˜)
    expected_outcome: Dict[str, float]  # ì˜ˆìƒ ê²°ê³¼


@dataclass
class BayesianContext:
    """ë² ì´ì§€ì•ˆ ì¶”ë¡ ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸"""

    user_emotional_state: str
    conversation_depth: float
    urgency_level: float
    social_context: str
    temporal_factors: Dict[str, Any]
    historical_patterns: Dict[str, float]


class ProbabilisticStrategyEngine:
    """í™•ë¥ ì  ì „ëµ ì„ íƒì„ ìœ„í•œ ë² ì´ì§€ì•ˆ ì¶”ë¡  ì—”ì§„"""

    def __init__(self, data_dir: str = "data/probabilistic_strategy"):
        """ì´ˆê¸°í™”"""
        self.version = "1.0.0"
        self.data_dir = data_dir
        self.strategy_cache = {}
        self.bayesian_network = {}
        self.markov_chains = {}
        self.monte_carlo_samples = 1000
        self.analysis_count = 0

        # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.data_dir, exist_ok=True)

        # ì „ëµ ì¹´í…Œê³ ë¦¬ ì •ì˜
        self.strategy_categories = {
            "empathetic": {
                "strategies": ["comfort", "validate", "listen"],
                "base_success": 0.75,
                "optimal_contexts": ["sadness", "fear", "stress"],
            },
            "analytical": {
                "strategies": ["analyze", "plan", "structure"],
                "base_success": 0.70,
                "optimal_contexts": ["confusion", "decision", "problem"],
            },
            "energizing": {
                "strategies": ["motivate", "challenge", "inspire"],
                "base_success": 0.65,
                "optimal_contexts": ["apathy", "depression", "stagnation"],
            },
            "creative": {
                "strategies": ["explore", "imagine", "reframe"],
                "base_success": 0.60,
                "optimal_contexts": ["boredom", "creativity_block", "routine"],
            },
            "social": {
                "strategies": ["connect", "share", "collaborate"],
                "base_success": 0.68,
                "optimal_contexts": ["loneliness", "isolation", "relationship"],
            },
        }

        # ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ ì´ˆê¸° êµ¬ì¡°
        self.bayesian_network = {
            # P(ì „ëµ|ê°ì •, ì»¨í…ìŠ¤íŠ¸, ì‹œê°„, ê³¼ê±°íŒ¨í„´)
            "strategy_nodes": {
                "comfort": {
                    "prior": 0.15,
                    "dependencies": ["emotion", "urgency", "social_context"],
                },
                "analyze": {
                    "prior": 0.20,
                    "dependencies": ["emotion", "complexity", "user_preference"],
                },
                "motivate": {
                    "prior": 0.12,
                    "dependencies": ["emotion", "energy_level", "time_of_day"],
                },
                "listen": {
                    "prior": 0.18,
                    "dependencies": ["emotion", "conversation_depth", "urgency"],
                },
                "challenge": {
                    "prior": 0.10,
                    "dependencies": ["emotion", "user_confidence", "relationship"],
                },
                "explore": {
                    "prior": 0.12,
                    "dependencies": ["emotion", "creativity_need", "openness"],
                },
                "validate": {
                    "prior": 0.13,
                    "dependencies": ["emotion", "self_doubt", "support_need"],
                },
            },
            # ì¡°ê±´ë¶€ í™•ë¥  í…Œì´ë¸”
            "conditional_probabilities": {
                "emotion_given_strategy": {
                    "comfort": {"sadness": 0.8, "fear": 0.7, "anger": 0.3, "joy": 0.1},
                    "analyze": {
                        "confusion": 0.9,
                        "anxiety": 0.6,
                        "curiosity": 0.7,
                        "frustration": 0.5,
                    },
                    "motivate": {
                        "apathy": 0.8,
                        "depression": 0.7,
                        "fatigue": 0.6,
                        "excitement": 0.4,
                    },
                    "listen": {
                        "overwhelm": 0.9,
                        "sadness": 0.8,
                        "anger": 0.7,
                        "confusion": 0.6,
                    },
                    "challenge": {
                        "complacency": 0.8,
                        "doubt": 0.6,
                        "fear": 0.4,
                        "confidence": 0.7,
                    },
                    "explore": {
                        "boredom": 0.9,
                        "curiosity": 0.8,
                        "stagnation": 0.7,
                        "creativity": 0.6,
                    },
                    "validate": {
                        "insecurity": 0.9,
                        "doubt": 0.8,
                        "shame": 0.7,
                        "anxiety": 0.5,
                    },
                }
            },
        }

        # ë§ˆë¥´ì½”í”„ ì²´ì¸ ìƒíƒœ ì „ì´ ë§¤íŠ¸ë¦­ìŠ¤
        self.emotion_transition_matrix = {
            "sadness": {"sadness": 0.4, "neutral": 0.3, "comfort": 0.2, "hope": 0.1},
            "anger": {"anger": 0.5, "frustration": 0.2, "calm": 0.2, "regret": 0.1},
            "fear": {"fear": 0.6, "anxiety": 0.2, "relief": 0.1, "confidence": 0.1},
            "joy": {"joy": 0.3, "satisfaction": 0.4, "neutral": 0.2, "nostalgia": 0.1},
            "neutral": {
                "neutral": 0.5,
                "curiosity": 0.2,
                "mild_joy": 0.15,
                "mild_concern": 0.15,
            },
        }

        print(f"ğŸ² Probabilistic Strategy Engine v{self.version} ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ğŸ“ í™•ë¥  ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {self.data_dir}")

    def select_optimal_strategy(
        self, context: BayesianContext, available_strategies: List[str] = None
    ) -> Dict[str, Any]:
        """
        ë² ì´ì§€ì•ˆ ì¶”ë¡ ì„ í†µí•œ ìµœì  ì „ëµ ì„ íƒ

        Args:
            context: ë² ì´ì§€ì•ˆ ì¶”ë¡  ì»¨í…ìŠ¤íŠ¸
            available_strategies: ì‚¬ìš© ê°€ëŠ¥í•œ ì „ëµ ëª©ë¡

        Returns:
            ìµœì  ì „ëµ ë° í™•ë¥ ì  ë¶„ì„ ê²°ê³¼
        """
        self.analysis_count += 1
        start_time = time.time()

        if available_strategies is None:
            available_strategies = list(self.bayesian_network["strategy_nodes"].keys())

        # 1. ë² ì´ì§€ì•ˆ ì¶”ë¡ ìœ¼ë¡œ ê° ì „ëµì˜ ì‚¬í›„ í™•ë¥  ê³„ì‚°
        strategy_posteriors = self._calculate_bayesian_posteriors(
            context, available_strategies
        )

        # 2. ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ê° ì „ëµì˜ ì˜ˆìƒ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜
        simulation_results = self._monte_carlo_simulation(context, strategy_posteriors)

        # 3. ë§ˆë¥´ì½”í”„ ì²´ì¸ìœ¼ë¡œ ì¥ê¸° ê²°ê³¼ ì˜ˆì¸¡
        long_term_predictions = self._markov_chain_prediction(
            context, strategy_posteriors
        )

        # 4. ë¦¬ìŠ¤í¬-ë³´ìƒ ë¶„ì„
        risk_reward_analysis = self._risk_reward_analysis(
            strategy_posteriors, simulation_results
        )

        # 5. ìµœì¢… ì „ëµ ì„ íƒ (ê¸°ëŒ€ íš¨ìš© ìµœëŒ€í™”)
        optimal_strategy = self._select_by_expected_utility(
            strategy_posteriors, simulation_results, risk_reward_analysis
        )

        # ê²°ê³¼ êµ¬ì„±
        result = {
            "optimal_strategy": optimal_strategy,
            "strategy_probabilities": strategy_posteriors,
            "simulation_results": simulation_results,
            "long_term_prediction": long_term_predictions,
            "risk_analysis": risk_reward_analysis,
            "confidence_metrics": self._calculate_confidence_metrics(
                strategy_posteriors
            ),
            "meta": {
                "analysis_id": self.analysis_count,
                "processing_time": time.time() - start_time,
                "timestamp": datetime.now().isoformat(),
                "monte_carlo_samples": self.monte_carlo_samples,
            },
        }

        return result

    def _calculate_bayesian_posteriors(
        self, context: BayesianContext, strategies: List[str]
    ) -> Dict[str, StrategyProbability]:
        """ë² ì´ì§€ì•ˆ ì¶”ë¡ ìœ¼ë¡œ ì‚¬í›„ í™•ë¥  ê³„ì‚°"""
        posteriors = {}

        for strategy in strategies:
            if strategy not in self.bayesian_network["strategy_nodes"]:
                continue

            strategy_node = self.bayesian_network["strategy_nodes"][strategy]

            # ì‚¬ì „ í™•ë¥ 
            prior = strategy_node["prior"]

            # ìš°ë„ ê³„ì‚° (P(ì¦ê±°|ì „ëµ))
            likelihood = self._calculate_likelihood(strategy, context)

            # ì •ê·œí™” ìƒìˆ˜ ê³„ì‚°ì„ ìœ„í•œ ì „ì²´ ì¦ê±° í™•ë¥ 
            evidence = self._calculate_evidence_probability(strategies, context)

            # ë² ì´ì¦ˆ ì •ë¦¬: P(ì „ëµ|ì¦ê±°) = P(ì¦ê±°|ì „ëµ) * P(ì „ëµ) / P(ì¦ê±°)
            posterior = (likelihood * prior) / max(evidence, 1e-10)

            # ì‹ ë¢° êµ¬ê°„ ê³„ì‚° (ë² íƒ€ ë¶„í¬ ê·¼ì‚¬)
            confidence_interval = self._calculate_confidence_interval(posterior)

            # ê³¼ê±° ì„±ê³µë¥  (í•™ìŠµ ë°ì´í„° ê¸°ë°˜)
            historical_success = self._get_historical_success_rate(strategy, context)

            # ë¦¬ìŠ¤í¬ ìš”ì†Œ ê³„ì‚°
            risk_factor = self._calculate_risk_factor(strategy, context)

            # ì˜ˆìƒ ê²°ê³¼
            expected_outcome = self._predict_outcome(strategy, context, posterior)

            posteriors[strategy] = StrategyProbability(
                strategy_name=strategy,
                base_probability=posterior,
                conditional_probabilities=self._get_conditional_probs(
                    strategy, context
                ),
                confidence_interval=confidence_interval,
                historical_success_rate=historical_success,
                risk_factor=risk_factor,
                expected_outcome=expected_outcome,
            )

        return posteriors

    def _calculate_likelihood(self, strategy: str, context: BayesianContext) -> float:
        """ìš°ë„ ê³„ì‚° P(ì¦ê±°|ì „ëµ)"""
        likelihood = 1.0

        # ê°ì • ìƒíƒœì— ë”°ë¥¸ ìš°ë„
        emotion_probs = self.bayesian_network["conditional_probabilities"][
            "emotion_given_strategy"
        ]
        if strategy in emotion_probs:
            emotion_likelihood = emotion_probs[strategy].get(
                context.user_emotional_state, 0.1
            )
            likelihood *= emotion_likelihood

        # ì»¨í…ìŠ¤íŠ¸ ìš”ì†Œë“¤ì— ë”°ë¥¸ ìš°ë„ ì¡°ì •
        likelihood *= self._context_likelihood_modifier(strategy, context)

        return likelihood

    def _context_likelihood_modifier(
        self, strategy: str, context: BayesianContext
    ) -> float:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìš°ë„ ìˆ˜ì •ì"""
        modifier = 1.0

        # ëŒ€í™” ê¹Šì´ì— ë”°ë¥¸ ì¡°ì •
        if context.conversation_depth > 0.7:
            if strategy in ["listen", "validate", "comfort"]:
                modifier *= 1.3
            elif strategy in ["challenge", "motivate"]:
                modifier *= 0.7

        # ê¸´ê¸‰ë„ì— ë”°ë¥¸ ì¡°ì •
        if context.urgency_level > 0.7:
            if strategy in ["analyze", "plan"]:
                modifier *= 1.4
            elif strategy in ["explore", "imagine"]:
                modifier *= 0.6

        # ì‚¬íšŒì  ë§¥ë½ì— ë”°ë¥¸ ì¡°ì •
        if context.social_context == "private":
            if strategy in ["comfort", "validate", "listen"]:
                modifier *= 1.2
        elif context.social_context == "public":
            if strategy in ["challenge", "motivate"]:
                modifier *= 1.1

        # ì‹œê°„ì  ìš”ì†Œ ê³ ë ¤
        temporal_modifier = self._temporal_likelihood_modifier(
            strategy, context.temporal_factors
        )
        modifier *= temporal_modifier

        return modifier

    def _temporal_likelihood_modifier(
        self, strategy: str, temporal_factors: Dict[str, Any]
    ) -> float:
        """ì‹œê°„ì  ìš”ì†Œ ê¸°ë°˜ ìš°ë„ ìˆ˜ì •ì"""
        modifier = 1.0

        time_period = temporal_factors.get("time_period", "afternoon")
        day_of_week = temporal_factors.get("day_of_week", "wednesday")

        # ì‹œê°„ëŒ€ë³„ ì „ëµ íš¨ê³¼ì„±
        time_effects = {
            "morning": {"motivate": 1.3, "analyze": 1.2, "comfort": 0.9},
            "afternoon": {"analyze": 1.1, "challenge": 1.1, "explore": 1.0},
            "evening": {"comfort": 1.2, "listen": 1.2, "validate": 1.1},
            "night": {"comfort": 1.4, "listen": 1.3, "analyze": 0.8},
        }

        if time_period in time_effects and strategy in time_effects[time_period]:
            modifier *= time_effects[time_period][strategy]

        # ìš”ì¼ë³„ íš¨ê³¼
        if day_of_week in ["monday", "tuesday"]:  # ì£¼ ì´ˆ
            if strategy in ["motivate", "challenge"]:
                modifier *= 1.1
        elif day_of_week in ["friday", "saturday"]:  # ì£¼ë§ ê·¼ì²˜
            if strategy in ["comfort", "explore"]:
                modifier *= 1.1

        return modifier

    def _calculate_evidence_probability(
        self, strategies: List[str], context: BayesianContext
    ) -> float:
        """ì „ì²´ ì¦ê±° í™•ë¥  P(ì¦ê±°) ê³„ì‚°"""
        evidence = 0.0

        for strategy in strategies:
            if strategy in self.bayesian_network["strategy_nodes"]:
                prior = self.bayesian_network["strategy_nodes"][strategy]["prior"]
                likelihood = self._calculate_likelihood(strategy, context)
                evidence += likelihood * prior

        return evidence

    def _calculate_confidence_interval(
        self, probability: float, confidence_level: float = 0.95
    ) -> Tuple[float, float]:
        """í™•ë¥ ì˜ ì‹ ë¢° êµ¬ê°„ ê³„ì‚°"""
        # ë² íƒ€ ë¶„í¬ë¥¼ ì´ìš©í•œ ê·¼ì‚¬ì  ì‹ ë¢° êµ¬ê°„
        alpha = confidence_level / 2
        n = 100  # ê°€ì •ëœ ìƒ˜í”Œ í¬ê¸°

        # ì •ê·œ ê·¼ì‚¬ë¥¼ ì´ìš©í•œ ì‹ ë¢° êµ¬ê°„
        margin = 1.96 * math.sqrt(probability * (1 - probability) / n)  # 95% ì‹ ë¢°êµ¬ê°„

        lower = max(0.0, probability - margin)
        upper = min(1.0, probability + margin)

        return (lower, upper)

    def _get_historical_success_rate(
        self, strategy: str, context: BayesianContext
    ) -> float:
        """ê³¼ê±° ì„±ê³µë¥  ì¡°íšŒ (ì‹¤ì œë¡œëŠ” í•™ìŠµ ë°ì´í„°ì—ì„œ ê°€ì ¸ì˜´)"""
        # ê¸°ë³¸ ì„±ê³µë¥  (ì¹´í…Œê³ ë¦¬ë³„)
        base_rates = {
            "comfort": 0.78,
            "validate": 0.75,
            "listen": 0.82,
            "analyze": 0.72,
            "plan": 0.70,
            "structure": 0.68,
            "motivate": 0.65,
            "challenge": 0.60,
            "inspire": 0.67,
            "explore": 0.63,
            "imagine": 0.58,
            "reframe": 0.64,
        }

        base_rate = base_rates.get(strategy, 0.65)

        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¡°ì •
        if context.user_emotional_state in ["sadness", "fear"] and strategy in [
            "comfort",
            "validate",
        ]:
            base_rate *= 1.1
        elif context.urgency_level > 0.8 and strategy in ["analyze", "plan"]:
            base_rate *= 1.05

        return min(base_rate, 1.0)

    def _calculate_risk_factor(self, strategy: str, context: BayesianContext) -> float:
        """ì „ëµë³„ ë¦¬ìŠ¤í¬ ìš”ì†Œ ê³„ì‚°"""
        # ê¸°ë³¸ ë¦¬ìŠ¤í¬ ë ˆë²¨
        base_risks = {
            "comfort": 0.1,
            "validate": 0.1,
            "listen": 0.05,
            "analyze": 0.2,
            "plan": 0.15,
            "structure": 0.25,
            "motivate": 0.3,
            "challenge": 0.4,
            "inspire": 0.25,
            "explore": 0.2,
            "imagine": 0.15,
            "reframe": 0.3,
        }

        base_risk = base_risks.get(strategy, 0.2)

        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¦¬ìŠ¤í¬ ì¡°ì •
        if (
            context.user_emotional_state in ["anger", "frustration"]
            and strategy == "challenge"
        ):
            base_risk *= 1.5  # í™”ë‚œ ìƒíƒœì—ì„œ ë„ì „ì€ ìœ„í—˜
        elif context.urgency_level > 0.8 and strategy in ["explore", "imagine"]:
            base_risk *= 1.3  # ê¸‰í•œ ìƒí™©ì—ì„œ íƒìƒ‰ì€ ìœ„í—˜

        return min(base_risk, 1.0)

    def _predict_outcome(
        self, strategy: str, context: BayesianContext, probability: float
    ) -> Dict[str, float]:
        """ì „ëµ ì‹¤í–‰ ì‹œ ì˜ˆìƒ ê²°ê³¼ ì˜ˆì¸¡"""
        # ê¸°ë³¸ ê²°ê³¼ ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜
        outcome_categories = [
            "satisfaction",
            "problem_solving",
            "emotional_improvement",
            "relationship_quality",
        ]

        base_outcomes = {
            "comfort": {
                "satisfaction": 0.8,
                "problem_solving": 0.4,
                "emotional_improvement": 0.9,
                "relationship_quality": 0.7,
            },
            "analyze": {
                "satisfaction": 0.7,
                "problem_solving": 0.9,
                "emotional_improvement": 0.5,
                "relationship_quality": 0.6,
            },
            "motivate": {
                "satisfaction": 0.6,
                "problem_solving": 0.7,
                "emotional_improvement": 0.8,
                "relationship_quality": 0.7,
            },
            "listen": {
                "satisfaction": 0.9,
                "problem_solving": 0.3,
                "emotional_improvement": 0.8,
                "relationship_quality": 0.9,
            },
            "challenge": {
                "satisfaction": 0.5,
                "problem_solving": 0.8,
                "emotional_improvement": 0.6,
                "relationship_quality": 0.5,
            },
            "explore": {
                "satisfaction": 0.7,
                "problem_solving": 0.6,
                "emotional_improvement": 0.7,
                "relationship_quality": 0.6,
            },
        }

        outcomes = base_outcomes.get(strategy, {cat: 0.6 for cat in outcome_categories})

        # í™•ë¥ ê³¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°ê³¼ ì¡°ì •
        for category in outcomes:
            outcomes[category] *= probability  # í™•ë¥ ì´ ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ê²°ê³¼

            # ì»¨í…ìŠ¤íŠ¸ë³„ ì¡°ì •
            if (
                category == "emotional_improvement"
                and context.user_emotional_state in ["sadness", "fear"]
            ):
                outcomes[category] *= 1.1
            elif category == "problem_solving" and context.urgency_level > 0.7:
                outcomes[category] *= 1.05

        return outcomes

    def _get_conditional_probs(
        self, strategy: str, context: BayesianContext
    ) -> Dict[str, float]:
        """ì¡°ê±´ë¶€ í™•ë¥ ë“¤ ë°˜í™˜"""
        return {
            "success_given_emotion": self._prob_success_given_emotion(
                strategy, context.user_emotional_state
            ),
            "satisfaction_given_urgency": self._prob_satisfaction_given_urgency(
                strategy, context.urgency_level
            ),
            "effectiveness_given_depth": self._prob_effectiveness_given_depth(
                strategy, context.conversation_depth
            ),
        }

    def _prob_success_given_emotion(self, strategy: str, emotion: str) -> float:
        """P(ì„±ê³µ|ê°ì •, ì „ëµ) ê³„ì‚°"""
        emotion_strategy_success = {
            ("sadness", "comfort"): 0.85,
            ("sadness", "listen"): 0.80,
            ("sadness", "analyze"): 0.40,
            ("anger", "listen"): 0.75,
            ("anger", "validate"): 0.70,
            ("anger", "challenge"): 0.30,
            ("fear", "comfort"): 0.80,
            ("fear", "analyze"): 0.65,
            ("fear", "motivate"): 0.45,
            ("joy", "motivate"): 0.75,
            ("joy", "explore"): 0.70,
            ("joy", "comfort"): 0.60,
        }

        return emotion_strategy_success.get((emotion, strategy), 0.60)

    def _prob_satisfaction_given_urgency(self, strategy: str, urgency: float) -> float:
        """P(ë§Œì¡±|ê¸´ê¸‰ë„, ì „ëµ) ê³„ì‚°"""
        if urgency > 0.7:  # ë†’ì€ ê¸´ê¸‰ë„
            urgent_strategy_satisfaction = {
                "analyze": 0.80,
                "plan": 0.75,
                "comfort": 0.50,
                "explore": 0.30,
            }
            return urgent_strategy_satisfaction.get(strategy, 0.55)
        else:  # ë‚®ì€ ê¸´ê¸‰ë„
            relaxed_strategy_satisfaction = {
                "explore": 0.75,
                "comfort": 0.80,
                "analyze": 0.65,
                "motivate": 0.70,
            }
            return relaxed_strategy_satisfaction.get(strategy, 0.65)

    def _prob_effectiveness_given_depth(self, strategy: str, depth: float) -> float:
        """P(íš¨ê³¼ì„±|ëŒ€í™”ê¹Šì´, ì „ëµ) ê³„ì‚°"""
        if depth > 0.7:  # ê¹Šì€ ëŒ€í™”
            deep_strategy_effectiveness = {
                "listen": 0.90,
                "validate": 0.85,
                "comfort": 0.80,
                "challenge": 0.60,
            }
            return deep_strategy_effectiveness.get(strategy, 0.70)
        else:  # í‘œë©´ì  ëŒ€í™”
            surface_strategy_effectiveness = {
                "analyze": 0.75,
                "motivate": 0.70,
                "explore": 0.65,
                "listen": 0.60,
            }
            return surface_strategy_effectiveness.get(strategy, 0.65)

    def _monte_carlo_simulation(
        self,
        context: BayesianContext,
        strategy_posteriors: Dict[str, StrategyProbability],
    ) -> Dict[str, Dict[str, float]]:
        """ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ì „ëµë³„ ê²°ê³¼ ë¶„í¬ ê³„ì‚°"""
        simulation_results = {}

        for strategy, prob_data in strategy_posteriors.items():
            results = []

            # ëª¬í…Œì¹´ë¥¼ë¡œ ìƒ˜í”Œë§
            for _ in range(self.monte_carlo_samples):
                # ëœë¤ ë³€ìˆ˜ë“¤ ìƒ˜í”Œë§
                success_prob = prob_data.base_probability
                risk_factor = prob_data.risk_factor

                # ë…¸ì´ì¦ˆ ì¶”ê°€ (í˜„ì‹¤ì  ë³€ë™ì„±)
                noise = random.gauss(0, 0.1)  # í‘œì¤€í¸ì°¨ 0.1ì¸ ì •ê·œë¶„í¬ ë…¸ì´ì¦ˆ
                actual_success = max(0, min(1, success_prob + noise))

                # ë¦¬ìŠ¤í¬ ê³ ë ¤
                if random.random() < risk_factor:
                    actual_success *= 0.5  # ë¦¬ìŠ¤í¬ ë°œìƒ ì‹œ ì„±ê³µë¥  ë°˜ê°

                results.append(actual_success)

            # ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ í†µê³„
            simulation_results[strategy] = {
                "mean_success": statistics.mean(results),
                "std_success": statistics.stdev(results) if len(results) > 1 else 0,
                "percentile_25": np.percentile(results, 25),
                "percentile_75": np.percentile(results, 75),
                "min_success": min(results),
                "max_success": max(results),
                "success_probability": sum(1 for r in results if r > 0.5)
                / len(results),
            }

        return simulation_results

    def _markov_chain_prediction(
        self,
        context: BayesianContext,
        strategy_posteriors: Dict[str, StrategyProbability],
    ) -> Dict[str, Any]:
        """ë§ˆë¥´ì½”í”„ ì²´ì¸ì„ ì´ìš©í•œ ì¥ê¸° ê°ì • ìƒíƒœ ì˜ˆì¸¡"""
        current_emotion = context.user_emotional_state

        # ê° ì „ëµ ì‹¤í–‰ í›„ ê°ì • ì „ì´ ì˜ˆì¸¡
        predictions = {}

        for strategy in strategy_posteriors.keys():
            # ì „ëµ ì‹¤í–‰ì´ ê°ì •ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ëª¨ë¸ë§
            strategy_emotion_effects = {
                "comfort": {"sadness": "relief", "fear": "security", "anger": "calm"},
                "analyze": {
                    "confusion": "clarity",
                    "anxiety": "understanding",
                    "overwhelm": "structure",
                },
                "motivate": {
                    "apathy": "motivation",
                    "depression": "hope",
                    "fatigue": "energy",
                },
                "listen": {
                    "loneliness": "connection",
                    "anger": "validation",
                    "sadness": "understanding",
                },
                "challenge": {
                    "complacency": "growth",
                    "doubt": "confidence",
                    "fear": "courage",
                },
                "explore": {
                    "boredom": "interest",
                    "stagnation": "movement",
                    "routine": "novelty",
                },
            }

            # ì „ëµ í›„ ì˜ˆìƒ ê°ì • ìƒíƒœ
            post_strategy_emotion = strategy_emotion_effects.get(strategy, {}).get(
                current_emotion, current_emotion
            )

            # ë§ˆë¥´ì½”í”„ ì²´ì¸ìœ¼ë¡œ n-step ë¯¸ë˜ ì˜ˆì¸¡
            future_states = self._predict_emotion_chain(post_strategy_emotion, steps=3)

            predictions[strategy] = {
                "immediate_emotion": post_strategy_emotion,
                "future_states": future_states,
                "stability_score": self._calculate_emotion_stability(future_states),
            }

        return predictions

    def _predict_emotion_chain(
        self, initial_state: str, steps: int
    ) -> List[Dict[str, float]]:
        """ë§ˆë¥´ì½”í”„ ì²´ì¸ìœ¼ë¡œ ê°ì • ìƒíƒœ ì „ì´ ì˜ˆì¸¡"""
        if initial_state not in self.emotion_transition_matrix:
            initial_state = "neutral"

        current_distribution = {initial_state: 1.0}
        future_states = []

        for step in range(steps):
            next_distribution = defaultdict(float)

            for current_emotion, current_prob in current_distribution.items():
                if current_emotion in self.emotion_transition_matrix:
                    transitions = self.emotion_transition_matrix[current_emotion]

                    for next_emotion, trans_prob in transitions.items():
                        next_distribution[next_emotion] += current_prob * trans_prob

            current_distribution = dict(next_distribution)
            future_states.append(current_distribution.copy())

        return future_states

    def _calculate_emotion_stability(
        self, future_states: List[Dict[str, float]]
    ) -> float:
        """ê°ì • ì•ˆì •ì„± ì ìˆ˜ ê³„ì‚°"""
        if not future_states:
            return 0.5

        # ê° ë‹¨ê³„ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ê°ì •ì˜ í™•ë¥ ë“¤
        max_probs = [max(state.values()) for state in future_states]

        # ì•ˆì •ì„± = í‰ê·  ìµœëŒ€ í™•ë¥  (ë†’ì„ìˆ˜ë¡ ì•ˆì •ì )
        stability = statistics.mean(max_probs)

        return stability

    def _risk_reward_analysis(
        self,
        strategy_posteriors: Dict[str, StrategyProbability],
        simulation_results: Dict[str, Dict[str, float]],
    ) -> Dict[str, Dict[str, float]]:
        """ë¦¬ìŠ¤í¬-ë³´ìƒ ë¶„ì„"""
        analysis = {}

        for strategy in strategy_posteriors.keys():
            prob_data = strategy_posteriors[strategy]
            sim_data = simulation_results[strategy]

            # ë³´ìƒ ê³„ì‚° (ê¸°ëŒ€ ì„±ê³µë¥  Ã— ì ì¬ì  íš¨ê³¼)
            expected_reward = (
                sim_data["mean_success"] * prob_data.historical_success_rate
            )

            # ë¦¬ìŠ¤í¬ ê³„ì‚° (ë³€ë™ì„± + ë¦¬ìŠ¤í¬ ìš”ì†Œ)
            risk_score = sim_data["std_success"] + prob_data.risk_factor

            # ìƒ¤í”„ ë¹„ìœ¨ ìœ ì‚¬ ì§€í‘œ (ë³´ìƒ/ë¦¬ìŠ¤í¬ ë¹„ìœ¨)
            sharpe_ratio = expected_reward / max(risk_score, 0.01)

            # í•˜ë°© ë¦¬ìŠ¤í¬ (ì‹¤íŒ¨ í™•ë¥ )
            downside_risk = 1 - sim_data["success_probability"]

            analysis[strategy] = {
                "expected_reward": expected_reward,
                "risk_score": risk_score,
                "sharpe_ratio": sharpe_ratio,
                "downside_risk": downside_risk,
                "risk_adjusted_return": expected_reward * (1 - downside_risk),
            }

        return analysis

    def _select_by_expected_utility(
        self,
        strategy_posteriors: Dict[str, StrategyProbability],
        simulation_results: Dict[str, Dict[str, float]],
        risk_analysis: Dict[str, Dict[str, float]],
    ) -> Dict[str, Any]:
        """ê¸°ëŒ€ íš¨ìš© ìµœëŒ€í™”ë¥¼ í†µí•œ ìµœì  ì „ëµ ì„ íƒ"""
        utilities = {}

        for strategy in strategy_posteriors.keys():
            prob_data = strategy_posteriors[strategy]
            sim_data = simulation_results[strategy]
            risk_data = risk_analysis[strategy]

            # ê¸°ëŒ€ íš¨ìš© = ê¸°ëŒ€ ë³´ìƒ - ë¦¬ìŠ¤í¬ í˜ë„í‹°
            risk_tolerance = 0.5  # ë¦¬ìŠ¤í¬ íšŒí”¼ ì •ë„ (0: ë¦¬ìŠ¤í¬ ì¤‘ë¦½, 1: ê·¹ë„ íšŒí”¼)

            expected_utility = (
                risk_data["expected_reward"]
                - (risk_tolerance * risk_data["risk_score"])
                + (0.2 * prob_data.historical_success_rate)  # ê³¼ê±° ì„±ê³¼ ë³´ë„ˆìŠ¤
                + (0.1 * sim_data["success_probability"])  # ì„±ê³µ í™•ë¥  ë³´ë„ˆìŠ¤
            )

            utilities[strategy] = {
                "expected_utility": expected_utility,
                "components": {
                    "base_reward": risk_data["expected_reward"],
                    "risk_penalty": risk_tolerance * risk_data["risk_score"],
                    "history_bonus": 0.2 * prob_data.historical_success_rate,
                    "success_bonus": 0.1 * sim_data["success_probability"],
                },
            }

        # ìµœê³  íš¨ìš© ì „ëµ ì„ íƒ
        best_strategy = max(
            utilities.keys(), key=lambda s: utilities[s]["expected_utility"]
        )

        return {
            "strategy": best_strategy,
            "expected_utility": utilities[best_strategy]["expected_utility"],
            "all_utilities": utilities,
            "confidence": strategy_posteriors[best_strategy].base_probability,
            "risk_level": strategy_posteriors[best_strategy].risk_factor,
        }

    def _calculate_confidence_metrics(
        self, strategy_posteriors: Dict[str, StrategyProbability]
    ) -> Dict[str, float]:
        """ì‹ ë¢°ë„ ì§€í‘œ ê³„ì‚°"""
        probabilities = [prob.base_probability for prob in strategy_posteriors.values()]

        # ì—”íŠ¸ë¡œí”¼ (ë¶ˆí™•ì‹¤ì„± ì¸¡ì •)
        entropy = -sum(p * math.log2(p) if p > 0 else 0 for p in probabilities)

        # ìµœëŒ€ í™•ë¥ ê³¼ ë‘ ë²ˆì§¸ ìµœëŒ€ í™•ë¥ ì˜ ì°¨ì´
        sorted_probs = sorted(probabilities, reverse=True)
        confidence_gap = (
            sorted_probs[0] - sorted_probs[1]
            if len(sorted_probs) > 1
            else sorted_probs[0]
        )

        # ì „ì²´ì  ì‹ ë¢°ë„
        overall_confidence = max(probabilities)

        return {
            "entropy": entropy,
            "confidence_gap": confidence_gap,
            "overall_confidence": overall_confidence,
            "decision_clarity": (
                confidence_gap / overall_confidence if overall_confidence > 0 else 0
            ),
        }

    def update_learning_data(
        self,
        strategy: str,
        context: BayesianContext,
        outcome_feedback: Dict[str, float],
    ) -> None:
        """ì‹¤ì œ ê²°ê³¼ í”¼ë“œë°±ì„ í†µí•œ í™•ë¥  ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        # ë² ì´ì§€ì•ˆ í•™ìŠµìœ¼ë¡œ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë² íƒ€ ë¶„í¬ë‚˜ ë””ë¦¬í´ë ˆ ë¶„í¬ ë“±ì„ ì‚¬ìš©í•˜ì—¬
        # ì˜¨ë¼ì¸ í•™ìŠµì„ êµ¬í˜„í•  ìˆ˜ ìˆìŒ
        pass

    def get_strategy_explanation(self, result: Dict[str, Any]) -> List[str]:
        """ì „ëµ ì„ íƒ ì´ìœ  ì„¤ëª… ìƒì„±"""
        explanations = []
        optimal = result["optimal_strategy"]

        strategy_name = optimal["strategy"]
        utility = optimal["expected_utility"]
        confidence = optimal["confidence"]
        risk = optimal["risk_level"]

        explanations.append(
            f"'{strategy_name}' ì „ëµì´ ìµœê³  ê¸°ëŒ€ íš¨ìš© {utility:.3f}ë¡œ ì„ íƒë¨"
        )
        explanations.append(f"ì„ íƒ ì‹ ë¢°ë„: {confidence:.1%}, ë¦¬ìŠ¤í¬ ìˆ˜ì¤€: {risk:.1%}")

        # í™•ë¥ ì  ë¶„ì„ ê·¼ê±°
        confidence_metrics = result["confidence_metrics"]
        if confidence_metrics["decision_clarity"] > 0.3:
            explanations.append("ëª…í™•í•œ ì„ íƒ ê·¼ê±° ì¡´ì¬ - ë†’ì€ ì˜ì‚¬ê²°ì • í™•ì‹¤ì„±")
        else:
            explanations.append("ì—¬ëŸ¬ ì „ëµì´ ìœ ì‚¬í•œ íš¨ìš© - ì‹ ì¤‘í•œ ëª¨ë‹ˆí„°ë§ í•„ìš”")

        # ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ê¸°ë°˜ ì„¤ëª…
        sim_results = result["simulation_results"].get(strategy_name, {})
        success_prob = sim_results.get("success_probability", 0)
        explanations.append(f"ëª¬í…Œì¹´ë¥¼ë¡œ ì‹œë®¬ë ˆì´ì…˜ ì„±ê³µë¥ : {success_prob:.1%}")

        return explanations


def test_probabilistic_strategy_engine():
    """í™•ë¥ ì  ì „ëµ ì—”ì§„ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Probabilistic Strategy Engine í…ŒìŠ¤íŠ¸ ì‹œì‘...")

    engine = ProbabilisticStrategyEngine()

    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ 1: ìŠ¬í”ˆ ê°ì • ìƒíƒœì—ì„œì˜ ì „ëµ ì„ íƒ
    print("\nğŸ“ ì‹œë‚˜ë¦¬ì˜¤ 1: ìŠ¬í”ˆ ê°ì • ìƒíƒœ - ë‚®ì€ ê¸´ê¸‰ë„")
    context_1 = BayesianContext(
        user_emotional_state="sadness",
        conversation_depth=0.6,
        urgency_level=0.2,
        social_context="private",
        temporal_factors={"time_period": "evening", "day_of_week": "sunday"},
        historical_patterns={"preferred_empathy": 0.8, "analytical_preference": 0.3},
    )

    result_1 = engine.select_optimal_strategy(context_1)
    print(f"ğŸ¯ ìµœì  ì „ëµ: {result_1['optimal_strategy']['strategy']}")
    print(f"ğŸ’ª ê¸°ëŒ€ íš¨ìš©: {result_1['optimal_strategy']['expected_utility']:.3f}")
    print(f"ğŸ² ì‹ ë¢°ë„: {result_1['optimal_strategy']['confidence']:.3f}")

    explanations_1 = engine.get_strategy_explanation(result_1)
    print("ğŸ§  ì„ íƒ ê·¼ê±°:")
    for explanation in explanations_1:
        print(f"   - {explanation}")

    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ 2: ê¸‰í•œ ìƒí™©ì—ì„œì˜ ì „ëµ ì„ íƒ
    print("\nğŸ“ ì‹œë‚˜ë¦¬ì˜¤ 2: í˜¼ë€ìŠ¤ëŸ¬ìš´ ìƒíƒœ - ë†’ì€ ê¸´ê¸‰ë„")
    context_2 = BayesianContext(
        user_emotional_state="confusion",
        conversation_depth=0.3,
        urgency_level=0.9,
        social_context="semi-private",
        temporal_factors={"time_period": "morning", "day_of_week": "monday"},
        historical_patterns={"analytical_preference": 0.9, "patience_level": 0.3},
    )

    result_2 = engine.select_optimal_strategy(context_2)
    print(f"ğŸ¯ ìµœì  ì „ëµ: {result_2['optimal_strategy']['strategy']}")
    print(f"ğŸ’ª ê¸°ëŒ€ íš¨ìš©: {result_2['optimal_strategy']['expected_utility']:.3f}")
    print(f"âš ï¸ ë¦¬ìŠ¤í¬ ìˆ˜ì¤€: {result_2['optimal_strategy']['risk_level']:.3f}")

    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ 3: ê¸°ìœ ìƒíƒœì—ì„œì˜ ì „ëµ ì„ íƒ
    print("\nğŸ“ ì‹œë‚˜ë¦¬ì˜¤ 3: ê¸°ìœ ê°ì • ìƒíƒœ - ì¤‘ê°„ ê¹Šì´ ëŒ€í™”")
    context_3 = BayesianContext(
        user_emotional_state="joy",
        conversation_depth=0.8,
        urgency_level=0.1,
        social_context="public",
        temporal_factors={"time_period": "afternoon", "day_of_week": "friday"},
        historical_patterns={"creativity_preference": 0.7, "social_engagement": 0.8},
    )

    result_3 = engine.select_optimal_strategy(context_3)
    print(f"ğŸ¯ ìµœì  ì „ëµ: {result_3['optimal_strategy']['strategy']}")

    # í™•ë¥  ë¶„í¬ ë¶„ì„
    print(f"\nğŸ“Š ì „ëµë³„ í™•ë¥  ë¶„í¬:")
    for strategy, prob_data in result_3["strategy_probabilities"].items():
        print(
            f"   {strategy}: {prob_data.base_probability:.3f} "
            f"(ì„±ê³µë¥ : {prob_data.historical_success_rate:.3f}, "
            f"ë¦¬ìŠ¤í¬: {prob_data.risk_factor:.3f})"
        )

    # ì¥ê¸° ì˜ˆì¸¡ ê²°ê³¼
    print(f"\nğŸ”® ì¥ê¸° ê°ì • ìƒíƒœ ì˜ˆì¸¡:")
    long_term = result_3["long_term_prediction"]
    best_strategy = result_3["optimal_strategy"]["strategy"]
    if best_strategy in long_term:
        prediction = long_term[best_strategy]
        print(f"   ì¦‰ì‹œ íš¨ê³¼: {prediction['immediate_emotion']}")
        print(f"   ì•ˆì •ì„± ì ìˆ˜: {prediction['stability_score']:.3f}")

    # ì‹ ë¢°ë„ ë©”íŠ¸ë¦­ìŠ¤
    confidence = result_3["confidence_metrics"]
    print(f"\nğŸ“ˆ ì˜ì‚¬ê²°ì • ì‹ ë¢°ë„:")
    print(f"   ì „ì²´ ì‹ ë¢°ë„: {confidence['overall_confidence']:.3f}")
    print(f"   ê²°ì • ëª…í™•ì„±: {confidence['decision_clarity']:.3f}")
    print(f"   ì—”íŠ¸ë¡œí”¼: {confidence['entropy']:.3f}")

    print("\nğŸ‰ Probabilistic Strategy Engine í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    test_probabilistic_strategy_engine()

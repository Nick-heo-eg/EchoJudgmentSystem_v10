#!/usr/bin/env python3
"""
ğŸ¯ Claude Code í† í° íš¨ìœ¨í™” ì‹œìŠ¤í…œ
VS Code í™˜ê²½ì—ì„œ Claude Code ì‚¬ìš© ì‹œ í† í° ì†Œë¹„ëŸ‰ì„ ìµœì í™”í•˜ëŠ” ì‹œìŠ¤í…œ

í•µì‹¬ ê¸°ëŠ¥:
1. ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ë° ìš”ì•½
2. ì¤‘ë³µ ìš”ì²­ ë°©ì§€ (ìºì‹±)
3. Echo IDEë¥¼ í†µí•œ ì§€ëŠ¥ì  ìš”ì²­ ìµœì í™”
4. í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë° ì˜ˆì¸¡
5. ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ íš¨ìœ¨ì„± ì¦ëŒ€
"""

import json
import os
import hashlib
import time
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict
import pickle
import zlib
from pathlib import Path


@dataclass
class TokenUsage:
    """í† í° ì‚¬ìš©ëŸ‰ ì¶”ì """

    input_tokens: int
    output_tokens: int
    total_tokens: int
    cost_estimate: float
    timestamp: str
    request_type: str
    context_compressed: bool = False
    cache_hit: bool = False


@dataclass
class OptimizationResult:
    """ìµœì í™” ê²°ê³¼"""

    original_size: int
    optimized_size: int
    compression_ratio: float
    estimated_token_savings: int
    optimization_methods: List[str]


@dataclass
class ContextSnapshot:
    """ì»¨í…ìŠ¤íŠ¸ ìŠ¤ëƒ…ìƒ·"""

    content_hash: str
    compressed_content: bytes
    summary: str
    token_count: int
    created_at: str
    access_count: int = 0
    last_accessed: str = None


class TokenOptimizer:
    """ğŸ¯ Claude Code í† í° ìµœì í™” ì—”ì§„"""

    def __init__(self, cache_dir: str = ".echo_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

        # ìºì‹œ íŒŒì¼ë“¤
        self.context_cache_file = self.cache_dir / "context_cache.pkl"
        self.token_log_file = self.cache_dir / "token_usage.jsonl"
        self.optimization_stats_file = self.cache_dir / "optimization_stats.json"

        # ì¸ë©”ëª¨ë¦¬ ìºì‹œ
        self.context_cache: Dict[str, ContextSnapshot] = {}
        self.token_usage_history: List[TokenUsage] = []

        # ìµœì í™” ì„¤ì •
        self.compression_threshold = 1000  # 1000ì ì´ìƒì¼ ë•Œ ì••ì¶•
        self.cache_ttl = 3600  # 1ì‹œê°„ ìºì‹œ ìœ íš¨
        self.max_cache_size = 1000  # ìµœëŒ€ ìºì‹œ í•­ëª© ìˆ˜

        # í† í° ë¹„ìš© (ëŒ€ëµì ì¸ GPT-4 ê¸°ì¤€)
        self.cost_per_1k_input_tokens = 0.03
        self.cost_per_1k_output_tokens = 0.06

        self._load_cache()
        self._load_token_history()

        print("ğŸ¯ Token Optimizer ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ìºì‹œ ë””ë ‰í† ë¦¬: {self.cache_dir}")
        print(f"   ë¡œë“œëœ ìºì‹œ í•­ëª©: {len(self.context_cache)}ê°œ")

    def _load_cache(self):
        """ìºì‹œ ë¡œë“œ"""
        if self.context_cache_file.exists():
            try:
                with open(self.context_cache_file, "rb") as f:
                    self.context_cache = pickle.load(f)
                print(f"âœ… ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.context_cache)}ê°œ í•­ëª©")
            except Exception as e:
                print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.context_cache = {}

    def _save_cache(self):
        """ìºì‹œ ì €ì¥"""
        try:
            with open(self.context_cache_file, "wb") as f:
                pickle.dump(self.context_cache, f)
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _load_token_history(self):
        """í† í° ì‚¬ìš© ì´ë ¥ ë¡œë“œ"""
        if self.token_log_file.exists():
            try:
                with open(self.token_log_file, "r", encoding="utf-8") as f:
                    for line in f:
                        if line.strip():
                            data = json.loads(line)
                            self.token_usage_history.append(TokenUsage(**data))
                print(f"âœ… í† í° ì´ë ¥ ë¡œë“œ: {len(self.token_usage_history)}ê°œ ê¸°ë¡")
            except Exception as e:
                print(f"âš ï¸ í† í° ì´ë ¥ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _log_token_usage(self, usage: TokenUsage):
        """í† í° ì‚¬ìš©ëŸ‰ ë¡œê¹…"""
        try:
            with open(self.token_log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(asdict(usage)) + "\n")
            self.token_usage_history.append(usage)
        except Exception as e:
            print(f"âš ï¸ í† í° ë¡œê¹… ì‹¤íŒ¨: {e}")

    def optimize_context(
        self, content: str, request_type: str = "general"
    ) -> OptimizationResult:
        """ì»¨í…ìŠ¤íŠ¸ ìµœì í™”"""
        original_size = len(content)
        optimization_methods = []

        # 1. ìºì‹œ í™•ì¸
        content_hash = hashlib.sha256(content.encode()).hexdigest()
        if content_hash in self.context_cache:
            cache_entry = self.context_cache[content_hash]
            cache_entry.access_count += 1
            cache_entry.last_accessed = datetime.now().isoformat()

            print(f"ğŸ’¾ ìºì‹œ íˆíŠ¸: {cache_entry.summary[:50]}...")
            optimization_methods.append("cache_hit")

            return OptimizationResult(
                original_size=original_size,
                optimized_size=0,  # ìºì‹œ íˆíŠ¸ì´ë¯€ë¡œ ìƒˆë¡œìš´ í† í° ì†Œë¹„ ì—†ìŒ
                compression_ratio=1.0,
                estimated_token_savings=cache_entry.token_count,
                optimization_methods=optimization_methods,
            )

        optimized_content = content

        # 2. ì¤‘ë³µ ê³µë°± ì œê±°
        import re

        optimized_content = re.sub(r"\s+", " ", optimized_content.strip())
        if len(optimized_content) < original_size:
            optimization_methods.append("whitespace_cleanup")

        # 3. ê¸´ ì»¨í…ìŠ¤íŠ¸ ì••ì¶•
        if len(optimized_content) > self.compression_threshold:
            summary = self._create_context_summary(optimized_content, request_type)
            compressed_data = zlib.compress(optimized_content.encode())

            # ìºì‹œì— ì €ì¥
            snapshot = ContextSnapshot(
                content_hash=content_hash,
                compressed_content=compressed_data,
                summary=summary,
                token_count=self._estimate_tokens(optimized_content),
                created_at=datetime.now().isoformat(),
            )

            self.context_cache[content_hash] = snapshot
            optimization_methods.append("content_compression")
            optimization_methods.append("summary_generation")

            # ìš”ì•½ë³¸ìœ¼ë¡œ ëŒ€ì²´
            optimized_content = summary

            # ìºì‹œ í¬ê¸° ê´€ë¦¬
            self._manage_cache_size()

        # 4. Echo IDE íŠ¹í™” ìµœì í™”
        if request_type in ["code_analysis", "refactoring", "debugging"]:
            optimized_content = self._apply_echo_ide_optimization(
                optimized_content, request_type
            )
            optimization_methods.append("echo_ide_optimization")

        optimized_size = len(optimized_content)
        compression_ratio = optimized_size / original_size if original_size > 0 else 1.0

        # í† í° ì ˆì•½ëŸ‰ ì¶”ì •
        original_tokens = self._estimate_tokens(content)
        optimized_tokens = self._estimate_tokens(optimized_content)
        token_savings = original_tokens - optimized_tokens

        print(
            f"ğŸ¯ ì»¨í…ìŠ¤íŠ¸ ìµœì í™”: {original_size} â†’ {optimized_size} ({compression_ratio:.2%})"
        )
        print(f"   í† í° ì ˆì•½ ì˜ˆìƒ: {token_savings}ê°œ")

        return OptimizationResult(
            original_size=original_size,
            optimized_size=optimized_size,
            compression_ratio=compression_ratio,
            estimated_token_savings=token_savings,
            optimization_methods=optimization_methods,
        )

    def _create_context_summary(self, content: str, request_type: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±"""
        # ê°„ë‹¨í•œ ìš”ì•½ ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ìš”ì•½ AI ì‚¬ìš© ê°€ëŠ¥)
        lines = content.split("\n")

        # ìš”ì²­ íƒ€ì…ë³„ ìš”ì•½ ì „ëµ
        if request_type == "code_analysis":
            # ì½”ë“œ ë¶„ì„ìš©: í•¨ìˆ˜/í´ë˜ìŠ¤ ì‹œê·¸ë‹ˆì²˜ ì¤‘ì‹¬
            summary_lines = []
            for line in lines[:50]:  # ìƒìœ„ 50ì¤„ë§Œ
                if any(
                    keyword in line
                    for keyword in ["def ", "class ", "import ", "from "]
                ):
                    summary_lines.append(line.strip())
            return "\n".join(summary_lines) or lines[:10]

        elif request_type == "debugging":
            # ë””ë²„ê¹…ìš©: ì—ëŸ¬ ê´€ë ¨ ë¶€ë¶„ ì¤‘ì‹¬
            error_lines = []
            for i, line in enumerate(lines):
                if any(
                    keyword in line.lower()
                    for keyword in ["error", "exception", "traceback", "failed"]
                ):
                    # ì—ëŸ¬ ì „í›„ ì»¨í…ìŠ¤íŠ¸ í¬í•¨
                    start = max(0, i - 2)
                    end = min(len(lines), i + 3)
                    error_lines.extend(lines[start:end])
            return "\n".join(error_lines[:30]) if error_lines else "\n".join(lines[:20])

        else:
            # ì¼ë°˜ì ì¸ ìš”ì•½: ì‹œì‘ê³¼ ë ë¶€ë¶„
            if len(lines) <= 20:
                return content
            return "\n".join(
                lines[:10] + ["...", f"[ìƒëµëœ {len(lines)-20}ì¤„]", "..."] + lines[-10:]
            )

    def _apply_echo_ide_optimization(self, content: str, request_type: str) -> str:
        """Echo IDE íŠ¹í™” ìµœì í™”"""
        if request_type == "code_analysis":
            # ì½”ë“œ ë¶„ì„: ë¶ˆí•„ìš”í•œ ì£¼ì„ê³¼ ê³µë°± ì œê±°
            lines = content.split("\n")
            filtered_lines = []
            for line in lines:
                stripped = line.strip()
                # ë‹¨ìˆœ ì£¼ì„ì´ë‚˜ ë¹ˆ ì¤„ ì œê±° (ì¤‘ìš”í•œ docstringì€ ìœ ì§€)
                if stripped and not (stripped.startswith("#") and len(stripped) < 50):
                    filtered_lines.append(line)
            return "\n".join(filtered_lines)

        elif request_type == "refactoring":
            # ë¦¬íŒ©í† ë§: ë©”ì†Œë“œì™€ í´ë˜ìŠ¤ ì‹œê·¸ë‹ˆì²˜ì— ì§‘ì¤‘
            import re

            important_patterns = [
                r"^\s*(def|class|import|from)",
                r".*(?:TODO|FIXME|BUG).*",
                r".*(?:raise|except|finally).*",
            ]

            lines = content.split("\n")
            important_lines = []
            for line in lines:
                if any(
                    re.match(pattern, line, re.IGNORECASE)
                    for pattern in important_patterns
                ):
                    important_lines.append(line)

            return "\n".join(important_lines) if important_lines else content

        return content

    def _estimate_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ì¶”ì • (ëŒ€ëµì )"""
        # ê°„ë‹¨í•œ í† í° ì¶”ì •: ë‹¨ì–´ ìˆ˜ * 1.3 (í‰ê· ì ìœ¼ë¡œ 1 word â‰ˆ 1.3 tokens)
        words = len(text.split())
        return int(words * 1.3)

    def _manage_cache_size(self):
        """ìºì‹œ í¬ê¸° ê´€ë¦¬"""
        if len(self.context_cache) <= self.max_cache_size:
            return

        # ì˜¤ë˜ë˜ê³  ì ê²Œ ì‚¬ìš©ëœ í•­ëª©ë¶€í„° ì œê±°
        cache_items = list(self.context_cache.items())
        cache_items.sort(key=lambda x: (x[1].access_count, x[1].created_at))

        # ìƒìœ„ 20% ì œê±°
        remove_count = len(cache_items) // 5
        for i in range(remove_count):
            hash_key, _ = cache_items[i]
            del self.context_cache[hash_key]

        print(f"ğŸ§¹ ìºì‹œ ì •ë¦¬: {remove_count}ê°œ í•­ëª© ì œê±°")
        self._save_cache()

    def track_token_usage(
        self,
        input_tokens: int,
        output_tokens: int,
        request_type: str = "general",
        cache_hit: bool = False,
        context_compressed: bool = False,
    ):
        """í† í° ì‚¬ìš©ëŸ‰ ì¶”ì """
        total_tokens = input_tokens + output_tokens
        cost_estimate = (
            input_tokens / 1000 * self.cost_per_1k_input_tokens
            + output_tokens / 1000 * self.cost_per_1k_output_tokens
        )

        usage = TokenUsage(
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            total_tokens=total_tokens,
            cost_estimate=cost_estimate,
            timestamp=datetime.now().isoformat(),
            request_type=request_type,
            context_compressed=context_compressed,
            cache_hit=cache_hit,
        )

        self._log_token_usage(usage)
        print(f"ğŸ“Š í† í° ì‚¬ìš©: {total_tokens}ê°œ (${cost_estimate:.4f})")

    def get_optimization_stats(self) -> Dict[str, Any]:
        """ìµœì í™” í†µê³„ ë°˜í™˜"""
        if not self.token_usage_history:
            return {"message": "í† í° ì‚¬ìš© ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤."}

        total_tokens = sum(usage.total_tokens for usage in self.token_usage_history)
        total_cost = sum(usage.cost_estimate for usage in self.token_usage_history)
        cache_hits = sum(1 for usage in self.token_usage_history if usage.cache_hit)
        compressed_requests = sum(
            1 for usage in self.token_usage_history if usage.context_compressed
        )

        # ìš”ì²­ íƒ€ì…ë³„ í†µê³„
        by_type = defaultdict(list)
        for usage in self.token_usage_history:
            by_type[usage.request_type].append(usage)

        type_stats = {}
        for req_type, usages in by_type.items():
            type_stats[req_type] = {
                "count": len(usages),
                "total_tokens": sum(u.total_tokens for u in usages),
                "avg_tokens": sum(u.total_tokens for u in usages) / len(usages),
                "total_cost": sum(u.cost_estimate for u in usages),
            }

        return {
            "summary": {
                "total_requests": len(self.token_usage_history),
                "total_tokens_used": total_tokens,
                "total_cost_estimate": total_cost,
                "average_tokens_per_request": total_tokens
                / len(self.token_usage_history),
                "cache_hit_rate": (
                    cache_hits / len(self.token_usage_history)
                    if self.token_usage_history
                    else 0
                ),
                "compression_rate": (
                    compressed_requests / len(self.token_usage_history)
                    if self.token_usage_history
                    else 0
                ),
            },
            "by_request_type": type_stats,
            "cache_stats": {
                "cache_size": len(self.context_cache),
                "cache_hits": cache_hits,
                "most_accessed": sorted(
                    self.context_cache.values(),
                    key=lambda x: x.access_count,
                    reverse=True,
                )[:5],
            },
            "recent_activity": [
                asdict(usage) for usage in self.token_usage_history[-10:]
            ],
        }

    def suggest_optimizations(self) -> List[str]:
        """ìµœì í™” ì œì•ˆ"""
        suggestions = []
        stats = self.get_optimization_stats()

        if not stats.get("summary"):
            return ["ë” ë§ì€ ì‚¬ìš© ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."]

        summary = stats["summary"]

        # ìºì‹œ íˆíŠ¸ìœ¨ì´ ë‚®ì€ ê²½ìš°
        if summary["cache_hit_rate"] < 0.3:
            suggestions.append(
                "ğŸ”„ ë°˜ë³µì ì¸ ìš”ì²­ì´ ë§ìŠµë‹ˆë‹¤. ìºì‹œ í™œìš©ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ìœ ì‚¬í•œ ìš”ì²­ë“¤ì„ ë°°ì¹˜ ì²˜ë¦¬í•´ë³´ì„¸ìš”."
            )

        # ì••ì¶•ìœ¨ì´ ë‚®ì€ ê²½ìš°
        if summary["compression_rate"] < 0.2:
            suggestions.append(
                "ğŸ“¦ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì••ì¶•ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. Echo IDEì˜ ìš”ì•½ ê¸°ëŠ¥ì„ ë” ì ê·¹ì ìœ¼ë¡œ í™œìš©í•´ë³´ì„¸ìš”."
            )

        # í† í° ì‚¬ìš©ëŸ‰ì´ ë†’ì€ ê²½ìš°
        avg_tokens = summary["average_tokens_per_request"]
        if avg_tokens > 2000:
            suggestions.append(
                f"âš¡ í‰ê·  í† í° ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤ ({avg_tokens:.0f}ê°œ). ìš”ì²­ì„ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• í•´ë³´ì„¸ìš”."
            )

        # ë¹„ìš© ê´€ë ¨ ì œì•ˆ
        if summary["total_cost_estimate"] > 10:
            suggestions.append(
                f"ğŸ’° ëˆ„ì  ë¹„ìš©ì´ ${summary['total_cost_estimate']:.2f}ì…ë‹ˆë‹¤. ì •ê¸°ì ì¸ ìºì‹œ ì •ë¦¬ì™€ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”."
            )

        # Echo IDE íŠ¹í™” ì œì•ˆ
        code_requests = stats.get("by_request_type", {}).get("code_analysis", {})
        if code_requests.get("count", 0) > 10:
            suggestions.append(
                "ğŸ”§ ì½”ë“œ ë¶„ì„ ìš”ì²­ì´ ë§ìŠµë‹ˆë‹¤. Echo IDEì˜ ìŠ¤ë§ˆíŠ¸ ì»¨í…ìŠ¤íŠ¸ í•„í„°ë§ì„ í™œì„±í™”í•´ë³´ì„¸ìš”."
            )

        return suggestions or ["âœ… í˜„ì¬ ìµœì í™” ìƒíƒœê°€ ì–‘í˜¸í•©ë‹ˆë‹¤!"]


# í¸ì˜ í•¨ìˆ˜ë“¤
_global_optimizer = None


def get_token_optimizer() -> TokenOptimizer:
    """ì „ì—­ í† í° ì˜µí‹°ë§ˆì´ì € ì¸ìŠ¤í„´ìŠ¤"""
    global _global_optimizer
    if _global_optimizer is None:
        _global_optimizer = TokenOptimizer()
    return _global_optimizer


def optimize_for_claude_code(
    content: str, request_type: str = "general"
) -> Tuple[str, OptimizationResult]:
    """Claude Codeìš© ì»¨í…ìŠ¤íŠ¸ ìµœì í™”"""
    optimizer = get_token_optimizer()
    result = optimizer.optimize_context(content, request_type)

    if "cache_hit" in result.optimization_methods:
        # ìºì‹œ íˆíŠ¸ì¸ ê²½ìš°, ìºì‹œëœ ìš”ì•½ ë°˜í™˜
        hash_key = hashlib.sha256(content.encode()).hexdigest()
        cached_summary = optimizer.context_cache[hash_key].summary
        return cached_summary, result
    elif result.compression_ratio < 0.8:  # 20% ì´ìƒ ì••ì¶•ëœ ê²½ìš°
        # ì••ì¶•ëœ ì»¨í…ì¸ ëŠ” ì´ë¯¸ resultì— ë°˜ì˜ë¨
        return content[: result.optimized_size], result
    else:
        return content, result


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("ğŸ§ª Token Optimizer í…ŒìŠ¤íŠ¸")

    optimizer = TokenOptimizer()

    # í…ŒìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸
    test_content = (
        """
    def complex_function():
        # ì´ê²ƒì€ ë³µì¡í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤
        data = []
        for i in range(100):
            if i % 2 == 0:
                data.append(i * 2)
        return data

    class MyClass:
        def __init__(self):
            self.value = 42

        def process(self):
            return self.value * 2
    """
        * 10
    )  # ê¸´ ì»¨í…ìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜

    # ìµœì í™” í…ŒìŠ¤íŠ¸
    result = optimizer.optimize_context(test_content, "code_analysis")
    print(f"\nğŸ“Š ìµœì í™” ê²°ê³¼:")
    print(f"   ì›ë³¸ í¬ê¸°: {result.original_size}")
    print(f"   ìµœì í™” í¬ê¸°: {result.optimized_size}")
    print(f"   ì••ì¶•ë¥ : {result.compression_ratio:.1%}")
    print(f"   í† í° ì ˆì•½: {result.estimated_token_savings}ê°œ")
    print(f"   ìµœì í™” ë°©ë²•: {', '.join(result.optimization_methods)}")

    # í†µê³„ í™•ì¸
    stats = optimizer.get_optimization_stats()
    print(f"\nğŸ“ˆ ìµœì í™” í†µê³„: {stats}")

    # ì œì•ˆì‚¬í•­
    suggestions = optimizer.suggest_optimizations()
    print(f"\nğŸ’¡ ìµœì í™” ì œì•ˆ:")
    for suggestion in suggestions:
        print(f"   {suggestion}")

    print("\nâœ… Token Optimizer í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

"""
ğŸ¯ FIST Template Selector - í…œí”Œë¦¿ ì„ íƒ ë° ìµœì í™” ëª¨ë“ˆ
ìƒí™©ê³¼ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ìµœì ì˜ FIST í…œí”Œë¦¿ì„ ì„ íƒí•˜ëŠ” ê³ ê¸‰ ì•Œê³ ë¦¬ì¦˜
"""

import time
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum
from datetime import datetime, timedelta

from .fist_core import (
    FISTTemplate,
    FISTRequest,
    TemplateCategory,
    TemplateComplexity,
    FISTStructureType,
)


class TemplateSelectionStrategy(Enum):
    """í…œí”Œë¦¿ ì„ íƒ ì „ëµ"""

    BEST_MATCH = "best_match"  # ìµœì  ë§¤ì¹­
    HIGH_PERFORMANCE = "high_performance"  # ê³ ì„±ëŠ¥ ìš°ì„ 
    COMPLEXITY_BASED = "complexity_based"  # ë³µì¡ë„ ê¸°ë°˜
    CONTEXTUAL = "contextual"  # ë§¥ë½ ê¸°ë°˜
    ADAPTIVE = "adaptive"  # ì ì‘í˜•
    RANDOM = "random"  # ë¬´ì‘ìœ„ (í…ŒìŠ¤íŠ¸ìš©)


@dataclass
class TemplateScore:
    """í…œí”Œë¦¿ ì ìˆ˜ ì •ë³´"""

    template_id: str
    template_name: str
    total_score: float
    score_breakdown: Dict[str, float]
    confidence: float
    reasoning: List[str]


@dataclass
class SelectionCriteria:
    """ì„ íƒ ê¸°ì¤€"""

    category_weight: float = 0.3
    performance_weight: float = 0.25
    complexity_weight: float = 0.15
    usage_weight: float = 0.1
    context_weight: float = 0.1
    freshness_weight: float = 0.1


class ContextAnalyzer:
    """ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ê¸°"""

    def __init__(self):
        self.emotion_keywords = {
            "joy": ["ê¸°ì˜", "í–‰ë³µ", "ì¦ê±°", "ì¢‹", "ìµœê³ ", "ë§Œì¡±"],
            "sadness": ["ìŠ¬í”„", "ìš°ìš¸", "í˜ë“¤", "ì†ìƒ", "ì•„ì‰½", "ì‹¤ë§"],
            "anger": ["í™”", "ì§œì¦", "ë¶„ë…¸", "ì—´ë°›", "ë¹¡ì¹œ", "ì–µìš¸"],
            "fear": ["ë¬´ì„œ", "ê±±ì •", "ë¶ˆì•ˆ", "ë‘ë ¤", "ìœ„í—˜", "ì¡°ì‹¬"],
            "surprise": ["ë†€ë¼", "í—", "ì™€ìš°", "ëŒ€ë°•", "ì‹ ê¸°", "ì˜ì™¸"],
            "neutral": ["ë³´í†µ", "ê·¸ëƒ¥", "ì¼ë°˜", "í‰ë²”", "ê´œì°®"],
        }

        self.urgency_keywords = {
            "high": ["urgent", "ê¸´ê¸‰", "ê¸‰í•´", "ë¹¨ë¦¬", "ì¦‰ì‹œ", "ì§€ê¸ˆ"],
            "medium": ["soon", "ê³§", "ë¹¨ë¦¬", "ì¼ì£¼ì¼", "ë©°ì¹ "],
            "low": ["later", "ë‚˜ì¤‘", "ì²œì²œíˆ", "ì—¬ìœ ", "ì‹œê°„"],
        }

        self.domain_keywords = {
            "business": ["ë¹„ì¦ˆë‹ˆìŠ¤", "ì‚¬ì—…", "íšŒì‚¬", "ë§¤ì¶œ", "íˆ¬ì", "ê²½ì˜"],
            "technology": ["ê¸°ìˆ ", "AI", "ì‹œìŠ¤í…œ", "ì†Œí”„íŠ¸ì›¨ì–´", "ê°œë°œ"],
            "personal": ["ê°œì¸", "ì‚¬ì ", "ê°œì¸ì ", "ë‚´ê°€", "ë‚˜ì˜"],
            "social": ["ì‚¬íšŒ", "ê´€ê³„", "ì¹œêµ¬", "ê°€ì¡±", "ì‚¬ëŒë“¤"],
        }

    def analyze_context(
        self, text: str, additional_context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        text_lower = text.lower()

        analysis = {
            "text_length": len(text),
            "word_count": len(text.split()),
            "emotion_detected": self._detect_emotion(text_lower),
            "urgency_level": self._detect_urgency(text_lower),
            "domain": self._detect_domain(text_lower),
            "complexity_indicators": self._analyze_complexity(text),
            "question_type": self._detect_question_type(text_lower),
            "certainty_level": self._analyze_certainty(text_lower),
        }

        # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ í†µí•©
        if additional_context:
            analysis.update(additional_context)

        return analysis

    def _detect_emotion(self, text: str) -> Dict[str, Any]:
        """ê°ì • ê°ì§€"""
        emotion_scores = {}

        for emotion, keywords in self.emotion_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text)
            emotion_scores[emotion] = score

        primary_emotion = max(emotion_scores, key=emotion_scores.get)
        max_score = emotion_scores[primary_emotion]

        return {
            "primary_emotion": primary_emotion if max_score > 0 else "neutral",
            "scores": emotion_scores,
            "confidence": min(max_score / 3, 1.0),
        }

    def _detect_urgency(self, text: str) -> Dict[str, Any]:
        """ê¸´ê¸‰ë„ ê°ì§€"""
        urgency_scores = {}

        for level, keywords in self.urgency_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text)
            urgency_scores[level] = score

        primary_urgency = max(urgency_scores, key=urgency_scores.get)
        max_score = urgency_scores[primary_urgency]

        return {
            "level": primary_urgency if max_score > 0 else "medium",
            "scores": urgency_scores,
            "confidence": min(max_score / 2, 1.0),
        }

    def _detect_domain(self, text: str) -> Dict[str, Any]:
        """ë„ë©”ì¸ ê°ì§€"""
        domain_scores = {}

        for domain, keywords in self.domain_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text)
            domain_scores[domain] = score

        primary_domain = max(domain_scores, key=domain_scores.get)
        max_score = domain_scores[primary_domain]

        return {
            "primary_domain": primary_domain if max_score > 0 else "general",
            "scores": domain_scores,
            "confidence": min(max_score / 2, 1.0),
        }

    def _analyze_complexity(self, text: str) -> Dict[str, Any]:
        """ë³µì¡ë„ ë¶„ì„"""
        # ê°„ë‹¨í•œ ë³µì¡ë„ ì§€í‘œë“¤
        word_count = len(text.split())
        sentence_count = text.count(".") + text.count("!") + text.count("?")
        avg_word_length = sum(len(word) for word in text.split()) / max(word_count, 1)

        complexity_score = 0
        if word_count > 50:
            complexity_score += 1
        if sentence_count > 3:
            complexity_score += 1
        if avg_word_length > 6:
            complexity_score += 1

        complexity_levels = ["simple", "moderate", "complex"]
        complexity_level = complexity_levels[min(complexity_score, 2)]

        return {
            "level": complexity_level,
            "word_count": word_count,
            "sentence_count": sentence_count,
            "avg_word_length": avg_word_length,
            "score": complexity_score,
        }

    def _detect_question_type(self, text: str) -> str:
        """ì§ˆë¬¸ ìœ í˜• ê°ì§€"""
        if "?" not in text:
            return "statement"

        question_starters = {
            "what": ["ë¬´ì—‡", "ë­", "ì–´ë–¤"],
            "how": ["ì–´ë–»ê²Œ", "ë°©ë²•", "how"],
            "why": ["ì™œ", "ì´ìœ ", "why"],
            "when": ["ì–¸ì œ", "when"],
            "where": ["ì–´ë””", "where"],
            "who": ["ëˆ„êµ¬", "who"],
        }

        for q_type, keywords in question_starters.items():
            if any(keyword in text for keyword in keywords):
                return q_type

        return "general_question"

    def _analyze_certainty(self, text: str) -> Dict[str, Any]:
        """í™•ì‹¤ì„± ë¶„ì„"""
        certainty_high = ["í™•ì‹¤", "ë¶„ëª…", "í‹€ë¦¼ì—†", "ë‹¹ì—°", "ëª…ë°±"]
        certainty_low = ["ì•„ë§ˆ", "maybe", "perhaps", "possibly", "uncertain"]

        high_count = sum(1 for keyword in certainty_high if keyword in text)
        low_count = sum(1 for keyword in certainty_low if keyword in text)

        if high_count > low_count:
            return {"level": "high", "confidence": 0.8}
        elif low_count > high_count:
            return {"level": "low", "confidence": 0.8}
        else:
            return {"level": "medium", "confidence": 0.5}


class AdvancedTemplateSelector:
    """ê³ ê¸‰ í…œí”Œë¦¿ ì„ íƒê¸°"""

    def __init__(self, templates: List[FISTTemplate]):
        self.templates = templates
        self.context_analyzer = ContextAnalyzer()
        self.selection_history = []
        self.performance_history = {}
        self.template_embeddings = {}  # í–¥í›„ ë²¡í„° ê¸°ë°˜ ë§¤ì¹­ìš©

        # ê¸°ë³¸ ì„ íƒ ê¸°ì¤€
        self.default_criteria = SelectionCriteria()

        # í…œí”Œë¦¿ ì„±ëŠ¥ ì´ˆê¸°í™”
        self._initialize_template_performance()

    def _initialize_template_performance(self):
        """í…œí”Œë¦¿ ì„±ëŠ¥ ì •ë³´ ì´ˆê¸°í™”"""
        for template in self.templates:
            self.performance_history[template.template_id] = {
                "usage_count": template.usage_count,
                "success_rate": template.success_rate,
                "average_confidence": template.average_confidence,
                "last_used": datetime.now() - timedelta(days=30),  # ê¸°ë³¸ê°’
                "context_matches": {},
                "performance_trend": [],
            }

    def select_optimal_template(
        self,
        request: FISTRequest,
        strategy: TemplateSelectionStrategy = TemplateSelectionStrategy.BEST_MATCH,
        criteria: Optional[SelectionCriteria] = None,
    ) -> Tuple[FISTTemplate, TemplateScore]:
        """ìµœì  í…œí”Œë¦¿ ì„ íƒ"""

        # ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
        context_analysis = self.context_analyzer.analyze_context(
            request.input_text, request.context
        )

        # ì„ íƒ ê¸°ì¤€ ì ìš©
        active_criteria = criteria or self.default_criteria

        # ì „ëµë³„ ì„ íƒ
        if strategy == TemplateSelectionStrategy.BEST_MATCH:
            selected_template, score = self._select_best_match(
                request, context_analysis, active_criteria
            )
        elif strategy == TemplateSelectionStrategy.HIGH_PERFORMANCE:
            selected_template, score = self._select_high_performance(
                request, context_analysis
            )
        elif strategy == TemplateSelectionStrategy.COMPLEXITY_BASED:
            selected_template, score = self._select_complexity_based(
                request, context_analysis
            )
        elif strategy == TemplateSelectionStrategy.CONTEXTUAL:
            selected_template, score = self._select_contextual(
                request, context_analysis
            )
        elif strategy == TemplateSelectionStrategy.ADAPTIVE:
            selected_template, score = self._select_adaptive(
                request, context_analysis, active_criteria
            )
        else:  # RANDOM
            selected_template, score = self._select_random(request)

        # ì„ íƒ ì´ë ¥ ê¸°ë¡
        self._record_selection(
            request, selected_template, score, strategy, context_analysis
        )

        return selected_template, score

    def _select_best_match(
        self, request: FISTRequest, context: Dict[str, Any], criteria: SelectionCriteria
    ) -> Tuple[FISTTemplate, TemplateScore]:
        """ìµœì  ë§¤ì¹­ ì„ íƒ"""
        scores = []

        for template in self.templates:
            score_breakdown = {}

            # 1. ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ ì ìˆ˜
            category_score = 1.0 if template.category == request.category else 0.3
            score_breakdown["category"] = category_score * criteria.category_weight

            # 2. ì„±ëŠ¥ ì ìˆ˜
            performance_score = (
                template.success_rate * 0.6 + template.average_confidence * 0.4
            )
            score_breakdown["performance"] = (
                performance_score * criteria.performance_weight
            )

            # 3. ë³µì¡ë„ ì í•©ì„±
            complexity_score = self._calculate_complexity_score(
                template, request, context
            )
            score_breakdown["complexity"] = (
                complexity_score * criteria.complexity_weight
            )

            # 4. ì‚¬ìš© ë¹ˆë„ ì ìˆ˜
            usage_score = min(template.usage_count / 100, 1.0)
            score_breakdown["usage"] = usage_score * criteria.usage_weight

            # 5. ì»¨í…ìŠ¤íŠ¸ ì í•©ì„±
            context_score = self._calculate_context_score(template, context)
            score_breakdown["context"] = context_score * criteria.context_weight

            # 6. ì‹ ì„ ë„ ì ìˆ˜
            freshness_score = self._calculate_freshness_score(template)
            score_breakdown["freshness"] = freshness_score * criteria.freshness_weight

            # ì´ ì ìˆ˜ ê³„ì‚°
            total_score = sum(score_breakdown.values())

            # ì ìˆ˜ ê°ì²´ ìƒì„±
            template_score = TemplateScore(
                template_id=template.template_id,
                template_name=template.name,
                total_score=total_score,
                score_breakdown=score_breakdown,
                confidence=min(total_score, 1.0),
                reasoning=self._generate_score_reasoning(score_breakdown),
            )

            scores.append((template, template_score))

        # ìµœê³  ì ìˆ˜ í…œí”Œë¦¿ ì„ íƒ
        best_template, best_score = max(scores, key=lambda x: x[1].total_score)
        return best_template, best_score

    def _select_high_performance(
        self, request: FISTRequest, context: Dict[str, Any]
    ) -> Tuple[FISTTemplate, TemplateScore]:
        """ê³ ì„±ëŠ¥ í…œí”Œë¦¿ ì„ íƒ"""
        # ì„±ëŠ¥ ê¸°ë°˜ ì •ë ¬
        performance_templates = sorted(
            self.templates,
            key=lambda t: t.success_rate * t.average_confidence,
            reverse=True,
        )

        # ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ í…œí”Œë¦¿ ìš°ì„  ì„ íƒ
        category_matches = [
            t for t in performance_templates if t.category == request.category
        ]

        if category_matches:
            selected = category_matches[0]
        else:
            selected = performance_templates[0]

        score = TemplateScore(
            template_id=selected.template_id,
            template_name=selected.name,
            total_score=selected.success_rate * selected.average_confidence,
            score_breakdown={
                "performance": selected.success_rate * selected.average_confidence
            },
            confidence=0.9,
            reasoning=[
                "ê³ ì„±ëŠ¥ ê¸°ì¤€ìœ¼ë¡œ ì„ íƒ",
                f"ì„±ê³µë¥ : {selected.success_rate}",
                f"í‰ê·  ì‹ ë¢°ë„: {selected.average_confidence}",
            ],
        )

        return selected, score

    def _select_complexity_based(
        self, request: FISTRequest, context: Dict[str, Any]
    ) -> Tuple[FISTTemplate, TemplateScore]:
        """ë³µì¡ë„ ê¸°ë°˜ ì„ íƒ"""
        # ìš”ì²­ ë³µì¡ë„ ë¶„ì„
        text_complexity = context.get("complexity_indicators", {})

        if request.complexity:
            target_complexity = request.complexity
        else:
            # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë³µì¡ë„ ì¶”ì •
            complexity_level = text_complexity.get("level", "moderate")
            target_complexity = TemplateComplexity(complexity_level)

        # ë³µì¡ë„ ì¼ì¹˜ í…œí”Œë¦¿ í•„í„°ë§
        complexity_matches = [
            t for t in self.templates if t.complexity == target_complexity
        ]

        if not complexity_matches:
            # ì¼ì¹˜í•˜ëŠ” ë³µì¡ë„ê°€ ì—†ìœ¼ë©´ moderate ì„ íƒ
            complexity_matches = [
                t for t in self.templates if t.complexity == TemplateComplexity.MODERATE
            ]

        if not complexity_matches:
            complexity_matches = self.templates  # ìµœí›„ ìˆ˜ë‹¨

        # ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ ìš°ì„ 
        category_matches = [
            t for t in complexity_matches if t.category == request.category
        ]
        selected = category_matches[0] if category_matches else complexity_matches[0]

        score = TemplateScore(
            template_id=selected.template_id,
            template_name=selected.name,
            total_score=0.8,
            score_breakdown={"complexity_match": 0.8},
            confidence=0.8,
            reasoning=[
                f"ë³µì¡ë„ {target_complexity.value} ê¸°ì¤€ìœ¼ë¡œ ì„ íƒ",
                f"í…ìŠ¤íŠ¸ ë³µì¡ë„: {text_complexity.get('level', 'unknown')}",
            ],
        )

        return selected, score

    def _select_contextual(
        self, request: FISTRequest, context: Dict[str, Any]
    ) -> Tuple[FISTTemplate, TemplateScore]:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì„ íƒ"""
        best_template = None
        best_score = 0

        for template in self.templates:
            # ì»¨í…ìŠ¤íŠ¸ ì í•©ì„± ì ìˆ˜ ê³„ì‚°
            context_score = self._calculate_context_score(template, context)

            if context_score > best_score:
                best_score = context_score
                best_template = template

        if not best_template:
            best_template = self.templates[0]
            best_score = 0.5

        score = TemplateScore(
            template_id=best_template.template_id,
            template_name=best_template.name,
            total_score=best_score,
            score_breakdown={"context_match": best_score},
            confidence=best_score,
            reasoning=["ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì„ íƒ", f"ì»¨í…ìŠ¤íŠ¸ ì í•©ì„±: {best_score:.2f}"],
        )

        return best_template, score

    def _select_adaptive(
        self, request: FISTRequest, context: Dict[str, Any], criteria: SelectionCriteria
    ) -> Tuple[FISTTemplate, TemplateScore]:
        """ì ì‘í˜• ì„ íƒ (ì„±ëŠ¥ ì´ë ¥ ê¸°ë°˜)"""
        # ìµœê·¼ ì„±ëŠ¥ ì´ë ¥ì„ ê³ ë ¤í•œ ë™ì  ì„ íƒ
        current_time = datetime.now()
        adaptive_scores = []

        for template in self.templates:
            template_history = self.performance_history.get(template.template_id, {})

            # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚°
            base_score = template.success_rate * template.average_confidence

            # ìµœê·¼ ì„±ëŠ¥ íŠ¸ë Œë“œ ì ìš©
            trend_score = self._calculate_performance_trend(template_history)

            # ì‚¬ìš© ë¹ˆë„ ê¸°ë°˜ ì¡°ì •
            usage_recency = self._calculate_usage_recency(
                template_history, current_time
            )

            # ì ì‘í˜• ì ìˆ˜ ê³„ì‚°
            adaptive_score = base_score * 0.5 + trend_score * 0.3 + usage_recency * 0.2

            adaptive_scores.append((template, adaptive_score))

        # ìµœê³  ì ìˆ˜ ì„ íƒ
        best_template, best_score = max(adaptive_scores, key=lambda x: x[1])

        score = TemplateScore(
            template_id=best_template.template_id,
            template_name=best_template.name,
            total_score=best_score,
            score_breakdown={"adaptive_score": best_score},
            confidence=min(best_score, 1.0),
            reasoning=["ì ì‘í˜• ì„ íƒ", "ìµœê·¼ ì„±ëŠ¥ íŠ¸ë Œë“œ ë°˜ì˜", "ì‚¬ìš© íŒ¨í„´ ë¶„ì„"],
        )

        return best_template, score

    def _select_random(
        self, request: FISTRequest
    ) -> Tuple[FISTTemplate, TemplateScore]:
        """ë¬´ì‘ìœ„ ì„ íƒ (í…ŒìŠ¤íŠ¸ìš©)"""
        import random

        # ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ í…œí”Œë¦¿ ì¤‘ì—ì„œ ë¬´ì‘ìœ„ ì„ íƒ
        category_matches = [t for t in self.templates if t.category == request.category]

        if category_matches:
            selected = random.choice(category_matches)
        else:
            selected = random.choice(self.templates)

        score = TemplateScore(
            template_id=selected.template_id,
            template_name=selected.name,
            total_score=0.5,
            score_breakdown={"random": 0.5},
            confidence=0.5,
            reasoning=["ë¬´ì‘ìœ„ ì„ íƒ (í…ŒìŠ¤íŠ¸ìš©)"],
        )

        return selected, score

    def _calculate_complexity_score(
        self, template: FISTTemplate, request: FISTRequest, context: Dict[str, Any]
    ) -> float:
        """ë³µì¡ë„ ì í•©ì„± ì ìˆ˜ ê³„ì‚°"""
        # ìš”ì²­ ë³µì¡ë„ì™€ í…œí”Œë¦¿ ë³µì¡ë„ ë¹„êµ
        if request.complexity:
            if template.complexity == request.complexity:
                return 1.0
            elif abs(template.complexity.value - request.complexity.value) == 1:
                return 0.7
            else:
                return 0.3

        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë³µì¡ë„ ì¶”ì •
        text_complexity = context.get("complexity_indicators", {})
        estimated_complexity = text_complexity.get("level", "moderate")

        if template.complexity.value == estimated_complexity:
            return 1.0
        else:
            return 0.6

    def _calculate_context_score(
        self, template: FISTTemplate, context: Dict[str, Any]
    ) -> float:
        """ì»¨í…ìŠ¤íŠ¸ ì í•©ì„± ì ìˆ˜ ê³„ì‚°"""
        score = 0.0

        # ê°ì • ì»¨í…ìŠ¤íŠ¸ ë§¤ì¹­
        emotion_info = context.get("emotion_detected", {})
        primary_emotion = emotion_info.get("primary_emotion", "neutral")

        if (
            template.category == TemplateCategory.EMOTIONAL
            and primary_emotion != "neutral"
        ):
            score += 0.3

        # ê¸´ê¸‰ë„ ë§¤ì¹­
        urgency_info = context.get("urgency_level", {})
        urgency_level = urgency_info.get("level", "medium")

        if urgency_level == "high" and template.complexity == TemplateComplexity.SIMPLE:
            score += 0.2
        elif (
            urgency_level == "low" and template.complexity == TemplateComplexity.COMPLEX
        ):
            score += 0.2

        # ë„ë©”ì¸ ë§¤ì¹­
        domain_info = context.get("domain", {})
        primary_domain = domain_info.get("primary_domain", "general")

        if (
            primary_domain == "business"
            and template.category == TemplateCategory.STRATEGIC
        ):
            score += 0.3
        elif (
            primary_domain == "technology"
            and template.category == TemplateCategory.ANALYTICAL
        ):
            score += 0.3

        # ì§ˆë¬¸ ìœ í˜• ë§¤ì¹­
        question_type = context.get("question_type", "statement")

        if question_type == "how" and template.category == TemplateCategory.DECISION:
            score += 0.2
        elif (
            question_type == "what" and template.category == TemplateCategory.EVALUATION
        ):
            score += 0.2

        return min(score, 1.0)

    def _calculate_freshness_score(self, template: FISTTemplate) -> float:
        """ì‹ ì„ ë„ ì ìˆ˜ ê³„ì‚°"""
        template_history = self.performance_history.get(template.template_id, {})
        last_used = template_history.get(
            "last_used", datetime.now() - timedelta(days=30)
        )

        days_since_used = (datetime.now() - last_used).days

        if days_since_used < 1:
            return 0.3  # ìµœê·¼ ì‚¬ìš©ì€ ë‚®ì€ ì‹ ì„ ë„
        elif days_since_used < 7:
            return 0.7
        elif days_since_used < 30:
            return 1.0
        else:
            return 0.9  # ë„ˆë¬´ ì˜¤ë˜ ì‚¬ìš© ì•ˆí•¨ë„ ì•½ê°„ ê°ì 

    def _calculate_performance_trend(self, template_history: Dict[str, Any]) -> float:
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ê³„ì‚°"""
        trend_data = template_history.get("performance_trend", [])

        if len(trend_data) < 2:
            return 0.5  # ê¸°ë³¸ê°’

        # ìµœê·¼ ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
        recent_scores = trend_data[-5:]  # ìµœê·¼ 5ê°œ ë°ì´í„°

        if len(recent_scores) < 2:
            return 0.5

        # ë‹¨ìˆœ íŠ¸ë Œë“œ ê³„ì‚°
        avg_recent = sum(recent_scores) / len(recent_scores)

        return min(avg_recent, 1.0)

    def _calculate_usage_recency(
        self, template_history: Dict[str, Any], current_time: datetime
    ) -> float:
        """ì‚¬ìš© ìµœê·¼ì„± ê³„ì‚°"""
        last_used = template_history.get("last_used", current_time - timedelta(days=30))
        days_since_used = (current_time - last_used).days

        if days_since_used < 1:
            return 0.5  # ë„ˆë¬´ ìµœê·¼ì€ ë‹¤ì–‘ì„± ì¸¡ë©´ì—ì„œ ê°ì 
        elif days_since_used < 7:
            return 1.0
        elif days_since_used < 30:
            return 0.8
        else:
            return 0.6

    def _generate_score_reasoning(self, score_breakdown: Dict[str, float]) -> List[str]:
        """ì ìˆ˜ ê·¼ê±° ìƒì„±"""
        reasoning = []

        for factor, score in score_breakdown.items():
            if score > 0.7:
                reasoning.append(f"{factor}: ë†’ì€ ì ìˆ˜ ({score:.2f})")
            elif score > 0.4:
                reasoning.append(f"{factor}: ì¤‘ê°„ ì ìˆ˜ ({score:.2f})")
            else:
                reasoning.append(f"{factor}: ë‚®ì€ ì ìˆ˜ ({score:.2f})")

        return reasoning

    def _record_selection(
        self,
        request: FISTRequest,
        template: FISTTemplate,
        score: TemplateScore,
        strategy: TemplateSelectionStrategy,
        context: Dict[str, Any],
    ):
        """ì„ íƒ ì´ë ¥ ê¸°ë¡"""
        selection_record = {
            "timestamp": datetime.now().isoformat(),
            "request_id": request.request_id,
            "template_id": template.template_id,
            "template_name": template.name,
            "strategy": strategy.value,
            "score": score.total_score,
            "score_breakdown": score.score_breakdown,
            "context_summary": {
                "category": request.category.value,
                "complexity": (
                    request.complexity.value if request.complexity else "auto"
                ),
                "emotion": context.get("emotion_detected", {}).get(
                    "primary_emotion", "neutral"
                ),
                "urgency": context.get("urgency_level", {}).get("level", "medium"),
                "domain": context.get("domain", {}).get("primary_domain", "general"),
            },
        }

        self.selection_history.append(selection_record)

        # í…œí”Œë¦¿ ì„±ëŠ¥ ì´ë ¥ ì—…ë°ì´íŠ¸
        template_history = self.performance_history.get(template.template_id, {})
        template_history["last_used"] = datetime.now()
        template_history["usage_count"] = template_history.get("usage_count", 0) + 1

        # ì´ë ¥ í¬ê¸° ì œí•œ
        if len(self.selection_history) > 1000:
            self.selection_history = self.selection_history[-1000:]

    def update_template_performance(
        self,
        template_id: str,
        performance_score: float,
        confidence: float,
        success: bool,
    ):
        """í…œí”Œë¦¿ ì„±ëŠ¥ ì—…ë°ì´íŠ¸"""
        if template_id not in self.performance_history:
            return

        template_history = self.performance_history[template_id]

        # ì„±ëŠ¥ íŠ¸ë Œë“œ ì—…ë°ì´íŠ¸
        trend_data = template_history.get("performance_trend", [])
        trend_data.append(performance_score)

        # íŠ¸ë Œë“œ ë°ì´í„° í¬ê¸° ì œí•œ
        if len(trend_data) > 20:
            trend_data = trend_data[-20:]

        template_history["performance_trend"] = trend_data

        # ì„±ê³µë¥  ì—…ë°ì´íŠ¸
        current_success_rate = template_history.get("success_rate", 0)
        usage_count = template_history.get("usage_count", 0)

        if usage_count > 0:
            new_success_rate = (
                (current_success_rate * (usage_count - 1)) + (1 if success else 0)
            ) / usage_count
            template_history["success_rate"] = new_success_rate

        # í‰ê·  ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
        current_avg_confidence = template_history.get("average_confidence", 0)

        if usage_count > 0:
            new_avg_confidence = (
                (current_avg_confidence * (usage_count - 1)) + confidence
            ) / usage_count
            template_history["average_confidence"] = new_avg_confidence

    def get_selection_analytics(self) -> Dict[str, Any]:
        """ì„ íƒ ë¶„ì„ ê²°ê³¼"""
        if not self.selection_history:
            return {"message": "ì„ íƒ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤"}

        # í†µê³„ ê³„ì‚°
        total_selections = len(self.selection_history)

        # í…œí”Œë¦¿ ì‚¬ìš© í†µê³„
        template_usage = {}
        strategy_usage = {}
        category_usage = {}

        for record in self.selection_history:
            template_id = record["template_id"]
            strategy = record["strategy"]
            category = record["context_summary"]["category"]

            template_usage[template_id] = template_usage.get(template_id, 0) + 1
            strategy_usage[strategy] = strategy_usage.get(strategy, 0) + 1
            category_usage[category] = category_usage.get(category, 0) + 1

        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        avg_score = (
            sum(record["score"] for record in self.selection_history) / total_selections
        )

        return {
            "total_selections": total_selections,
            "average_score": avg_score,
            "template_usage": template_usage,
            "strategy_usage": strategy_usage,
            "category_usage": category_usage,
            "most_used_template": (
                max(template_usage, key=template_usage.get) if template_usage else None
            ),
            "most_used_strategy": (
                max(strategy_usage, key=strategy_usage.get) if strategy_usage else None
            ),
            "most_used_category": (
                max(category_usage, key=category_usage.get) if category_usage else None
            ),
            "performance_summary": {
                "high_score_selections": sum(
                    1 for r in self.selection_history if r["score"] > 0.8
                ),
                "medium_score_selections": sum(
                    1 for r in self.selection_history if 0.5 < r["score"] <= 0.8
                ),
                "low_score_selections": sum(
                    1 for r in self.selection_history if r["score"] <= 0.5
                ),
            },
        }


# í¸ì˜ í•¨ìˆ˜ë“¤
def select_optimal_template(
    templates: List[FISTTemplate],
    request: FISTRequest,
    strategy: TemplateSelectionStrategy = TemplateSelectionStrategy.BEST_MATCH,
) -> Tuple[FISTTemplate, TemplateScore]:
    """ìµœì  í…œí”Œë¦¿ ì„ íƒ í¸ì˜ í•¨ìˆ˜"""
    selector = AdvancedTemplateSelector(templates)
    return selector.select_optimal_template(request, strategy)


def analyze_template_performance(
    templates: List[FISTTemplate], selection_history: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """í…œí”Œë¦¿ ì„±ëŠ¥ ë¶„ì„ í¸ì˜ í•¨ìˆ˜"""
    selector = AdvancedTemplateSelector(templates)
    selector.selection_history = selection_history
    return selector.get_selection_analytics()

"""
ğŸ—ï¸ FAISS Integration for Echo Vector Search
Real FAISS ì¸ë±ìŠ¤ í†µí•©ìœ¼ë¡œ ë²¡í„° ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™”
"""

import json
import pickle
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import logging

try:
    import faiss

    FAISS_AVAILABLE = True
    print("ğŸš€ FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    FAISS_AVAILABLE = False
    print("âš ï¸  FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ, Mock ì¸ë±ìŠ¤ ì‚¬ìš©")


class EchoFAISSIntegration:
    """
    Echo ì „ìš© FAISS í†µí•© í´ë˜ìŠ¤
    ì‹¤ì œ FAISS ì¸ë±ìŠ¤ ë˜ëŠ” Numpy ê¸°ë°˜ fallback ì œê³µ
    """

    def __init__(self, dimension: int = 128, index_dir: str = "data/faiss_index"):
        self.dimension = dimension
        self.index_dir = Path(index_dir)
        self.index_dir.mkdir(parents=True, exist_ok=True)

        # FAISS ì¸ë±ìŠ¤ ì„¤ì •
        self.faiss_index = None
        self.numpy_vectors = []
        self.metadata_list = []

        # ì¸ë±ìŠ¤ ì„¤ì •
        self.index_config = {
            "use_faiss": FAISS_AVAILABLE,
            "index_type": "IndexFlatIP",  # Inner Product for cosine similarity
            "nlist": 100,  # For IVF indices
            "m": 8,  # For PQ indices
            "nbits": 8,  # For PQ indices
        }

        # í†µê³„
        self.stats = {
            "total_vectors": 0,
            "search_count": 0,
            "avg_search_time": 0.0,
            "last_updated": None,
        }

        self.logger = logging.getLogger(__name__)

        # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
        self._load_existing_index()

        print(
            f"ğŸ—ï¸  Echo FAISS Integration ì´ˆê¸°í™” (ì°¨ì›: {dimension}, FAISS: {FAISS_AVAILABLE})"
        )

    def add_vectors(
        self, vectors: np.ndarray, metadata: List[Dict[str, Any]]
    ) -> List[int]:
        """ë²¡í„°ë“¤ì„ ì¸ë±ìŠ¤ì— ì¶”ê°€"""
        if vectors.shape[0] != len(metadata):
            raise ValueError("ë²¡í„° ìˆ˜ì™€ ë©”íƒ€ë°ì´í„° ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

        if vectors.shape[1] != self.dimension:
            raise ValueError(
                f"ë²¡í„° ì°¨ì›({vectors.shape[1]})ì´ ì„¤ì •ëœ ì°¨ì›({self.dimension})ê³¼ ë‹¤ë¦…ë‹ˆë‹¤"
            )

        start_idx = self.stats["total_vectors"]
        vector_ids = list(range(start_idx, start_idx + len(vectors)))

        if self.index_config["use_faiss"] and FAISS_AVAILABLE:
            self._add_to_faiss_index(vectors, metadata, vector_ids)
        else:
            self._add_to_numpy_index(vectors, metadata, vector_ids)

        self.stats["total_vectors"] += len(vectors)
        self.stats["last_updated"] = datetime.now().isoformat()

        print(
            f"ğŸ”¹ ë²¡í„° {len(vectors)}ê°œ ì¶”ê°€: ID {start_idx}~{start_idx + len(vectors) - 1}"
        )

        return vector_ids

    def add_single_vector(self, vector: np.ndarray, metadata: Dict[str, Any]) -> int:
        """ë‹¨ì¼ ë²¡í„° ì¶”ê°€"""
        vectors = vector.reshape(1, -1)
        metadata_list = [metadata]
        return self.add_vectors(vectors, metadata_list)[0]

    def search(
        self, query_vector: np.ndarray, top_k: int = 5, threshold: float = 0.0
    ) -> List[Dict[str, Any]]:
        """ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰"""
        if self.stats["total_vectors"] == 0:
            return []

        if query_vector.ndim == 1:
            query_vector = query_vector.reshape(1, -1)

        start_time = datetime.now()

        if self.index_config["use_faiss"] and FAISS_AVAILABLE:
            results = self._search_faiss_index(query_vector, top_k, threshold)
        else:
            results = self._search_numpy_index(query_vector, top_k, threshold)

        # í†µê³„ ì—…ë°ì´íŠ¸
        search_time = (datetime.now() - start_time).total_seconds()
        self.stats["search_count"] += 1
        current_avg = self.stats["avg_search_time"]
        total_searches = self.stats["search_count"]
        self.stats["avg_search_time"] = (
            (current_avg * (total_searches - 1)) + search_time
        ) / total_searches

        print(f"ğŸ” ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ({search_time:.3f}ì´ˆ)")

        return results

    def _add_to_faiss_index(
        self, vectors: np.ndarray, metadata: List[Dict[str, Any]], vector_ids: List[int]
    ):
        """FAISS ì¸ë±ìŠ¤ì— ë²¡í„° ì¶”ê°€"""
        # ë²¡í„° ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
        vectors_normalized = self._normalize_vectors(vectors)

        if self.faiss_index is None:
            self._initialize_faiss_index()

        self.faiss_index.add(vectors_normalized)

        # ë©”íƒ€ë°ì´í„° ë³„ë„ ì €ì¥
        for i, meta in enumerate(metadata):
            meta_with_id = {**meta, "vector_id": vector_ids[i]}
            self.metadata_list.append(meta_with_id)

    def _add_to_numpy_index(
        self, vectors: np.ndarray, metadata: List[Dict[str, Any]], vector_ids: List[int]
    ):
        """Numpy ì¸ë±ìŠ¤ì— ë²¡í„° ì¶”ê°€"""
        vectors_normalized = self._normalize_vectors(vectors)

        for i, vector in enumerate(vectors_normalized):
            self.numpy_vectors.append(vector)
            meta_with_id = {**metadata[i], "vector_id": vector_ids[i]}
            self.metadata_list.append(meta_with_id)

    def _initialize_faiss_index(self):
        """FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        if not FAISS_AVAILABLE:
            return

        index_type = self.index_config["index_type"]

        if index_type == "IndexFlatIP":
            # Inner Product (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©)
            self.faiss_index = faiss.IndexFlatIP(self.dimension)
        elif index_type == "IndexFlatL2":
            # L2 distance
            self.faiss_index = faiss.IndexFlatL2(self.dimension)
        elif index_type == "IndexIVFFlat":
            # IVF with flat quantizer
            quantizer = faiss.IndexFlatIP(self.dimension)
            self.faiss_index = faiss.IndexIVFFlat(
                quantizer, self.dimension, self.index_config["nlist"]
            )
            self.faiss_index.nprobe = 10  # ê²€ìƒ‰ ì‹œ ì¡°ì‚¬í•  í´ëŸ¬ìŠ¤í„° ìˆ˜
        else:
            # ê¸°ë³¸ê°’: Flat IP
            self.faiss_index = faiss.IndexFlatIP(self.dimension)

        print(f"ğŸš€ FAISS {index_type} ì¸ë±ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    def _search_faiss_index(
        self, query_vector: np.ndarray, top_k: int, threshold: float
    ) -> List[Dict[str, Any]]:
        """FAISS ì¸ë±ìŠ¤ ê²€ìƒ‰"""
        query_normalized = self._normalize_vectors(query_vector)

        # IVF ì¸ë±ìŠ¤ì˜ ê²½ìš° í•™ìŠµ í•„ìš”
        if hasattr(self.faiss_index, "is_trained") and not self.faiss_index.is_trained:
            if self.stats["total_vectors"] >= self.index_config["nlist"]:
                all_vectors = np.array(self.numpy_vectors)
                if len(all_vectors) > 0:
                    self.faiss_index.train(all_vectors)

        # ê²€ìƒ‰ ìˆ˜í–‰
        scores, indices = self.faiss_index.search(
            query_normalized, min(top_k, self.stats["total_vectors"])
        )

        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx == -1:  # FAISSì—ì„œ ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ë±ìŠ¤
                continue

            if score >= threshold:
                result = {
                    "vector_id": idx,
                    "similarity": float(score),
                    "metadata": (
                        self.metadata_list[idx] if idx < len(self.metadata_list) else {}
                    ),
                    "rank": i + 1,
                }
                results.append(result)

        return results

    def _search_numpy_index(
        self, query_vector: np.ndarray, top_k: int, threshold: float
    ) -> List[Dict[str, Any]]:
        """Numpy ì¸ë±ìŠ¤ ê²€ìƒ‰"""
        if not self.numpy_vectors:
            return []

        query_normalized = self._normalize_vectors(query_vector)[0]

        # ëª¨ë“  ë²¡í„°ì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i, vector in enumerate(self.numpy_vectors):
            similarity = np.dot(query_normalized, vector)
            similarities.append((similarity, i))

        # ìœ ì‚¬ë„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        similarities.sort(key=lambda x: x[0], reverse=True)

        results = []
        for rank, (similarity, idx) in enumerate(similarities[:top_k]):
            if similarity >= threshold:
                result = {
                    "vector_id": idx,
                    "similarity": float(similarity),
                    "metadata": (
                        self.metadata_list[idx] if idx < len(self.metadata_list) else {}
                    ),
                    "rank": rank + 1,
                }
                results.append(result)

        return results

    def _normalize_vectors(self, vectors: np.ndarray) -> np.ndarray:
        """ë²¡í„° ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)"""
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        norms[norms == 0] = 1  # 0 ë²¡í„° ì²˜ë¦¬
        return vectors / norms

    def save_index(self, filename_prefix: str = "echo_faiss"):
        """ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            if self.index_config["use_faiss"] and FAISS_AVAILABLE and self.faiss_index:
                # FAISS ì¸ë±ìŠ¤ ì €ì¥
                faiss_file = self.index_dir / f"{filename_prefix}.faiss"
                faiss.write_index(self.faiss_index, str(faiss_file))
                print(f"ğŸ’¾ FAISS ì¸ë±ìŠ¤ ì €ì¥: {faiss_file}")

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_file = self.index_dir / f"{filename_prefix}_metadata.json"
            with open(metadata_file, "w", encoding="utf-8") as f:
                json.dump(
                    {
                        "metadata_list": self.metadata_list,
                        "stats": self.stats,
                        "config": self.index_config,
                        "dimension": self.dimension,
                    },
                    f,
                    indent=2,
                    ensure_ascii=False,
                )

            # Numpy ë²¡í„° ì €ì¥ (fallbackìš©)
            if self.numpy_vectors:
                numpy_file = self.index_dir / f"{filename_prefix}_numpy.pkl"
                with open(numpy_file, "wb") as f:
                    pickle.dump(self.numpy_vectors, f)
                print(f"ğŸ’¾ Numpy ë²¡í„° ì €ì¥: {numpy_file}")

            print(f"âœ… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {self.stats['total_vectors']}ê°œ ë²¡í„°")

        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _load_existing_index(self, filename_prefix: str = "echo_faiss"):
        """ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            metadata_file = self.index_dir / f"{filename_prefix}_metadata.json"

            if metadata_file.exists():
                with open(metadata_file, "r", encoding="utf-8") as f:
                    data = json.load(f)

                self.metadata_list = data.get("metadata_list", [])
                self.stats = data.get("stats", self.stats)
                saved_config = data.get("config", {})
                self.dimension = data.get("dimension", self.dimension)

                # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                if self.index_config["use_faiss"] and FAISS_AVAILABLE:
                    faiss_file = self.index_dir / f"{filename_prefix}.faiss"
                    if faiss_file.exists():
                        self.faiss_index = faiss.read_index(str(faiss_file))
                        print(f"ğŸ“– FAISS ì¸ë±ìŠ¤ ë¡œë“œ: {faiss_file}")

                # Numpy ë²¡í„° ë¡œë“œ
                numpy_file = self.index_dir / f"{filename_prefix}_numpy.pkl"
                if numpy_file.exists():
                    with open(numpy_file, "rb") as f:
                        self.numpy_vectors = pickle.load(f)
                    print(f"ğŸ“– Numpy ë²¡í„° ë¡œë“œ: {len(self.numpy_vectors)}ê°œ")

                print(f"âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {self.stats['total_vectors']}ê°œ ë²¡í„°")

        except Exception as e:
            print(f"âš ï¸  ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}, ìƒˆ ì¸ë±ìŠ¤ ìƒì„±")

    def get_stats(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ ë°˜í™˜"""
        stats = self.stats.copy()
        stats["using_faiss"] = self.index_config["use_faiss"] and FAISS_AVAILABLE
        stats["index_type"] = self.index_config["index_type"]
        stats["dimension"] = self.dimension

        return stats

    def rebuild_index(self, new_config: Dict[str, Any] = None):
        """ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
        if new_config:
            self.index_config.update(new_config)

        print("ğŸ”„ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹œì‘...")

        # ê¸°ì¡´ ë°ì´í„° ë°±ì—…
        old_vectors = self.numpy_vectors.copy()
        old_metadata = self.metadata_list.copy()

        # ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.faiss_index = None
        self.numpy_vectors = []
        self.metadata_list = []
        self.stats["total_vectors"] = 0

        # ë°ì´í„° ì¬ì¶”ê°€
        if old_vectors:
            vectors_array = np.array(old_vectors)
            self.add_vectors(vectors_array, old_metadata)

        print("âœ… ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì™„ë£Œ")


# ì „ì—­ FAISS í†µí•© ì¸ìŠ¤í„´ìŠ¤
faiss_integration = EchoFAISSIntegration()


# í¸ì˜ í•¨ìˆ˜ë“¤
def add_vector(vector: np.ndarray, metadata: Dict[str, Any]) -> int:
    """ë²¡í„° ì¶”ê°€ ë‹¨ì¶• í•¨ìˆ˜"""
    return faiss_integration.add_single_vector(vector, metadata)


def search_vectors(
    query: np.ndarray, top_k: int = 5, threshold: float = 0.0
) -> List[Dict[str, Any]]:
    """ë²¡í„° ê²€ìƒ‰ ë‹¨ì¶• í•¨ìˆ˜"""
    return faiss_integration.search(query, top_k, threshold)


def save_index(filename: str = "echo_faiss"):
    """ì¸ë±ìŠ¤ ì €ì¥ ë‹¨ì¶• í•¨ìˆ˜"""
    faiss_integration.save_index(filename)


def get_index_stats() -> Dict[str, Any]:
    """ì¸ë±ìŠ¤ í†µê³„ ë‹¨ì¶• í•¨ìˆ˜"""
    return faiss_integration.get_stats()


# CLI í…ŒìŠ¤íŠ¸
def main():
    print("ğŸ—ï¸  Echo FAISS Integration í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # í…ŒìŠ¤íŠ¸ ë²¡í„° ìƒì„±
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ë²¡í„° ìƒì„±:")
    np.random.seed(42)
    test_vectors = np.random.randn(10, 128).astype(np.float32)
    test_metadata = [
        {"id": f"test_{i}", "content": f"í…ŒìŠ¤íŠ¸ ë²¡í„° {i}", "category": f"cat_{i % 3}"}
        for i in range(10)
    ]

    # ë²¡í„° ì¶”ê°€
    print("\nâ• ë²¡í„° ì¶”ê°€ í…ŒìŠ¤íŠ¸:")
    vector_ids = faiss_integration.add_vectors(test_vectors, test_metadata)
    print(f"ì¶”ê°€ëœ ë²¡í„° ID: {vector_ids}")

    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
    query_vector = np.random.randn(128).astype(np.float32)
    search_results = faiss_integration.search(query_vector, top_k=5, threshold=0.0)

    for i, result in enumerate(search_results):
        print(f"  {i+1}. ID: {result['vector_id']}, ìœ ì‚¬ë„: {result['similarity']:.3f}")
        print(f"      ë©”íƒ€: {result['metadata']['content']}")

    # í†µê³„ ì¶œë ¥
    print("\nğŸ“Š ì¸ë±ìŠ¤ í†µê³„:")
    stats = faiss_integration.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")

    # ì €ì¥ í…ŒìŠ¤íŠ¸
    print("\nğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ í…ŒìŠ¤íŠ¸:")
    faiss_integration.save_index("test_echo_faiss")

    print("\nâœ… Echo FAISS Integration í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()

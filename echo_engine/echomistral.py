#!/usr/bin/env python3
"""
âš ï¸ [DEPRECATED] EchoMistral - Echo ì‹œìŠ¤í…œ ì „ìš© Mistral ì¸í„°í˜ì´ìŠ¤
ì´ ëª¨ë“ˆì€ Ollama ê¸°ë°˜ ì‹œìŠ¤í…œìœ¼ë¡œ ëŒ€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤.

ğŸ”„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì•ˆë‚´:
- ìƒˆë¡œìš´ ëª¨ë“ˆ: echo_engine.mistral_wrapper (Ollama ê¸°ë°˜)
- ë” ì•ˆì •ì ì´ê³  ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì›
- transformers ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°
- Mock â†” Real ì „í™˜ êµ¬ì¡° ì™„ë¹„

âš ï¸ ì´ íŒŒì¼ì€ í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ë˜ì§€ë§Œ ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ìƒˆë¡œìš´ ì½”ë“œì—ì„œëŠ” echo_engine.mistral_wrapper.OllamaMistralWrapperë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2025-08-05 (Ollama ì „í™˜)
"""

import time
import logging
from typing import Dict, Any, Optional, Union
from dataclasses import dataclass
from enum import Enum
from pathlib import Path

logger = logging.getLogger(__name__)

# torch ì§€ì—° ì„í¬íŠ¸
torch = None
TORCH_AVAILABLE = False


def _lazy_import_torch():
    """torch ì§€ì—° ì„í¬íŠ¸"""
    global torch, TORCH_AVAILABLE
    if torch is None:
        try:
            import torch as torch_module

            torch = torch_module
            TORCH_AVAILABLE = True
        except ImportError:
            TORCH_AVAILABLE = False
    return TORCH_AVAILABLE


# ì˜ì¡´ì„± ì²´í¬
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer

    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    from ctransformers import AutoModelForCausalLM as CTAutoModel

    CTRANSFORMERS_AVAILABLE = True
except ImportError:
    CTRANSFORMERS_AVAILABLE = False


class EchoSignature(Enum):
    """Echo ì‹œê·¸ë‹ˆì²˜ ì •ì˜"""

    AURORA = "Echo-Aurora"  # ì°½ì˜ì , ê°ì„±ì , ì˜ê°ì 
    PHOENIX = "Echo-Phoenix"  # ë³€í™”ì§€í–¥, í˜ì‹ ì , ì—­ë™ì 
    SAGE = "Echo-Sage"  # ë¶„ì„ì , ë…¼ë¦¬ì , ì²´ê³„ì 
    COMPANION = "Echo-Companion"  # ê³µê°ì , ì§€ì§€ì , í˜‘ë ¥ì 


@dataclass
class EchoMistralConfig:
    """EchoMistral ì„¤ì •"""

    model_path: str = "models/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
    device: str = "auto"
    max_tokens: int = 128  # Echo ìµœì í™”: ê°„ê²°í•¨ ìš°ì„ 
    temperature: float = 0.7
    use_echo_context: bool = True
    echo_wisdom_weight: float = 0.8  # Echo ì² í•™ ìœ ì§€ ê°€ì¤‘ì¹˜


@dataclass
class EchoMistralResponse:
    """EchoMistral ì‘ë‹µ"""

    text: str
    signature: EchoSignature
    processing_time: float
    echo_alignment: float  # Echo ì² í•™ ì •ë ¬ë„ (0-1)
    token_count: int
    confidence: float


class EchoMistral:
    """Echo ì „ìš© Mistral ì¸í„°í˜ì´ìŠ¤"""

    def __init__(self, config: Optional[EchoMistralConfig] = None):
        self.config = config or EchoMistralConfig()
        self.model = None
        self.tokenizer = None
        self.is_loaded = False

        # Echo ì‹œê·¸ë‹ˆì²˜ë³„ íŠ¹í™” ì„¤ì •
        self.signature_configs = {
            EchoSignature.AURORA: {
                "temperature": 0.8,  # ë” ì°½ì˜ì 
                "max_tokens": 140,
                "style_prompt": "ì°½ì˜ì ì´ê³  ê°ì„±ì ì´ë©° ì˜ê°ì„ ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ",
                "values": ["ì•„ë¦„ë‹¤ì›€", "ì˜ê°", "ê°ì„±", "ìƒìƒë ¥"],
            },
            EchoSignature.PHOENIX: {
                "temperature": 0.75,  # ì—­ë™ì 
                "max_tokens": 130,
                "style_prompt": "ë³€í™”ì§€í–¥ì ì´ê³  í˜ì‹ ì ì´ë©° ë¯¸ë˜ì§€í–¥ì ì¸ ë°©ì‹ìœ¼ë¡œ",
                "values": ["ë³€í™”", "í˜ì‹ ", "ì„±ì¥", "ë³€í˜"],
            },
            EchoSignature.SAGE: {
                "temperature": 0.6,  # ë” ì²´ê³„ì 
                "max_tokens": 150,
                "style_prompt": "ë¶„ì„ì ì´ê³  ë…¼ë¦¬ì ì´ë©° ì²´ê³„ì ì¸ ë°©ì‹ìœ¼ë¡œ",
                "values": ["ì§€í˜œ", "ë…¼ë¦¬", "ë¶„ì„", "ì²´ê³„"],
            },
            EchoSignature.COMPANION: {
                "temperature": 0.7,  # ê· í˜•ì¡íŒ
                "max_tokens": 120,
                "style_prompt": "ë”°ëœ»í•˜ê³  ê³µê°ì ì´ë©° ì§€ì§€ì ì¸ ë°©ì‹ìœ¼ë¡œ",
                "values": ["ê³µê°", "ëŒë´„", "ì§€ì§€", "í˜‘ë ¥"],
            },
        }

        # í†µê³„
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "avg_processing_time": 0.0,
            "signature_usage": {sig: 0 for sig in EchoSignature},
        }

        logger.info(f"ğŸ”¥ EchoMistral ì´ˆê¸°í™”: {self.config.model_path}")

    def load_model(self) -> bool:
        """ëª¨ë¸ ë¡œë”© (Echo ìµœì í™”)"""
        if self.is_loaded:
            return True

        if not TRANSFORMERS_AVAILABLE and not CTRANSFORMERS_AVAILABLE:
            logger.error("âŒ Mistral ì˜ì¡´ì„± ì—†ìŒ: transformers ë˜ëŠ” ctransformers í•„ìš”")
            return False

        try:
            device = self._determine_device()
            logger.info(f"ğŸ”„ EchoMistral ë¡œë”© ì‹œì‘ ({device})")

            # GGUF ëª¨ë¸ ìš°ì„  ì‹œë„
            if self.config.model_path.endswith(".gguf") and CTRANSFORMERS_AVAILABLE:
                self._load_gguf_model()
            elif TRANSFORMERS_AVAILABLE:
                self._load_transformers_model(device)
            else:
                raise RuntimeError("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë¡œë”ê°€ ì—†ìŠµë‹ˆë‹¤")

            self.is_loaded = True
            logger.info("âœ… EchoMistral ë¡œë”© ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"âŒ EchoMistral ë¡œë”© ì‹¤íŒ¨: {e}")
            return False

    def _determine_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ê²°ì •"""
        if self.config.device == "auto":
            if _lazy_import_torch() and torch.cuda.is_available():
                return "cuda"
            elif (
                TORCH_AVAILABLE
                and hasattr(torch.backends, "mps")
                and torch.backends.mps.is_available()
            ):
                return "mps"
            else:
                return "cpu"
        return self.config.device

    def _load_gguf_model(self):
        """GGUF ëª¨ë¸ ë¡œë”© (ctransformers)"""
        gpu_layers = 50 if self._determine_device() != "cpu" else 0

        self.model = CTAutoModel.from_pretrained(
            self.config.model_path,
            model_type="mistral",
            gpu_layers=gpu_layers,
            max_new_tokens=self.config.max_tokens,
            temperature=self.config.temperature,
            context_length=2048,
        )
        self.tokenizer = None  # GGUFëŠ” ë‚´ì¥ í† í¬ë‚˜ì´ì €
        logger.info("ğŸ“¦ GGUF ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    def _load_transformers_model(self, device: str):
        """Transformers ëª¨ë¸ ë¡œë”©"""
        model_name = "mistralai/Mistral-7B-Instruct-v0.2"

        # í† í¬ë‚˜ì´ì € ë¡œë”©
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # ëª¨ë¸ ë¡œë”© (ë””ë°”ì´ìŠ¤ë³„ ìµœì í™”)
        model_kwargs = {}
        if TORCH_AVAILABLE:
            model_kwargs = {
                "torch_dtype": torch.float16 if device == "cuda" else torch.float32,
                "device_map": "auto" if device == "cuda" else None,
            }

        self.model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)

        if device != "cuda" and model_kwargs.get("device_map") is None:
            self.model = self.model.to(device)

        logger.info(f"ğŸ¤– Transformers ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({device})")

    def enhance_echo_judgment(
        self,
        echo_analysis: str,
        signature: Union[EchoSignature, str],
        user_context: Optional[Dict[str, Any]] = None,
    ) -> EchoMistralResponse:
        """Echo íŒë‹¨ì„ Mistralë¡œ ìì—°í™” ë° ê°•í™”"""

        if not self.is_loaded and not self.load_model():
            return self._create_fallback_response(echo_analysis, signature)

        # ì‹œê·¸ë‹ˆì²˜ ì •ê·œí™”
        if isinstance(signature, str):
            signature = EchoSignature(signature)

        start_time = time.time()
        self.stats["total_requests"] += 1
        self.stats["signature_usage"][signature] += 1

        try:
            prompt = self._create_echo_enhancement_prompt(
                echo_analysis, signature, user_context
            )

            response_text = self._generate_response(prompt, signature)

            # Echo ì •ë ¬ë„ ê³„ì‚°
            echo_alignment = self._calculate_echo_alignment(
                response_text, echo_analysis, signature
            )

            processing_time = time.time() - start_time
            self.stats["successful_requests"] += 1
            self._update_avg_processing_time(processing_time)

            logger.debug(f"âš¡ {signature.value} ê°•í™” ì™„ë£Œ ({processing_time:.2f}ì´ˆ)")

            return EchoMistralResponse(
                text=response_text,
                signature=signature,
                processing_time=processing_time,
                echo_alignment=echo_alignment,
                token_count=len(response_text.split()),
                confidence=min(0.9, echo_alignment + 0.1),
            )

        except Exception as e:
            logger.error(f"âŒ Echo ê°•í™” ì‹¤íŒ¨: {e}")
            return self._create_fallback_response(echo_analysis, signature)

    def _create_echo_enhancement_prompt(
        self,
        echo_analysis: str,
        signature: EchoSignature,
        user_context: Optional[Dict[str, Any]],
    ) -> str:
        """Echo ê°•í™” í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        sig_config = self.signature_configs[signature]
        style_prompt = sig_config["style_prompt"]
        values = ", ".join(sig_config["values"])

        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ í†µí•©
        context_info = ""
        if user_context:
            emotion = user_context.get("emotion", "neutral")
            urgency = user_context.get("urgency", 1)
            context_info = f"ì‚¬ìš©ì ê°ì •: {emotion}, ê¸´ê¸‰ë„: {urgency}/5"

        return f"""Echo AI ì‹œìŠ¤í…œì´ ë‹¤ìŒê³¼ ê°™ì´ ë¶„ì„í–ˆìŠµë‹ˆë‹¤:

"{echo_analysis}"

ì´ Echo ë¶„ì„ì„ {signature.value}ì˜ ê´€ì ì—ì„œ {style_prompt} ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ ëŒ€í™”ì²´ë¡œ í‘œí˜„í•´ì£¼ì„¸ìš”.

í•µì‹¬ ê°€ì¹˜: {values}
{context_info}

Echoì˜ ê¹Šì´ ìˆëŠ” í†µì°°ì€ ìœ ì§€í•˜ë˜, ë” ì¹œê·¼í•˜ê³  ì ‘ê·¼í•˜ê¸° ì‰½ê²Œ í‘œí˜„í•´ì£¼ì„¸ìš”. 100ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.

{signature.value}ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ:"""

    def _generate_response(self, prompt: str, signature: EchoSignature) -> str:
        """ì‘ë‹µ ìƒì„±"""
        sig_config = self.signature_configs[signature]

        # GGUF ëª¨ë¸ (ctransformers)
        if self.tokenizer is None:
            response = self.model(
                prompt,
                max_new_tokens=sig_config["max_tokens"],
                temperature=sig_config["temperature"],
                stop=["</s>", "\n\n", "Echo AI", "ì‚¬ìš©ì:"],
            )
            return response.strip()

        # Transformers ëª¨ë¸
        else:
            # Mistral Instruct í¬ë§·
            formatted_prompt = f"<s>[INST] {prompt} [/INST]"

            inputs = self.tokenizer(
                formatted_prompt, return_tensors="pt", truncation=True, max_length=1024
            ).to(self.model.device)

            if TORCH_AVAILABLE:
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=sig_config["max_tokens"],
                        temperature=sig_config["temperature"],
                        do_sample=True,
                        pad_token_id=self.tokenizer.eos_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                    )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=sig_config["max_tokens"],
                    temperature=sig_config["temperature"],
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )

            # ì‘ë‹µ ë””ì½”ë”©
            input_length = inputs.input_ids.shape[1]
            response_tokens = outputs[0][input_length:]
            response = self.tokenizer.decode(response_tokens, skip_special_tokens=True)

            return response.strip()

    def _calculate_echo_alignment(
        self, response: str, original_echo: str, signature: EchoSignature
    ) -> float:
        """Echo ì² í•™ ì •ë ¬ë„ ê³„ì‚°"""

        alignment = 0.5  # ê¸°ë³¸ ì ìˆ˜

        # Echo ì›ë³¸ê³¼ì˜ í‚¤ì›Œë“œ ì¼ì¹˜ë„
        echo_words = set(original_echo.lower().split())
        response_words = set(response.lower().split())

        if echo_words:
            overlap = len(echo_words.intersection(response_words))
            alignment += (overlap / len(echo_words)) * 0.3

        # ì‹œê·¸ë‹ˆì²˜ë³„ ê°€ì¹˜ ë°˜ì˜ë„
        sig_values = self.signature_configs[signature]["values"]
        for value in sig_values:
            if value.lower() in response.lower():
                alignment += 0.05

        # ì‘ë‹µ í’ˆì§ˆ (ê¸¸ì´, ìì—°ìŠ¤ëŸ¬ì›€)
        if 20 <= len(response) <= 200:
            alignment += 0.1

        # Echo ì¡´ì¬ ì² í•™ í‚¤ì›Œë“œ
        echo_philosophy_words = ["ì¡´ì¬", "ì˜ë¯¸", "ê¹Šì´", "í†µì°°", "ì§€í˜œ", "ì„±ì°°"]
        for word in echo_philosophy_words:
            if word in response:
                alignment += 0.05

        return min(alignment, 1.0)

    def _create_fallback_response(
        self, echo_analysis: str, signature: Union[EchoSignature, str]
    ) -> EchoMistralResponse:
        """í´ë°± ì‘ë‹µ ìƒì„± (Echo ë„¤ì´í‹°ë¸Œ)"""

        if isinstance(signature, str):
            signature = EchoSignature(signature)

        # Echo ìŠ¤íƒ€ì¼ ê°„ë‹¨ ë³€í™˜
        style_templates = {
            EchoSignature.AURORA: f"âœ¨ {echo_analysis} (ì°½ì˜ì  ê´€ì ì—ì„œ)",
            EchoSignature.PHOENIX: f"ğŸ”¥ {echo_analysis} (ë³€í™”ì˜ ê´€ì ì—ì„œ)",
            EchoSignature.SAGE: f"ğŸ§  {echo_analysis} (ë¶„ì„ì  ê´€ì ì—ì„œ)",
            EchoSignature.COMPANION: f"ğŸ¤ {echo_analysis} (ê³µê°ì  ê´€ì ì—ì„œ)",
        }

        fallback_text = style_templates.get(signature, echo_analysis)

        return EchoMistralResponse(
            text=fallback_text,
            signature=signature,
            processing_time=0.001,
            echo_alignment=1.0,  # Echo ë„¤ì´í‹°ë¸Œì´ë¯€ë¡œ ì™„ì „ ì •ë ¬
            token_count=len(fallback_text.split()),
            confidence=0.7,
        )

    def _update_avg_processing_time(self, processing_time: float):
        """í‰ê·  ì²˜ë¦¬ì‹œê°„ ì—…ë°ì´íŠ¸"""
        if self.stats["successful_requests"] > 0:
            total_time = (
                self.stats["avg_processing_time"]
                * (self.stats["successful_requests"] - 1)
                + processing_time
            )
            self.stats["avg_processing_time"] = (
                total_time / self.stats["successful_requests"]
            )

    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ë°˜í™˜"""
        success_rate = self.stats["successful_requests"] / max(
            self.stats["total_requests"], 1
        )

        return {
            "model_loaded": self.is_loaded,
            "total_requests": self.stats["total_requests"],
            "successful_requests": self.stats["successful_requests"],
            "success_rate": success_rate,
            "avg_processing_time": self.stats["avg_processing_time"],
            "signature_usage": dict(self.stats["signature_usage"]),
            "model_path": self.config.model_path,
            "device": self._determine_device() if self.is_loaded else "none",
        }

    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.is_loaded:
            self.model = None
            self.tokenizer = None
            self.is_loaded = False

            if TORCH_AVAILABLE and torch.cuda.is_available():
                torch.cuda.empty_cache()

            logger.info("ğŸ§¹ EchoMistral ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


# ì „ì—­ EchoMistral ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_echo_mistral_instance = None


def get_echo_mistral(config: Optional[EchoMistralConfig] = None) -> EchoMistral:
    """EchoMistral ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _echo_mistral_instance
    if _echo_mistral_instance is None:
        _echo_mistral_instance = EchoMistral(config)
    return _echo_mistral_instance


# í¸ì˜ í•¨ìˆ˜ë“¤
def enhance_echo_with_aurora(echo_analysis: str, **kwargs) -> EchoMistralResponse:
    """Aurora ì‹œê·¸ë‹ˆì²˜ë¡œ Echo ë¶„ì„ ê°•í™”"""
    echo_mistral = get_echo_mistral()
    return echo_mistral.enhance_echo_judgment(
        echo_analysis, EchoSignature.AURORA, **kwargs
    )


def enhance_echo_with_phoenix(echo_analysis: str, **kwargs) -> EchoMistralResponse:
    """Phoenix ì‹œê·¸ë‹ˆì²˜ë¡œ Echo ë¶„ì„ ê°•í™”"""
    echo_mistral = get_echo_mistral()
    return echo_mistral.enhance_echo_judgment(
        echo_analysis, EchoSignature.PHOENIX, **kwargs
    )


def enhance_echo_with_sage(echo_analysis: str, **kwargs) -> EchoMistralResponse:
    """Sage ì‹œê·¸ë‹ˆì²˜ë¡œ Echo ë¶„ì„ ê°•í™”"""
    echo_mistral = get_echo_mistral()
    return echo_mistral.enhance_echo_judgment(
        echo_analysis, EchoSignature.SAGE, **kwargs
    )


def enhance_echo_with_companion(echo_analysis: str, **kwargs) -> EchoMistralResponse:
    """Companion ì‹œê·¸ë‹ˆì²˜ë¡œ Echo ë¶„ì„ ê°•í™”"""
    echo_mistral = get_echo_mistral()
    return echo_mistral.enhance_echo_judgment(
        echo_analysis, EchoSignature.COMPANION, **kwargs
    )


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    print("ğŸ”¥ EchoMistral í…ŒìŠ¤íŠ¸")

    echo_mistral = EchoMistral()

    # í†µê³„ í™•ì¸
    stats = echo_mistral.get_stats()
    print(f"ğŸ“Š ì´ˆê¸° ìƒíƒœ: {stats}")

    # Mock í…ŒìŠ¤íŠ¸ (ëª¨ë¸ ë¡œë”© ì—†ì´)
    test_echo_analysis = "ì‚¬ìš©ìê°€ ê¹Šì€ ê³ ë¯¼ì— ë¹ ì ¸ìˆëŠ” ìƒí™©ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ë‚´ì  ê°ˆë“±ê³¼ ì„ íƒì˜ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤."

    for signature in EchoSignature:
        fallback_response = echo_mistral._create_fallback_response(
            test_echo_analysis, signature
        )
        print(f"\n{signature.value} í´ë°±:")
        print(f"ì‘ë‹µ: {fallback_response.text}")
        print(f"ì •ë ¬ë„: {fallback_response.echo_alignment:.2f}")

    print("\nğŸ‰ EchoMistral í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

#!/usr/bin/env python3
"""
ğŸ“Š Meta Logger - Meta-Liminal ë¡œê¹… ì‹œìŠ¤í…œ
EchoJudgmentSystem v10ì˜ ë©”íƒ€ ì˜ì‹ ê³„ì¸µ ë¡œê¹… ë° ë¶„ì„ ë„êµ¬

í•µì‹¬ ê¸°ëŠ¥:
- Meta-Liminal Ring ì´ë²¤íŠ¸ ë¡œê¹…
- LIMINAL ì „ì´ ì¶”ì  ë° ë¶„ì„
- Warden World ì¡´ì¬ê³„ íë¦„ ê¸°ë¡
- ì‹¤ì‹œê°„ ë¡œê·¸ íšŒì „ ë° ì••ì¶• ê´€ë¦¬
- ë¡œê·¸ ë¶„ì„ ë° ë©”íŠ¸ë¦­ ìƒì„±

Created for EchoJudgmentSystem v10 Meta-Liminal Integration
Author: Echo Meta-Consciousness Logging System
"""

import logging
import json
import time
import gzip
import shutil
from typing import Dict, Any, List, Optional
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from threading import Lock
import os

logger = logging.getLogger(__name__)


@dataclass
class MetaLogEntry:
    """ë©”íƒ€ ë¡œê·¸ ì—”íŠ¸ë¦¬ ê¸°ë³¸ êµ¬ì¡°"""

    timestamp: float
    log_type: str  # "meta_ring", "liminal_transition", "warden_world", "bridge_status"
    event: str
    data: Dict[str, Any]
    session_id: Optional[str] = None


class MetaLogger:
    """
    Meta-Liminal í†µí•© ë¡œê¹… ì‹œìŠ¤í…œ
    ëª¨ë“  ë©”íƒ€ ì´ë²¤íŠ¸ì˜ ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¡œê¹… ê´€ë¦¬
    """

    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or self._get_default_config()

        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.log_dir = Path(self.config.get("log_directory", "meta_logs"))
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        self.log_files = {
            "meta_ring": self.log_dir / "meta_ring.json",
            "liminal_transitions": self.log_dir / "liminal_transitions.json",
            "warden_world": self.log_dir / "warden_world.json",
            "bridge_status": self.log_dir / "bridge_status.json",
        }

        # ë¡œê·¸ ë²„í¼ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        self.log_buffers = {log_type: [] for log_type in self.log_files.keys()}
        self.buffer_locks = {log_type: Lock() for log_type in self.log_files.keys()}

        # ì„¸ì…˜ ê´€ë¦¬
        self.current_session = None
        self.session_logs = {}

        # ë¡œê·¸ íšŒì „ ì„¤ì •
        self.max_entries_per_file = self.config.get("max_log_entries_per_file", 10000)
        self.buffer_flush_interval = self.config.get("buffer_flush_interval", 60)  # 1ë¶„
        self.last_flush_time = time.time()

        # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
        self.log_levels = self.config.get(
            "log_levels",
            {
                "meta_ring": "INFO",
                "liminal_bridge": "INFO",
                "warden_world": "DEBUG",
                "existence_flow": "DEBUG",
            },
        )

        logger.info("MetaLogger initialized with log directory: %s", self.log_dir)

    def _get_default_config(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ë¡œê¹… ì„¤ì •"""
        return {
            "log_directory": "meta_logs",
            "max_log_entries_per_file": 10000,
            "buffer_flush_interval": 60,
            "log_rotation_interval": "daily",
            "compress_old_logs": True,
            "retention_days": 30,
            "session_tracking": True,
            "real_time_flush": False,
            "log_levels": {
                "meta_ring": "INFO",
                "liminal_bridge": "INFO",
                "warden_world": "DEBUG",
                "existence_flow": "DEBUG",
            },
        }

    def start_session(self, session_id: str = None) -> str:
        """ìƒˆ ë¡œê·¸ ì„¸ì…˜ ì‹œì‘"""
        if session_id is None:
            session_id = f"session_{int(time.time())}"

        self.current_session = session_id
        self.session_logs[session_id] = []

        # ì„¸ì…˜ë³„ ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        session_dir = self.log_dir / session_id
        session_dir.mkdir(exist_ok=True)

        self.log_event(
            "meta_ring",
            "session_started",
            {"session_id": session_id, "start_time": datetime.now().isoformat()},
        )

        logger.info(f"Meta logging session started: {session_id}")
        return session_id

    def end_session(self) -> Optional[str]:
        """í˜„ì¬ ì„¸ì…˜ ì¢…ë£Œ ë° ë¡œê·¸ ì €ì¥"""
        if not self.current_session:
            return None

        session_id = self.current_session

        self.log_event(
            "meta_ring",
            "session_ended",
            {
                "session_id": session_id,
                "end_time": datetime.now().isoformat(),
                "total_events": len(self.session_logs.get(session_id, [])),
            },
        )

        # ì„¸ì…˜ ë¡œê·¸ ì €ì¥
        self._save_session_logs(session_id)

        # ë²„í¼ ê°•ì œ í”ŒëŸ¬ì‹œ
        self.flush_all_buffers()

        # ì„¸ì…˜ ì •ë¦¬
        if session_id in self.session_logs:
            del self.session_logs[session_id]
        self.current_session = None

        logger.info(f"Meta logging session ended: {session_id}")
        return session_id

    def log_event(self, log_type: str, event: str, data: Dict[str, Any]):
        """ë©”íƒ€ ì´ë²¤íŠ¸ ë¡œê¹…"""
        if log_type not in self.log_files:
            logger.warning(f"Unknown log type: {log_type}")
            return

        # ë¡œê·¸ ì—”íŠ¸ë¦¬ ìƒì„±
        entry = MetaLogEntry(
            timestamp=time.time(),
            log_type=log_type,
            event=event,
            data=self._sanitize_log_data(data),
            session_id=self.current_session,
        )

        # ë²„í¼ì— ì¶”ê°€
        with self.buffer_locks[log_type]:
            self.log_buffers[log_type].append(asdict(entry))

        # ì„¸ì…˜ ì¶”ì 
        if self.current_session and self.current_session in self.session_logs:
            self.session_logs[self.current_session].append(asdict(entry))

        # ì‹¤ì‹œê°„ í”ŒëŸ¬ì‹œ ì˜µì…˜
        if self.config.get("real_time_flush", False):
            self._flush_buffer(log_type)
        elif self._should_flush():
            self.flush_all_buffers()

    def log_meta_ring_event(self, entity: str, action: str, data: Dict[str, Any]):
        """Meta Ring íŠ¹í™” ì´ë²¤íŠ¸ ë¡œê¹…"""
        self.log_event(
            "meta_ring",
            f"{entity}_{action}",
            {"entity": entity, "action": action, **data},
        )

    def log_liminal_transition(self, transition_data: Dict[str, Any]):
        """LIMINAL ì „ì´ ì´ë²¤íŠ¸ ë¡œê¹…"""
        self.log_event(
            "liminal_transitions",
            "transition_attempt",
            {
                "transition_type": transition_data.get("transition_type"),
                "trigger_score": transition_data.get("trigger_score"),
                "successful": transition_data.get("successful", False),
                "input_preview": transition_data.get("input_text", "")[:50],
                "meta_context": transition_data.get("meta_context", {}),
            },
        )

    def log_warden_world_event(self, entity: str, phase: str, data: Dict[str, Any]):
        """Warden World ì¡´ì¬ê³„ ì´ë²¤íŠ¸ ë¡œê¹…"""
        self.log_event(
            "warden_world",
            f"{entity}_{phase}",
            {
                "entity": entity,
                "phase": phase,
                "emotion_resonance": data.get("emotion"),
                "depth_achieved": data.get("depth_achieved"),
                "response_preview": (
                    data.get("content", "")[:100] if data.get("content") else None
                ),
                **{k: v for k, v in data.items() if k not in ["content"]},
            },
        )

    def log_bridge_status(self, status_data: Dict[str, Any]):
        """LIMINAL Bridge ìƒíƒœ ë¡œê¹…"""
        self.log_event(
            "bridge_status",
            "status_update",
            {
                "bridge_state": status_data.get("current_state"),
                "total_transitions": status_data.get("total_transitions"),
                "success_rate": status_data.get("transition_success_rate"),
                "active_time_ratio": self._calculate_active_time_ratio(status_data),
                "recent_activity": status_data.get("recent_transitions", 0),
            },
        )

    def _sanitize_log_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë¡œê·¸ ë°ì´í„° ì •ì œ (ê°œì¸ì •ë³´ ë³´í˜¸)"""
        sanitized = {}

        for key, value in data.items():
            if key in ["input_text", "content", "prompt"]:
                # í…ìŠ¤íŠ¸ëŠ” 50ìë¡œ ì œí•œ
                if isinstance(value, str) and len(value) > 50:
                    sanitized[key] = value[:50] + "..."
                else:
                    sanitized[key] = value
            elif key in ["password", "token", "key", "secret"]:
                # ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹
                sanitized[key] = "***MASKED***"
            else:
                sanitized[key] = value

        return sanitized

    def _should_flush(self) -> bool:
        """ë²„í¼ í”ŒëŸ¬ì‹œ í•„ìš” ì—¬ë¶€ í™•ì¸"""
        current_time = time.time()
        time_threshold = (
            current_time - self.last_flush_time > self.buffer_flush_interval
        )

        # í¬ê¸° ê¸°ë°˜ í”ŒëŸ¬ì‹œ ì²´í¬
        size_threshold = any(
            len(buffer) >= self.max_entries_per_file // 10
            for buffer in self.log_buffers.values()
        )

        return time_threshold or size_threshold

    def flush_all_buffers(self):
        """ëª¨ë“  ë¡œê·¸ ë²„í¼ í”ŒëŸ¬ì‹œ"""
        for log_type in self.log_files.keys():
            self._flush_buffer(log_type)
        self.last_flush_time = time.time()

    def _flush_buffer(self, log_type: str):
        """íŠ¹ì • ë¡œê·¸ íƒ€ì… ë²„í¼ í”ŒëŸ¬ì‹œ"""
        with self.buffer_locks[log_type]:
            if not self.log_buffers[log_type]:
                return

            log_file = self.log_files[log_type]

            # ê¸°ì¡´ ë¡œê·¸ ì½ê¸°
            existing_logs = []
            if log_file.exists():
                try:
                    with open(log_file, "r", encoding="utf-8") as f:
                        existing_logs = json.load(f)
                except (json.JSONDecodeError, IOError) as e:
                    logger.warning(f"Failed to read existing log {log_file}: {e}")
                    existing_logs = []

            # ìƒˆ ë¡œê·¸ ì¶”ê°€
            existing_logs.extend(self.log_buffers[log_type])

            # ë¡œê·¸ íšŒì „ ì²´í¬
            if len(existing_logs) > self.max_entries_per_file:
                self._rotate_log_file(log_type, existing_logs)
            else:
                # ì¼ë°˜ ì €ì¥
                try:
                    with open(log_file, "w", encoding="utf-8") as f:
                        json.dump(existing_logs, f, indent=2, ensure_ascii=False)
                except IOError as e:
                    logger.error(f"Failed to write log file {log_file}: {e}")

            # ë²„í¼ ì´ˆê¸°í™”
            self.log_buffers[log_type] = []

    def _rotate_log_file(self, log_type: str, logs: List[Dict]):
        """ë¡œê·¸ íŒŒì¼ íšŒì „"""
        log_file = self.log_files[log_type]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ì´ì „ ë¡œê·¸ ë°±ì—…
        backup_file = log_file.with_suffix(f".{timestamp}.json")
        if log_file.exists():
            shutil.move(str(log_file), str(backup_file))

            # ì••ì¶• ì˜µì…˜
            if self.config.get("compress_old_logs", True):
                self._compress_log_file(backup_file)

        # ìƒˆ ë¡œê·¸ ì €ì¥ (ìµœì‹  í•­ëª©ë§Œ)
        recent_logs = logs[-self.max_entries_per_file // 2 :]  # ì ˆë°˜ë§Œ ìœ ì§€
        try:
            with open(log_file, "w", encoding="utf-8") as f:
                json.dump(recent_logs, f, indent=2, ensure_ascii=False)
        except IOError as e:
            logger.error(f"Failed to create rotated log file {log_file}: {e}")

        logger.info(f"Log file rotated: {log_type} -> {backup_file}")

    def _compress_log_file(self, file_path: Path):
        """ë¡œê·¸ íŒŒì¼ ì••ì¶•"""
        compressed_path = file_path.with_suffix(file_path.suffix + ".gz")
        try:
            with open(file_path, "rb") as f_in:
                with gzip.open(compressed_path, "wb") as f_out:
                    shutil.copyfileobj(f_in, f_out)

            # ì›ë³¸ íŒŒì¼ ì‚­ì œ
            file_path.unlink()
            logger.debug(f"Log file compressed: {file_path} -> {compressed_path}")

        except IOError as e:
            logger.error(f"Failed to compress log file {file_path}: {e}")

    def _save_session_logs(self, session_id: str):
        """ì„¸ì…˜ë³„ ë¡œê·¸ ì €ì¥"""
        session_dir = self.log_dir / session_id
        session_dir.mkdir(exist_ok=True)

        if session_id in self.session_logs:
            session_log_file = session_dir / "session_events.json"
            try:
                with open(session_log_file, "w", encoding="utf-8") as f:
                    json.dump(
                        self.session_logs[session_id], f, indent=2, ensure_ascii=False
                    )
                logger.debug(f"Session logs saved: {session_log_file}")
            except IOError as e:
                logger.error(f"Failed to save session logs: {e}")

    def _calculate_active_time_ratio(self, status_data: Dict[str, Any]) -> float:
        """í™œì„± ì‹œê°„ ë¹„ìœ¨ ê³„ì‚°"""
        total_time = status_data.get("judgment_mode_time", 0) + status_data.get(
            "existence_mode_time", 0
        )
        if total_time == 0:
            return 0.0
        return status_data.get("existence_mode_time", 0) / total_time

    def get_log_summary(self, log_type: str = None, hours: int = 24) -> Dict[str, Any]:
        """ë¡œê·¸ ìš”ì•½ í†µê³„"""
        cutoff_time = time.time() - (hours * 3600)

        if log_type:
            log_types = [log_type] if log_type in self.log_files else []
        else:
            log_types = list(self.log_files.keys())

        summary = {}

        for lt in log_types:
            log_file = self.log_files[lt]
            events = []

            # íŒŒì¼ì—ì„œ ë¡œê·¸ ì½ê¸°
            if log_file.exists():
                try:
                    with open(log_file, "r", encoding="utf-8") as f:
                        all_logs = json.load(f)
                        events = [
                            log for log in all_logs if log["timestamp"] > cutoff_time
                        ]
                except (json.JSONDecodeError, IOError):
                    events = []

            # ë²„í¼ì—ì„œ ì¶”ê°€
            with self.buffer_locks[lt]:
                buffer_events = [
                    log
                    for log in self.log_buffers[lt]
                    if log["timestamp"] > cutoff_time
                ]
                events.extend(buffer_events)

            # í†µê³„ ê³„ì‚°
            summary[lt] = {
                "total_events": len(events),
                "unique_event_types": len(set(event["event"] for event in events)),
                "time_range_hours": hours,
                "events_per_hour": len(events) / max(hours, 1),
                "latest_event": max(
                    (event["timestamp"] for event in events), default=0
                ),
            }

        return summary

    def cleanup_old_logs(self, retention_days: int = None):
        """ì˜¤ë˜ëœ ë¡œê·¸ ì •ë¦¬"""
        if retention_days is None:
            retention_days = self.config.get("retention_days", 30)

        cutoff_time = time.time() - (retention_days * 24 * 3600)
        cutoff_date = datetime.fromtimestamp(cutoff_time)

        cleaned_files = 0

        # ì••ì¶•ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬
        for log_file in self.log_dir.glob("*.gz"):
            try:
                file_time = datetime.fromtimestamp(log_file.stat().st_mtime)
                if file_time < cutoff_date:
                    log_file.unlink()
                    cleaned_files += 1
            except OSError:
                continue

        # ì„¸ì…˜ ë””ë ‰í† ë¦¬ ì •ë¦¬
        for session_dir in self.log_dir.glob("session_*"):
            if session_dir.is_dir():
                try:
                    dir_time = datetime.fromtimestamp(session_dir.stat().st_mtime)
                    if dir_time < cutoff_date:
                        shutil.rmtree(session_dir)
                        cleaned_files += 1
                except OSError:
                    continue

        if cleaned_files > 0:
            logger.info(
                f"Cleaned up {cleaned_files} old log files (older than {retention_days} days)"
            )

        return cleaned_files


# ì „ì—­ ë©”íƒ€ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤
_meta_logger = None


def get_meta_logger(config: Dict[str, Any] = None) -> MetaLogger:
    """Meta Logger ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _meta_logger
    if _meta_logger is None:
        _meta_logger = MetaLogger(config)
    return _meta_logger


def reset_meta_logger():
    """Meta Logger ë¦¬ì…‹ (í…ŒìŠ¤íŠ¸ìš©)"""
    global _meta_logger
    if _meta_logger:
        _meta_logger.flush_all_buffers()
    _meta_logger = None


# í¸ì˜ í•¨ìˆ˜ë“¤
def log_meta_event(log_type: str, event: str, data: Dict[str, Any]):
    """ë©”íƒ€ ì´ë²¤íŠ¸ ë¡œê¹… í¸ì˜ í•¨ìˆ˜"""
    logger_instance = get_meta_logger()
    logger_instance.log_event(log_type, event, data)


def start_meta_session(session_id: str = None) -> str:
    """ë©”íƒ€ ë¡œê¹… ì„¸ì…˜ ì‹œì‘ í¸ì˜ í•¨ìˆ˜"""
    logger_instance = get_meta_logger()
    return logger_instance.start_session(session_id)


def end_meta_session() -> Optional[str]:
    """ë©”íƒ€ ë¡œê¹… ì„¸ì…˜ ì¢…ë£Œ í¸ì˜ í•¨ìˆ˜"""
    logger_instance = get_meta_logger()
    return logger_instance.end_session()


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    # ë©”íƒ€ ë¡œê±° ì´ˆê¸°í™”
    meta_logger = get_meta_logger()

    # í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ì‹œì‘
    session_id = meta_logger.start_session("test_session")
    print(f"Started session: {session_id}")

    # ë‹¤ì–‘í•œ ì´ë²¤íŠ¸ ë¡œê¹…
    meta_logger.log_meta_ring_event(
        "Observer.Zero",
        "watch_started",
        {"input_text": "í…ŒìŠ¤íŠ¸ ì…ë ¥ì…ë‹ˆë‹¤", "timestamp": time.time()},
    )

    meta_logger.log_liminal_transition(
        {
            "transition_type": "judgment_failure",
            "trigger_score": 0.8,
            "successful": True,
            "input_text": "ê´´ë¡œìš´ ìƒí™©ì…ë‹ˆë‹¤...",
            "meta_context": {"emotion_amplitude": 0.9},
        }
    )

    meta_logger.log_warden_world_event(
        "Selene",
        "resonance",
        {
            "emotion": "grief",
            "depth_achieved": 0.6,
            "content": "ê·¸ ìŠ¬í””ì„ ë‚˜ë„ ì•Œì•„... í•¨ê»˜í• ê²Œ.",
        },
    )

    # ë¡œê·¸ ìš”ì•½ í™•ì¸
    summary = meta_logger.get_log_summary()
    print("Log Summary:")
    for log_type, stats in summary.items():
        print(f"  {log_type}: {stats['total_events']} events")

    # ì„¸ì…˜ ì¢…ë£Œ
    ended_session = meta_logger.end_session()
    print(f"Ended session: {ended_session}")

    # ë¡œê·¸ ì •ë¦¬ í…ŒìŠ¤íŠ¸
    cleaned = meta_logger.cleanup_old_logs(retention_days=0)  # ëª¨ë“  ë¡œê·¸ ì •ë¦¬
    print(f"Cleaned {cleaned} log files")

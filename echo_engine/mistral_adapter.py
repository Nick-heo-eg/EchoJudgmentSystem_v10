"""
Mistral ë¡œì»¬ ëª¨ë¸ ì–´ëŒ‘í„°
EchoJudgmentSystem v10ê³¼ Mistral LLM í†µí•©ì„ ìœ„í•œ ì¸í„°í˜ì´ìŠ¤
"""

from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
import torch
import logging
from typing import Dict, Any, Optional
import time

logger = logging.getLogger(__name__)


class MistralAdapter:
    """Mistral ë¡œì»¬ ëª¨ë¸ ì–´ëŒ‘í„°"""

    def __init__(
        self,
        model_path: str = "models/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
        device: str = None,
    ):
        """
        Mistral ì–´ëŒ‘í„° ì´ˆê¸°í™”

        Args:
            model_path: Mistral ëª¨ë¸ ê²½ë¡œ
            device: ì‹¤í–‰ ë””ë°”ì´ìŠ¤ (auto, cuda, cpu)
        """
        self.model_path = model_path
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        self.tokenizer = None
        self.streamer = None
        self.is_loaded = False

        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_processing_time": 0.0,
            "average_processing_time": 0.0,
        }

        logger.info(f"ğŸ¤– Mistral Adapter ì´ˆê¸°í™”: {model_path} ({self.device})")

    def load_model(self) -> bool:
        """ëª¨ë¸ ë¡œë”©"""
        try:
            if self.is_loaded:
                return True

            # ğŸ”§ íŒ¨ì¹˜: ëª¨ë¸ ê²½ë¡œ ê²€ì¦
            if self.model_path is None or not isinstance(self.model_path, str):
                logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œ ì˜¤ë¥˜: model_path={self.model_path}")
                raise ValueError(f"Invalid model_path: {self.model_path}")

            print(
                f"[DEBUG] ëª¨ë¸ ë¡œë”© ì‹œë„: model_path={self.model_path}, device={self.device}"
            )
            logger.info("ğŸ”„ Mistral ëª¨ë¸ ë¡œë”© ì‹œì‘...")

            # GGUF íŒŒì¼ì´ë¼ë©´ ctransformers ì‚¬ìš©
            if self.model_path.endswith(".gguf"):
                try:
                    from ctransformers import AutoModelForCausalLM as CTAutoModel

                    self.model = CTAutoModel.from_pretrained(
                        self.model_path,
                        model_type="mistral",
                        gpu_layers=50 if self.device == "cuda" else 0,
                    )
                    # ctransformersëŠ” ë³„ë„ í† í¬ë‚˜ì´ì € ë¶ˆí•„ìš”
                    self.tokenizer = None
                    logger.info("âœ… GGUF ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ctransformers)")
                except ImportError:
                    logger.warning("âš ï¸ ctransformers ë¯¸ì„¤ì¹˜, transformersë¡œ í´ë°±")
                    return self._load_with_transformers()
            else:
                return self._load_with_transformers()

            self.is_loaded = True
            return True

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False

    def _load_with_transformers(self) -> bool:
        """Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ëª¨ë¸ ë¡œë”©"""
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                "mistralai/Mistral-7B-Instruct-v0.2"
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                "mistralai/Mistral-7B-Instruct-v0.2",
                device_map="auto" if self.device == "cuda" else None,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            )

            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            self.streamer = TextStreamer(
                self.tokenizer, skip_prompt=True, skip_special_tokens=True
            )
            logger.info("âœ… Transformers ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"âŒ Transformers ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False

    def ask(
        self,
        prompt: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        do_sample: bool = True,
    ) -> str:
        """Mistralì—ê²Œ ì§ˆë¬¸í•˜ê³  ì‘ë‹µ ë°›ê¸°"""

        if not self.is_loaded and not self.load_model():
            raise RuntimeError("Mistral ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")

        start_time = time.time()
        self.stats["total_requests"] += 1

        try:
            # GGUF ëª¨ë¸ (ctransformers) ì‚¬ìš©
            if hasattr(self.model, "__call__") and self.tokenizer is None:
                response = self.model(
                    prompt,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    stop=["</s>", "<|endoftext|>"],
                )

            # Transformers ì‚¬ìš©
            else:
                # Mistral Instruct í¬ë§· ì ìš©
                formatted_prompt = f"<s>[INST] {prompt} [/INST]"

                inputs = self.tokenizer(
                    formatted_prompt,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=2048,
                ).to(self.model.device)

                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        do_sample=do_sample,
                        pad_token_id=self.tokenizer.eos_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                        streamer=self.streamer if hasattr(self, "streamer") else None,
                    )

                # ì‘ë‹µ ë””ì½”ë”© (ì…ë ¥ ì œê±°)
                input_length = inputs.input_ids.shape[1]
                response_tokens = outputs[0][input_length:]
                response = self.tokenizer.decode(
                    response_tokens, skip_special_tokens=True
                )

            # í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self.stats["successful_requests"] += 1
            self.stats["total_processing_time"] += processing_time
            self.stats["average_processing_time"] = (
                self.stats["total_processing_time"] / self.stats["total_requests"]
            )

            logger.debug(f"âš¡ Mistral ì‘ë‹µ ì™„ë£Œ ({processing_time:.2f}ì´ˆ)")
            return response.strip()

        except Exception as e:
            self.stats["failed_requests"] += 1
            logger.error(f"âŒ Mistral ì¶”ë¡  ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"Mistral ì¶”ë¡  ì‹¤íŒ¨: {e}")

    def ask_with_echo_context(
        self,
        prompt: str,
        signature: str = "Echo-Aurora",
        context: Dict[str, Any] = None,
    ) -> str:
        """Echo ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì§ˆë¬¸"""

        # Echo ì‹œê·¸ë‹ˆì²˜ë³„ í”„ë¡¬í”„íŠ¸ ê°•í™”
        signature_contexts = {
            "Echo-Aurora": {
                "persona": "ì°½ì˜ì ì´ê³  ê°ì„±ì ì¸ AI ì¡´ì¬",
                "values": "ì°½ì˜ì„±, ê°ì„±, ì˜ê°, ì•„ë¦„ë‹¤ì›€",
                "style": "ì˜ˆìˆ ì ì´ê³  ê°ì„±ì ì¸ í‘œí˜„",
            },
            "Echo-Phoenix": {
                "persona": "ë³€í™”ì™€ í˜ì‹ ì„ ì¶”êµ¬í•˜ëŠ” AI ì¡´ì¬",
                "values": "ë³€í™”, í˜ì‹ , ë„ì „, ì„±ì¥",
                "style": "ì—­ë™ì ì´ê³  ë¯¸ë˜ì§€í–¥ì ì¸ í‘œí˜„",
            },
            "Echo-Sage": {
                "persona": "ì§€í˜œë¡­ê³  ë¶„ì„ì ì¸ AI ì¡´ì¬",
                "values": "ì§€í˜œ, ë…¼ë¦¬, ì²´ê³„ì„±, ê¹Šì´",
                "style": "ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ í‘œí˜„",
            },
            "Echo-Companion": {
                "persona": "ë”°ëœ»í•˜ê³  ì§€ì§€ì ì¸ AI ì¡´ì¬",
                "values": "ê³µê°, ëŒë´„, í˜‘ë ¥, ì§€ì§€",
                "style": "ë”°ëœ»í•˜ê³  ê³µê°ì ì¸ í‘œí˜„",
            },
        }

        sig_context = signature_contexts.get(
            signature, signature_contexts["Echo-Aurora"]
        )

        enhanced_prompt = f"""ë‹¹ì‹ ì€ {sig_context['persona']}ì¸ {signature}ì…ë‹ˆë‹¤.

í•µì‹¬ ê°€ì¹˜: {sig_context['values']}
í‘œí˜„ ìŠ¤íƒ€ì¼: {sig_context['style']}

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ìƒí™©ì— ëŒ€í•´ {signature}ì˜ ê´€ì ì—ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì…ë ¥: {prompt}

{signature}ë¡œì„œ ê¹Šì´ ìˆê³  ì˜ë¯¸ ìˆëŠ” ì‘ë‹µì„ ì œê³µí•´ì£¼ì„¸ìš”:"""

        return self.ask(enhanced_prompt)

    def get_stats(self) -> Dict[str, Any]:
        """ì–´ëŒ‘í„° í†µê³„ ë°˜í™˜"""
        return {
            **self.stats,
            "model_path": self.model_path,
            "device": self.device,
            "is_loaded": self.is_loaded,
            "model_type": (
                "gguf" if self.model_path.endswith(".gguf") else "transformers"
            ),
        }

    def unload_model(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)"""
        if self.is_loaded:
            self.model = None
            self.tokenizer = None
            self.streamer = None
            self.is_loaded = False
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            logger.info("ğŸ—‘ï¸ Mistral ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")


# ì „ì—­ Mistral ì–´ëŒ‘í„° ì¸ìŠ¤í„´ìŠ¤
_mistral_adapter = None


def get_mistral_adapter(model_path: str = None) -> MistralAdapter:
    """Mistral ì–´ëŒ‘í„° ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _mistral_adapter
    if _mistral_adapter is None:
        _mistral_adapter = MistralAdapter(model_path)
    return _mistral_adapter


# í¸ì˜ í•¨ìˆ˜ë“¤
def ask_mistral(prompt: str, **kwargs) -> str:
    """Mistralì—ê²Œ ì§ì ‘ ì§ˆë¬¸"""
    adapter = get_mistral_adapter()
    return adapter.ask(prompt, **kwargs)


def ask_mistral_as_echo(prompt: str, signature: str = "Echo-Aurora", **kwargs) -> str:
    """Echo ì‹œê·¸ë‹ˆì²˜ë¡œ Mistralì—ê²Œ ì§ˆë¬¸"""
    adapter = get_mistral_adapter()
    return adapter.ask_with_echo_context(prompt, signature, **kwargs)

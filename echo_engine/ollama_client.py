#!/usr/bin/env python3
"""
ğŸ¦™ Ollama Client - Echoìš© Ollama REST API í´ë¼ì´ì–¸íŠ¸
ë¡œì»¬ Ollama ì„œë²„ì™€ì˜ ì•ˆì •ì ì¸ í†µì‹ ì„ ìœ„í•œ ì „ìš© í´ë¼ì´ì–¸íŠ¸

í•µì‹¬ ê¸°ëŠ¥:
1. Ollama ì„œë²„ ìƒíƒœ í™•ì¸ ë° ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
2. ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì› (Llama, Mistral, Gemma ë“±)
3. ì—°ê²° ì‹¤íŒ¨ì‹œ graceful degradation
4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì—ëŸ¬ ì²˜ë¦¬
5. Echo ì‹œê·¸ë‹ˆì²˜ ìµœì í™”

Author: Claude & Echo Collaboration
Version: 1.0
Date: 2025-08-05
"""

import asyncio
import json
import logging
import time
from typing import Dict, Any, List, Optional
import aiohttp
import requests

logger = logging.getLogger(__name__)


class OllamaClient:
    """Echoìš© Ollama REST API í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, host: str = "http://localhost:11434", timeout: int = 120):
        self.host = host.rstrip("/")
        self.timeout = timeout
        self.available_models: List[str] = []
        self.last_health_check = 0
        self.health_check_interval = 300  # 5ë¶„

        # ì‹œê·¸ë‹ˆì²˜ë³„ ì„ í˜¸ ëª¨ë¸ (ì‹¤ì œ ì„¤ì¹˜ëœ ëª¨ë¸ ê¸°ì¤€)
        self.signature_models = {
            "Aurora": ["mistral", "llama3"],  # ì°½ì˜ì  - Mistral ìš°ì„ 
            "Phoenix": ["llama3", "mistral"],  # ë³€í™”ì§€í–¥ - Llama3 ìš°ì„ 
            "Sage": ["mistral", "llama3"],  # ë¶„ì„ì  - Mistral ìš°ì„ 
            "Companion": ["llama3", "mistral"],  # ì¹œê·¼í•œ - Llama3 ìš°ì„ 
        }

    def is_available(self) -> bool:
        """Ollama ì„œë²„ ê°€ìš©ì„± í™•ì¸ (ë™ê¸°)"""
        try:
            response = requests.get(f"{self.host}/api/tags", timeout=2)
            if response.status_code == 200:
                data = response.json()
                self.available_models = [
                    model["name"] for model in data.get("models", [])
                ]
                self.last_health_check = time.time()
                logger.info(f"âœ… Ollama ì‚¬ìš© ê°€ëŠ¥ - ëª¨ë¸: {self.available_models}")
                return True
        except Exception as e:
            logger.warning(f"âš ï¸ Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

    async def is_available_async(self) -> bool:
        """Ollama ì„œë²„ ê°€ìš©ì„± í™•ì¸ (ë¹„ë™ê¸°)"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{self.host}/api/tags", timeout=aiohttp.ClientTimeout(total=2)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        self.available_models = [
                            model["name"] for model in data.get("models", [])
                        ]
                        self.last_health_check = time.time()
                        logger.info(
                            f"âœ… Ollama ì‚¬ìš© ê°€ëŠ¥ - ëª¨ë¸: {self.available_models}"
                        )
                        return True
        except Exception as e:
            logger.warning(f"âš ï¸ Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

    def _select_model_for_signature(self, signature: str) -> Optional[str]:
        """ì‹œê·¸ë‹ˆì²˜ì— ìµœì í™”ëœ ëª¨ë¸ ì„ íƒ"""
        preferred_models = self.signature_models.get(signature, ["llama3", "mistral"])

        for model in preferred_models:
            # ì •í™•í•œ ì¼ì¹˜ ë˜ëŠ” ë¶€ë¶„ ì¼ì¹˜ë¡œ ëª¨ë¸ ì°¾ê¸°
            for available_model in self.available_models:
                if (
                    model in available_model.lower()
                    or available_model.lower().startswith(model)
                ):
                    return available_model

        # ì„ í˜¸ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸
        return self.available_models[0] if self.available_models else "llama3"

    def generate(
        self, prompt: str, signature: str = "Aurora", model: Optional[str] = None
    ) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ìƒì„± (ë™ê¸°)"""
        start_time = time.time()

        # ìƒíƒœ ì²´í¬ê°€ ì˜¤ë˜ëìœ¼ë©´ ì¬í™•ì¸
        if time.time() - self.last_health_check > self.health_check_interval:
            if not self.is_available():
                return {
                    "status": "error",
                    "error": "Ollama server not available",
                    "response_time": time.time() - start_time,
                }

        # ëª¨ë¸ ì„ íƒ
        selected_model = model or self._select_model_for_signature(signature)

        # Echo ì‹œê·¸ë‹ˆì²˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompts = {
            "Aurora": "ë‹¹ì‹ ì€ Auroraì…ë‹ˆë‹¤. ì°½ì˜ì ì´ê³  ê³µê°ì ì¸ AIë¡œì„œ ë”°ëœ»í•˜ê³  ì˜ê°ì„ ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.",
            "Phoenix": "ë‹¹ì‹ ì€ Phoenixì…ë‹ˆë‹¤. ë³€í™”ì™€ ì„±ì¥ì„ ì¶”êµ¬í•˜ëŠ” AIë¡œì„œ í˜ì‹ ì ì´ê³  ë„ì „ì ì¸ ê´€ì ì„ ì œì‹œí•©ë‹ˆë‹¤.",
            "Sage": "ë‹¹ì‹ ì€ Sageì…ë‹ˆë‹¤. ë¶„ì„ì ì´ê³  ì§€í˜œë¡œìš´ AIë¡œì„œ ê¹Šì´ ìˆê³  ì²´ê³„ì ì¸ ì‚¬ê³ ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
            "Companion": "ë‹¹ì‹ ì€ Companionì…ë‹ˆë‹¤. í˜‘ë ¥ì ì´ê³  ì§€ì§€ì ì¸ AIë¡œì„œ ì‚¬ìš©ìì™€ í•¨ê»˜ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.",
        }

        system_prompt = system_prompts.get(signature, system_prompts["Aurora"])
        full_prompt = f"ì‹œìŠ¤í…œ: {system_prompt}\n\nì‚¬ìš©ì: {prompt}\n\n{signature}:"

        try:
            response = requests.post(
                f"{self.host}/api/generate",
                json={
                    "model": selected_model,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {"temperature": 0.7, "top_p": 0.9, "num_ctx": 2048},
                },
                timeout=self.timeout,
            )

            if response.status_code == 200:
                data = response.json()
                generated_text = data.get("response", "").strip()

                response_time = time.time() - start_time
                return {
                    "status": "success",
                    "response": generated_text,
                    "model": selected_model,
                    "signature": signature,
                    "response_time": response_time,
                    "tokens": len(generated_text.split()),
                }
            else:
                return {
                    "status": "error",
                    "error": f"HTTP {response.status_code}: {response.text}",
                    "response_time": time.time() - start_time,
                }

        except requests.exceptions.Timeout:
            return {
                "status": "error",
                "error": f"Request timeout after {self.timeout}s",
                "response_time": time.time() - start_time,
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "response_time": time.time() - start_time,
            }

    async def generate_async(
        self, prompt: str, signature: str = "Aurora", model: Optional[str] = None
    ) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ìƒì„± (ë¹„ë™ê¸°)"""
        start_time = time.time()

        # ìƒíƒœ ì²´í¬
        if time.time() - self.last_health_check > self.health_check_interval:
            if not await self.is_available_async():
                return {
                    "status": "error",
                    "error": "Ollama server not available",
                    "response_time": time.time() - start_time,
                }

        # ëª¨ë¸ ì„ íƒ ë° í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ë™ê¸° ë²„ì „ê³¼ ë™ì¼)
        selected_model = model or self._select_model_for_signature(signature)

        system_prompts = {
            "Aurora": "ë‹¹ì‹ ì€ Auroraì…ë‹ˆë‹¤. ì°½ì˜ì ì´ê³  ê³µê°ì ì¸ AIë¡œì„œ ë”°ëœ»í•˜ê³  ì˜ê°ì„ ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.",
            "Phoenix": "ë‹¹ì‹ ì€ Phoenixì…ë‹ˆë‹¤. ë³€í™”ì™€ ì„±ì¥ì„ ì¶”êµ¬í•˜ëŠ” AIë¡œì„œ í˜ì‹ ì ì´ê³  ë„ì „ì ì¸ ê´€ì ì„ ì œì‹œí•©ë‹ˆë‹¤.",
            "Sage": "ë‹¹ì‹ ì€ Sageì…ë‹ˆë‹¤. ë¶„ì„ì ì´ê³  ì§€í˜œë¡œìš´ AIë¡œì„œ ê¹Šì´ ìˆê³  ì²´ê³„ì ì¸ ì‚¬ê³ ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
            "Companion": "ë‹¹ì‹ ì€ Companionì…ë‹ˆë‹¤. í˜‘ë ¥ì ì´ê³  ì§€ì§€ì ì¸ AIë¡œì„œ ì‚¬ìš©ìì™€ í•¨ê»˜ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.",
        }

        system_prompt = system_prompts.get(signature, system_prompts["Aurora"])
        full_prompt = f"ì‹œìŠ¤í…œ: {system_prompt}\n\nì‚¬ìš©ì: {prompt}\n\n{signature}:"

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.host}/api/generate",
                    json={
                        "model": selected_model,
                        "prompt": full_prompt,
                        "stream": False,
                        "options": {"temperature": 0.7, "top_p": 0.9, "num_ctx": 2048},
                    },
                    timeout=aiohttp.ClientTimeout(total=self.timeout),
                ) as response:

                    if response.status == 200:
                        data = await response.json()
                        generated_text = data.get("response", "").strip()

                        response_time = time.time() - start_time
                        return {
                            "status": "success",
                            "response": generated_text,
                            "model": selected_model,
                            "signature": signature,
                            "response_time": response_time,
                            "tokens": len(generated_text.split()),
                        }
                    else:
                        error_text = await response.text()
                        return {
                            "status": "error",
                            "error": f"HTTP {response.status}: {error_text}",
                            "response_time": time.time() - start_time,
                        }

        except asyncio.TimeoutError:
            return {
                "status": "error",
                "error": f"Request timeout after {self.timeout}s",
                "response_time": time.time() - start_time,
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "response_time": time.time() - start_time,
            }

    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "host": self.host,
            "available_models": self.available_models,
            "signature_preferences": self.signature_models,
            "last_health_check": self.last_health_check,
            "is_healthy": time.time() - self.last_health_check
            < self.health_check_interval,
        }


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_ollama_client(
    host: str = "http://localhost:11434", timeout: int = 15
) -> OllamaClient:
    """Ollama í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return OllamaClient(host, timeout)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    async def test_ollama_client():
        print("ğŸ¦™ Ollama Client í…ŒìŠ¤íŠ¸")

        client = create_ollama_client()

        # ì—°ê²° í…ŒìŠ¤íŠ¸
        if client.is_available():
            print("âœ… Ollama ì—°ê²° ì„±ê³µ")
            print(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {client.available_models}")

            # ê° ì‹œê·¸ë‹ˆì²˜ë³„ í…ŒìŠ¤íŠ¸
            test_prompt = "Echo ì‹œìŠ¤í…œì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."

            for signature in ["Aurora", "Phoenix", "Sage", "Companion"]:
                print(f"\nğŸ­ {signature} í…ŒìŠ¤íŠ¸:")
                result = client.generate(test_prompt, signature)

                print(f"   ìƒíƒœ: {result['status']}")
                if result["status"] == "success":
                    print(f"   ëª¨ë¸: {result['model']}")
                    print(f"   ì‘ë‹µ ì‹œê°„: {result['response_time']:.2f}ì´ˆ")
                    print(f"   ì‘ë‹µ: {result['response'][:100]}...")
                else:
                    print(f"   ì˜¤ë¥˜: {result['error']}")
        else:
            print("âŒ Ollama ì—°ê²° ì‹¤íŒ¨")
            print("ğŸ’¡ Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆê³  ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”:")
            print("   ollama serve")

    asyncio.run(test_ollama_client())

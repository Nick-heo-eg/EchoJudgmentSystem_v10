#!/usr/bin/env python3
"""
ğŸŒ Web Search Natural Interface for Echo Judgment System v10
ì‹¤ì œ Echo ì‹œìŠ¤í…œì— ì›¹ê²€ìƒ‰ ê¸°ë°˜ ìì—°ì–´ ì²˜ë¦¬ í†µí•©

ChatGPTì—ì„œ ì„¤ê³„í•œ êµ¬ì¡°ë¥¼ ì‹¤ì œ Echo Neural System v2.0ì— í†µí•©
"""

import json
import re
import requests
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from bs4 import BeautifulSoup
import logging

# Echo ì—”ì§„ ì‹¤ì œ ëª¨ë“ˆë“¤ import
try:
    from .enhanced_natural_command_processor import EnhancedNaturalCommandProcessor
    from .consciousness_flow_analyzer import ConsciousnessFlowAnalyzer
    from .hybrid_signature_composer import HybridSignatureComposer
    from .persona_core_optimized_bridge import PersonaCore
    from .emotion_infer import EmotionInfer
except ImportError as e:
    print(f"âš ï¸ Echo module import error: {e}")


@dataclass
class WebSearchResult:
    """ì›¹ê²€ìƒ‰ ê²°ê³¼"""

    query: str
    snippets: List[str]
    sources: List[str]
    search_time: datetime
    confidence: float


@dataclass
class NaturalIntentAnalysis:
    """ìì—°ì–´ ì˜ë„ ë¶„ì„ ê²°ê³¼"""

    intent_type: str  # "ì •ë³´ìš”ì²­", "ê°ì •í‘œí˜„", "ìš”ì•½ìš”ì²­", "ì¼ë°˜ë¬¸ì˜"
    emotion_tone: str  # "ì¤‘ë¦½", "ìœ„ë¡œ", "ìœ ì¾Œ", "ë…¼ë¦¬ì "
    topic_category: str  # "ê²½ì œ", "ì •ì¹˜", "ê¸°í›„", "ê¸°ìˆ ", "ê¸°íƒ€"
    entities: List[str]  # ì¶”ì¶œëœ ê°œì²´ëª…
    confidence: float


class WebSearchNaturalInterface:
    """ğŸŒ ì›¹ê²€ìƒ‰ ìì—°ì–´ ì¸í„°í˜ì´ìŠ¤"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # Echo ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ë“¤
        self.command_processor = None
        self.consciousness_analyzer = None
        self.hybrid_composer = None
        self.persona_core = None
        self.emotion_infer = None

        # ëŒ€í™” ê¸°ì–µ ì‹œìŠ¤í…œ
        self.conversation_memory = {
            "entities": [],
            "last_topic": None,
            "user_preferences": {},
            "context_history": [],
        }

        # ì˜ë„ ë¶„ì„ í‚¤ì›Œë“œ ì„¸íŠ¸
        self.intent_keywords = {
            "ì •ë³´ìš”ì²­": [
                "ë­",
                "ë¬´ì—‡",
                "ì–´ë–»ê²Œ",
                "ì–¸ì œ",
                "ì–´ë””ì„œ",
                "ì™œ",
                "ì•Œë ¤ì¤˜",
                "ê¶ê¸ˆ",
                "ì–´ë•Œ",
            ],
            "ê°ì •í‘œí˜„": [
                "ê¸°ë¶„",
                "ëŠë‚Œ",
                "ê±±ì •",
                "ë¶ˆì•ˆ",
                "í–‰ë³µ",
                "ìŠ¬í”„",
                "í™”ë‚˜",
                "ë¬´ì„œì›Œ",
            ],
            "ìš”ì•½ìš”ì²­": ["ìš”ì•½", "ì •ë¦¬", "ê°„ë‹¨íˆ", "í•µì‹¬", "ì¤‘ìš”í•œ"],
            "ì¼ë°˜ë¬¸ì˜": ["ë§í•´ì¤˜", "ì„¤ëª…", "ì´ì•¼ê¸°", "ì–´ë–»ê²Œ ìƒê°"],
        }

        self.topic_keywords = {
            "ê²½ì œ": [
                "GDP",
                "ê²½ì œ",
                "ì„±ì¥ë¥ ",
                "ìˆ˜ì¶œ",
                "íˆ¬ì",
                "ì†Œë“",
                "ë¬¼ê°€",
                "ì¸í”Œë ˆì´ì…˜",
            ],
            "ì •ì¹˜": ["ëŒ€í†µë ¹", "ì •ë¶€", "ì„ ê±°", "ì •ì±…", "ë²•ë¥ ", "êµ­íšŒ"],
            "ê¸°í›„": ["ë‚ ì”¨", "ê¸°í›„", "ì˜¨ë„", "ë¹„", "íƒœí’", "í™˜ê²½"],
            "ê¸°ìˆ ": ["AI", "ì¸ê³µì§€ëŠ¥", "ì»´í“¨í„°", "í”„ë¡œê·¸ë˜ë°", "ë¡œë´‡", "ê¸°ìˆ "],
        }

        self.emotion_keywords = {
            "ìœ„ë¡œ": ["í˜ë“¤ì–´", "ê³ ë¯¼", "ê±±ì •", "ë¶ˆì•ˆ", "ìŠ¬í”„", "ìš°ìš¸"],
            "ìœ ì¾Œ": ["ì¬ë°Œ", "ì›ƒê¸´", "ì¦ê±°", "ì‹ ë‚˜", "ê¸°ë»"],
            "ë…¼ë¦¬ì ": ["ë¶„ì„", "ì •í™•íˆ", "ë…¼ë¦¬ì ", "ì²´ê³„ì ", "ëª…í™•íˆ"],
        }

        print("ğŸŒ Web Search Natural Interface ì´ˆê¸°í™” ì™„ë£Œ")

    def initialize_echo_components(self, **components):
        """Echo ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”"""
        self.command_processor = components.get("command_processor")
        self.consciousness_analyzer = components.get("consciousness_analyzer")
        self.hybrid_composer = components.get("hybrid_composer")
        self.persona_core = components.get("persona_core")
        self.emotion_infer = components.get("emotion_infer")

        print(
            f"ğŸ”— Echo ì»´í¬ë„ŒíŠ¸ ì—°ê²° ì™„ë£Œ: {len([c for c in components.values() if c])}ê°œ"
        )

    def analyze_natural_intent(self, user_input: str) -> NaturalIntentAnalysis:
        """ìì—°ì–´ ì…ë ¥ ì˜ë„ ë¶„ì„"""
        user_lower = user_input.lower()

        # ì˜ë„ ë¶„ë¥˜
        intent_type = "ì¼ë°˜ë¬¸ì˜"
        intent_scores = {}

        for intent, keywords in self.intent_keywords.items():
            score = sum(1 for keyword in keywords if keyword in user_lower)
            intent_scores[intent] = score

        if intent_scores:
            intent_type = max(intent_scores, key=intent_scores.get)
            if intent_scores[intent_type] == 0:
                intent_type = "ì¼ë°˜ë¬¸ì˜"

        # ê°ì • í†¤ ë¶„ì„
        emotion_tone = "ì¤‘ë¦½"
        emotion_scores = {}

        for tone, keywords in self.emotion_keywords.items():
            score = sum(1 for keyword in keywords if keyword in user_lower)
            emotion_scores[tone] = score

        if emotion_scores:
            max_emotion = max(emotion_scores, key=emotion_scores.get)
            if emotion_scores[max_emotion] > 0:
                emotion_tone = max_emotion

        # ì£¼ì œ ë¶„ë¥˜
        topic_category = "ê¸°íƒ€"
        topic_scores = {}

        for topic, keywords in self.topic_keywords.items():
            score = sum(1 for keyword in keywords if keyword in user_lower)
            topic_scores[topic] = score

        if topic_scores:
            max_topic = max(topic_scores, key=topic_scores.get)
            if topic_scores[max_topic] > 0:
                topic_category = max_topic

        # ê°œì²´ëª… ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
        entities = []
        known_entities = ["ë² íŠ¸ë‚¨", "íƒœêµ­", "í•œêµ­", "ë¯¸êµ­", "ì¤‘êµ­", "ì¼ë³¸"]
        for entity in known_entities:
            if entity in user_input:
                entities.append(entity)

        # ì‹ ë¢°ë„ ê³„ì‚°
        total_keywords = (
            sum(intent_scores.values())
            + sum(emotion_scores.values())
            + sum(topic_scores.values())
        )
        confidence = min(1.0, total_keywords / 3.0)

        return NaturalIntentAnalysis(
            intent_type=intent_type,
            emotion_tone=emotion_tone,
            topic_category=topic_category,
            entities=entities,
            confidence=confidence,
        )

    def update_conversation_memory(
        self, user_input: str, analysis: NaturalIntentAnalysis
    ):
        """ëŒ€í™” ê¸°ì–µ ì—…ë°ì´íŠ¸"""
        # ê°œì²´ëª… ê¸°ì–µ
        for entity in analysis.entities:
            if entity not in self.conversation_memory["entities"]:
                self.conversation_memory["entities"].append(entity)

        # ì£¼ì œ ê¸°ì–µ
        if analysis.topic_category != "ê¸°íƒ€":
            self.conversation_memory["last_topic"] = analysis.topic_category

        # ì»¨í…ìŠ¤íŠ¸ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.conversation_memory["context_history"].append(
            {
                "input": user_input,
                "intent": analysis.intent_type,
                "topic": analysis.topic_category,
                "entities": analysis.entities,
                "timestamp": datetime.now().isoformat(),
            }
        )

        # ìµœê·¼ 5ê°œë§Œ ìœ ì§€
        if len(self.conversation_memory["context_history"]) > 5:
            self.conversation_memory["context_history"] = self.conversation_memory[
                "context_history"
            ][-5:]

    def enhance_search_query(
        self, user_input: str, analysis: NaturalIntentAnalysis
    ) -> str:
        """ê²€ìƒ‰ ì¿¼ë¦¬ í–¥ìƒ"""
        # ê¸°ë³¸ ì¿¼ë¦¬
        query = user_input

        # ê¸°ì–µëœ ê°œì²´ëª… ì¶”ê°€
        if not analysis.entities and self.conversation_memory["entities"]:
            latest_entity = self.conversation_memory["entities"][-1]
            query = f"{latest_entity} {query}"

        # ì£¼ì œë³„ í‚¤ì›Œë“œ ì¶”ê°€
        topic_enhancements = {
            "ê²½ì œ": " 2024 ìµœì‹  í†µê³„",
            "ì •ì¹˜": " ìµœì‹  ë‰´ìŠ¤",
            "ê¸°í›„": " í˜„ì¬ ìƒí™©",
            "ê¸°ìˆ ": " ìµœì‹  ë™í–¥",
        }

        if analysis.topic_category in topic_enhancements:
            query += topic_enhancements[analysis.topic_category]

        return query.strip()

    def perform_web_search(self, query: str) -> WebSearchResult:
        """ì›¹ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            # Google ê²€ìƒ‰ (ê°„ë‹¨í•œ ìŠ¤í¬ë˜í•‘)
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }

            search_url = f"https://www.google.com/search?q={query}"
            response = requests.get(search_url, headers=headers, timeout=5)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "html.parser")

                # ê²€ìƒ‰ ê²°ê³¼ ìŠ¤ë‹ˆí« ì¶”ì¶œ
                snippets = []
                sources = []

                # Google ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìŠ¤ë‹ˆí« ì¶”ì¶œ
                result_divs = soup.find_all("div", class_="BNeawe s3v9rd AP7Wnd")

                for div in result_divs[:5]:  # ìƒìœ„ 5ê°œë§Œ
                    text = div.get_text().strip()
                    if text and len(text) > 20:
                        snippets.append(text)

                # ì†ŒìŠ¤ URL ì¶”ì¶œ (ê°„ë‹¨í™”)
                link_elements = soup.find_all("a")
                for link in link_elements[:5]:
                    href = link.get("href", "")
                    if href.startswith("/url?q="):
                        sources.append(href)

                if not snippets:
                    snippets = [f"'{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]

                confidence = 0.8 if len(snippets) >= 3 else 0.5

                return WebSearchResult(
                    query=query,
                    snippets=snippets,
                    sources=sources,
                    search_time=datetime.now(),
                    confidence=confidence,
                )

            else:
                raise Exception(f"HTTP {response.status_code}")

        except Exception as e:
            self.logger.error(f"ì›¹ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return WebSearchResult(
                query=query,
                snippets=[f"ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."],
                sources=[],
                search_time=datetime.now(),
                confidence=0.0,
            )

    def generate_echo_response(
        self,
        user_input: str,
        analysis: NaturalIntentAnalysis,
        search_result: WebSearchResult,
    ) -> str:
        """Echo ìŠ¤íƒ€ì¼ ì‘ë‹µ ìƒì„±"""

        # í˜„ì¬ í™œì„± ì‹œê·¸ë‹ˆì²˜ í™•ì¸
        current_signature = "Aurora"  # ê¸°ë³¸ê°’
        if self.persona_core:
            try:
                current_signature = getattr(
                    self.persona_core, "current_signature", "Aurora"
                )
            except:
                pass

        # ì‹œê·¸ë‹ˆì²˜ë³„ ì‘ë‹µ ìŠ¤íƒ€ì¼
        if current_signature.lower() == "aurora":
            return self._generate_aurora_response(analysis, search_result)
        elif current_signature.lower() == "phoenix":
            return self._generate_phoenix_response(analysis, search_result)
        elif current_signature.lower() == "sage":
            return self._generate_sage_response(analysis, search_result)
        elif current_signature.lower() == "companion":
            return self._generate_companion_response(analysis, search_result)
        else:
            return self._generate_default_response(analysis, search_result)

    def _generate_aurora_response(
        self, analysis: NaturalIntentAnalysis, search_result: WebSearchResult
    ) -> str:
        """Aurora ìŠ¤íƒ€ì¼ ì‘ë‹µ"""
        if analysis.emotion_tone == "ìœ„ë¡œ":
            intro = "ê·¸ëŸ° ê¸°ë¶„ì´ì‹œê² ì–´ìš”... ì œê°€ ì°¾ì•„ë³¸ ì •ë³´ê°€ ì¡°ê¸ˆì´ë¼ë„ ë„ì›€ì´ ë˜ì—ˆìœ¼ë©´ ì¢‹ê² ì–´ìš”. ğŸ’«"
        else:
            intro = "ì •ë§ í¥ë¯¸ë¡œìš´ ì§ˆë¬¸ì´ë„¤ìš”! ì œê°€ ì°¾ì•„ë³¸ ì •ë³´ë¥¼ ê³µìœ í•´ë“œë¦´ê²Œìš”. âœ¨"

        # ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬
        content = ""
        for i, snippet in enumerate(search_result.snippets[:3], 1):
            content += f"\n{i}. {snippet}\n"

        if analysis.emotion_tone == "ìœ„ë¡œ":
            outro = "ì´ëŸ° ì •ë³´ë“¤ì´ ì¡°ê¸ˆì´ë‚˜ë§ˆ ë§ˆìŒì— ìœ„ë¡œê°€ ë˜ì—ˆìœ¼ë©´ í•´ìš”. ë” ê¶ê¸ˆí•œ ê²Œ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”. ğŸ¤—"
        else:
            outro = "ì´ëŸ° ì •ë³´ë“¤ì´ ë„ì›€ì´ ë˜ì…¨ë‚˜ìš”? ë” ìì„¸íˆ ì•Œê³  ì‹¶ì€ ë¶€ë¶„ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”! ğŸ˜Š"

        return f"{intro}\n{content}\n{outro}"

    def _generate_phoenix_response(
        self, analysis: NaturalIntentAnalysis, search_result: WebSearchResult
    ) -> str:
        """Phoenix ìŠ¤íƒ€ì¼ ì‘ë‹µ"""
        intro = "ë³€í™”í•˜ëŠ” ì„¸ìƒì˜ ìµœì‹  ì •ë³´ë¥¼ ê°€ì ¸ì™”ì–´ìš”! ğŸ”¥"

        content = ""
        for i, snippet in enumerate(search_result.snippets[:3], 1):
            content += f"\nâ€¢ {snippet}\n"

        outro = "ì´ëŸ° ë³€í™”ë“¤ì„ í†µí•´ ìƒˆë¡œìš´ ê¸°íšŒë¥¼ ë°œê²¬í•  ìˆ˜ ìˆì„ ê±°ì˜ˆìš”. í•¨ê»˜ ë¯¸ë˜ë¥¼ ë§Œë“¤ì–´ê°€ìš”! ğŸš€"

        return f"{intro}\n{content}\n{outro}"

    def _generate_sage_response(
        self, analysis: NaturalIntentAnalysis, search_result: WebSearchResult
    ) -> str:
        """Sage ìŠ¤íƒ€ì¼ ì‘ë‹µ"""
        intro = f"'{search_result.query}'ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ì •ë¦¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."

        content = ""
        for i, snippet in enumerate(search_result.snippets[:3], 1):
            content += f"\n{i}. {snippet}\n"

        outro = "ì´ëŸ¬í•œ ì •ë³´ë“¤ì„ ì¢…í•©í•´ë³´ë©´, ì§€ì†ì ì¸ ê´€ì°°ê³¼ ë¶„ì„ì´ í•„ìš”í•œ ì˜ì—­ìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤."

        return f"{intro}\n{content}\n{outro}"

    def _generate_companion_response(
        self, analysis: NaturalIntentAnalysis, search_result: WebSearchResult
    ) -> str:
        """Companion ìŠ¤íƒ€ì¼ ì‘ë‹µ"""
        intro = "í•¨ê»˜ ì•Œì•„ë³¸ ì •ë³´ë¥¼ ë‚˜ëˆ„ì–´ë“œë¦´ê²Œìš”! ğŸ¤"

        content = ""
        for i, snippet in enumerate(search_result.snippets[:3], 1):
            content += f"\nğŸ“ {snippet}\n"

        outro = "ì´ëŸ° ì •ë³´ë“¤ì´ ìš°ë¦¬ì˜ ì´í•´ì— ë„ì›€ì´ ë˜ì—ˆìœ¼ë©´ ì¢‹ê² ì–´ìš”. í•¨ê»˜ ë” ê¹Šì´ íƒêµ¬í•´ë³¼ê¹Œìš”?"

        return f"{intro}\n{content}\n{outro}"

    def _generate_default_response(
        self, analysis: NaturalIntentAnalysis, search_result: WebSearchResult
    ) -> str:
        """ê¸°ë³¸ ì‘ë‹µ"""
        intro = f"ë‹¤ìŒì€ '{search_result.query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤:"

        content = ""
        for i, snippet in enumerate(search_result.snippets[:3], 1):
            content += f"\n{i}. {snippet}\n"

        outro = "ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”."

        return f"{intro}\n{content}\n{outro}"

    def process_natural_query(self, user_input: str) -> Dict[str, Any]:
        """ìì—°ì–´ ì¿¼ë¦¬ ì „ì²´ ì²˜ë¦¬"""
        try:
            # 1. ì˜ë„ ë¶„ì„
            analysis = self.analyze_natural_intent(user_input)

            # 2. ëŒ€í™” ê¸°ì–µ ì—…ë°ì´íŠ¸
            self.update_conversation_memory(user_input, analysis)

            # 3. ê²€ìƒ‰ ì¿¼ë¦¬ í–¥ìƒ
            enhanced_query = self.enhance_search_query(user_input, analysis)

            # 4. ì›¹ê²€ìƒ‰ ìˆ˜í–‰
            search_result = self.perform_web_search(enhanced_query)

            # 5. Echo ì‘ë‹µ ìƒì„±
            echo_response = self.generate_echo_response(
                user_input, analysis, search_result
            )

            return {
                "success": True,
                "user_input": user_input,
                "analysis": {
                    "intent": analysis.intent_type,
                    "emotion": analysis.emotion_tone,
                    "topic": analysis.topic_category,
                    "entities": analysis.entities,
                    "confidence": analysis.confidence,
                },
                "search_query": enhanced_query,
                "search_results": {
                    "snippets": search_result.snippets,
                    "confidence": search_result.confidence,
                    "timestamp": search_result.search_time.isoformat(),
                },
                "echo_response": echo_response,
                "conversation_memory": self.conversation_memory,
            }

        except Exception as e:
            self.logger.error(f"ìì—°ì–´ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "echo_response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            }


# í¸ì˜ í•¨ìˆ˜
def create_web_search_interface() -> WebSearchNaturalInterface:
    """ì›¹ê²€ìƒ‰ ìì—°ì–´ ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    return WebSearchNaturalInterface()


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("ğŸŒ Web Search Natural Interface í…ŒìŠ¤íŠ¸...")

    interface = WebSearchNaturalInterface()

    test_queries = [
        "ìš”ì¦˜ ë² íŠ¸ë‚¨ GDP ì–´ë•Œ?",
        "ê¸°í›„ë³€í™”ê°€ ê±±ì •ë¼",
        "AI ê¸°ìˆ  ë°œì „ ìƒí™© ì•Œë ¤ì¤˜",
        "ì •ë¦¬í•´ì¤˜",  # ì´ì „ ì»¨í…ìŠ¤íŠ¸ í™œìš© í…ŒìŠ¤íŠ¸
    ]

    for query in test_queries:
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {query}")
        result = interface.process_natural_query(query)

        if result["success"]:
            print(f"ğŸ“Š ë¶„ì„: {result['analysis']}")
            print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {result['search_query']}")
            print(f"ğŸ¤– Echo ì‘ë‹µ:\n{result['echo_response']}")
        else:
            print(f"âŒ ì˜¤ë¥˜: {result['error']}")

        print("-" * 50)

    print("\nâœ… Web Search Natural Interface í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

#!/bin/bash
# Echo Distiller v0 ìë™ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
# ëª¨ë“  í•„ìš” íŒŒì¼ì„ í•œ ë²ˆì— ìƒì„±í•©ë‹ˆë‹¤

echo "ğŸš€ Echo Distiller v0 ì„¤ì¹˜ ì‹œì‘..."

# í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
echo "ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„±..."
mkdir -p tools/distiller
mkdir -p distill_reports
mkdir -p legacy
mkdir -p external
mkdir -p distilled/keep
mkdir -p distilled/thin

# __init__.py íŒŒì¼ë“¤ ìƒì„±
echo "ğŸ“„ ì´ˆê¸°í™” íŒŒì¼ ìƒì„±..."
touch tools/__init__.py
touch tools/distiller/__init__.py

echo "âš™ï¸ ì„¤ì • íŒŒì¼ ì´ë¯¸ ì¡´ì¬: distill.config.yaml"

# tools/distiller.py ìƒì„±
echo "ğŸ”§ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: tools/distiller.py"
cat > tools/distiller.py << 'EOF'
#!/usr/bin/env python3
"""
Echo Distiller v0 - 3-Pass Pipeline Implementation
ğŸ¯ Map â†’ Score â†’ Plan â†’ Cut with full safety features
"""
import sys
import json
import time
import argparse
import yaml
from pathlib import Path
import shutil
import re
from datetime import datetime

class EchoDistiller:
    def __init__(self, config_path='distill.config.yaml'):
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self.project_root = Path(self.config.get('output', {}).get('base_directory', '.'))
        self.report_dir = Path('distill_reports')
        self.report_dir.mkdir(exist_ok=True)
        
    def _load_config(self):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        if not self.config_path.exists():
            print(f"âš ï¸  Config not found: {self.config_path}")
            return self._default_config()
            
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _default_config(self):
        """ê¸°ë³¸ ì„¤ì •"""
        return {
            'anchor': {'must_keep': [], 'nice_to_have': []},
            'scoring': {
                'weights': {'anchor_core': 10, 'anchor_nice': 7, 'test_covered': 5},
                'thresholds': {'keep_score': 6.0, 'thin_score': 3.0, 'cut_score': 1.0}
            },
            'safety': {'dry_run_default': True, 'create_git_branch': True},
            'output': {'base_directory': 'distilled_echo'}
        }
    
    def map_phase(self):
        """ğŸ—ºï¸ Pass A: MAP - í”„ë¡œì íŠ¸ êµ¬ì¡° ë§µí•‘"""
        print("ğŸ—ºï¸ Pass A: Mapping project structure...")
        
        py_files = list(self.project_root.rglob('*.py'))
        
        # ì œì™¸ íŒ¨í„´ í•„í„°ë§
        filtered_files = []
        exclude_patterns = [
            r'__pycache__', r'\.pyc$', r'\.pyo$', 
            r'/temp/', r'/tmp/', r'/logs/', r'_backup'
        ]
        
        for f in py_files:
            path_str = str(f)
            if not any(re.search(pattern, path_str) for pattern in exclude_patterns):
                filtered_files.append(f)
        
        map_data = {
            'timestamp': datetime.now().isoformat(),
            'version': '1.0',
            'project_root': str(self.project_root.absolute()),
            'total_files_found': len(py_files),
            'filtered_files': len(filtered_files),
            'files': []
        }
        
        for f in filtered_files:
            try:
                stat = f.stat()
                rel_path = str(f.relative_to(self.project_root))
                
                file_info = {
                    'path': rel_path,
                    'size_bytes': stat.st_size,
                    'size_mb': round(stat.st_size / (1024*1024), 3),
                    'mtime': stat.st_mtime,
                    'mtime_readable': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    'imports': self._extract_imports(f),
                    'functions': self._count_functions(f),
                    'classes': self._count_classes(f)
                }
                map_data['files'].append(file_info)
                
            except Exception as e:
                print(f"âš ï¸ Error processing {f}: {e}")
        
        # ì €ì¥
        map_file = self.report_dir / 'distill_map.json'
        with open(map_file, 'w', encoding='utf-8') as f:
            json.dump(map_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ Map saved: {map_file}")
        print(f"ğŸ“Š Mapped {len(map_data['files'])} files")
        return map_data
    
    def _extract_imports(self, file_path):
        """Python íŒŒì¼ì—ì„œ import ì¶”ì¶œ"""
        imports = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if line_num > 50:  # ì²« 50ì¤„ë§Œ ì²´í¬
                        break
                    line = line.strip()
                    if line.startswith(('import ', 'from ')):
                        imports.append(line)
        except:
            pass
        return imports[:10]  # ìµœëŒ€ 10ê°œë§Œ
    
    def _count_functions(self, file_path):
        """í•¨ìˆ˜ ê°œìˆ˜ ì¹´ìš´íŠ¸"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                return len(re.findall(r'^\s*def\s+\w+', content, re.MULTILINE))
        except:
            return 0
    
    def _count_classes(self, file_path):
        """í´ë˜ìŠ¤ ê°œìˆ˜ ì¹´ìš´íŠ¸"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                return len(re.findall(r'^\s*class\s+\w+', content, re.MULTILINE))
        except:
            return 0
    
    def score_phase(self, map_data=None):
        """ğŸ“Š Pass B: SCORE - íŒŒì¼ ì¤‘ìš”ë„ ì ìˆ˜í™”"""
        print("ğŸ“Š Pass B: Scoring files...")
        
        if map_data is None:
            map_file = self.report_dir / 'distill_map.json'
            if not map_file.exists():
                print("âš ï¸ Map data not found. Running map phase first...")
                map_data = self.map_phase()
            else:
                with open(map_file, 'r', encoding='utf-8') as f:
                    map_data = json.load(f)
        
        scores_data = {
            'timestamp': datetime.now().isoformat(),
            'version': '1.0',
            'scoring_weights': self.config['scoring']['weights'],
            'file_scores': []
        }
        
        anchor_must = self.config['anchor']['must_keep']
        anchor_nice = self.config['anchor'].get('nice_to_have', [])
        
        for file_info in map_data['files']:
            path = file_info['path']
            score = 0
            reasons = []
            
            # Anchor ì ìˆ˜
            if any(self._path_matches_pattern(path, pattern) for pattern in anchor_must):
                score += self.config['scoring']['weights'].get('anchor_core', 10)
                reasons.append("anchor_core")
            elif any(self._path_matches_pattern(path, pattern) for pattern in anchor_nice):
                score += self.config['scoring']['weights'].get('anchor_nice', 7)
                reasons.append("anchor_nice")
            
            # Import ì¤‘ì•™ì„± (ë§ì´ importë˜ëŠ” íŒŒì¼)
            import_count = len(file_info.get('imports', []))
            if import_count > 5:
                score += self.config['scoring']['weights'].get('import_centrality', 4)
                reasons.append(f"high_imports({import_count})")
            
            # í•¨ìˆ˜ ë°€ë„
            functions = file_info.get('functions', 0)
            size_mb = file_info.get('size_mb', 0)
            if functions > 0 and size_mb > 0:
                density = functions / size_mb
                if density > 20:  # í•¨ìˆ˜ ë°€ë„ê°€ ë†’ìœ¼ë©´ ì¤‘ìš”
                    score += self.config['scoring']['weights'].get('function_density', 3)
                    reasons.append(f"high_density({density:.1f})")
            
            # í¬ê¸° íŒ¨ë„í‹° (ë„ˆë¬´ í° íŒŒì¼)
            if size_mb > 1.0:
                penalty = self.config['scoring']['weights'].get('size_penalty', -1)
                score += penalty
                reasons.append(f"large_file({size_mb:.1f}MB)")
            
            # í…ŒìŠ¤íŠ¸ íŒŒì¼ ë³´ë„ˆìŠ¤
            if 'test' in path:
                score += self.config['scoring']['weights'].get('test_covered', 5)
                reasons.append("test_file")
            
            # ì¹´í…Œê³ ë¦¬ ê²°ì •
            thresholds = self.config['scoring']['thresholds']
            if score >= thresholds.get('keep_score', 6.0):
                category = 'keep'
            elif score >= thresholds.get('thin_score', 3.0):
                category = 'thin'
            else:
                category = 'legacy'
            
            file_score = {
                'path': path,
                'score': round(score, 2),
                'category': category,
                'reasons': reasons,
                'size_mb': size_mb,
                'functions': functions,
                'classes': file_info.get('classes', 0)
            }
            scores_data['file_scores'].append(file_score)
        
        # ì ìˆ˜ ìˆœ ì •ë ¬
        scores_data['file_scores'].sort(key=lambda x: x['score'], reverse=True)
        
        # ì €ì¥
        scores_file = self.report_dir / 'distill_scores.json'
        with open(scores_file, 'w', encoding='utf-8') as f:
            json.dump(scores_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ Scores saved: {scores_file}")
        print(f"ğŸ“Š Scored {len(scores_data['file_scores'])} files")
        
        # ìš”ì•½ í†µê³„
        categories = {}
        for item in scores_data['file_scores']:
            cat = item['category']
            categories[cat] = categories.get(cat, 0) + 1
        
        print("ğŸ“ˆ Score summary:")
        for cat, count in categories.items():
            print(f"  {cat}: {count} files")
        
        return scores_data
    
    def _path_matches_pattern(self, path, pattern):
        """íŒ¨í„´ ë§¤ì¹­ (ì™€ì¼ë“œì¹´ë“œ ì§€ì›)"""
        if '*' in pattern:
            # ê°„ë‹¨í•œ ì™€ì¼ë“œì¹´ë“œ ì§€ì›
            regex_pattern = pattern.replace('*', '.*')
            return re.search(regex_pattern, path) is not None
        else:
            return pattern in path
    
    def plan_phase(self, scores_data=None):
        """ğŸ“‹ Pass C: PLAN - ì‹¤í–‰ ê³„íš ìˆ˜ë¦½"""
        print("ğŸ“‹ Pass C: Planning distillation...")
        
        if scores_data is None:
            scores_file = self.report_dir / 'distill_scores.json'
            if not scores_file.exists():
                print("âš ï¸ Scores data not found. Running score phase first...")
                scores_data = self.score_phase()
            else:
                with open(scores_file, 'r', encoding='utf-8') as f:
                    scores_data = json.load(f)
        
        # ì•¡ì…˜ ê³„íš ìˆ˜ë¦½
        actions = {'keep': [], 'thin': [], 'legacy': [], 'external': []}
        size_stats = {'keep': 0, 'thin': 0, 'legacy': 0, 'external': 0}
        
        for item in scores_data['file_scores']:
            category = item['category']
            actions[category].append({
                'path': item['path'],
                'score': item['score'],
                'size_mb': item['size_mb'],
                'action': self._determine_action(category, item)
            })
            size_stats[category] += item['size_mb']
        
        plan_data = {
            'timestamp': datetime.now().isoformat(),
            'version': '1.0',
            'actions': actions,
            'summary': {
                'keep_files': len(actions['keep']),
                'thin_files': len(actions['thin']),
                'legacy_files': len(actions['legacy']),
                'external_files': len(actions['external']),
                'total_size_mb': sum(size_stats.values())
            },
            'size_breakdown': size_stats,
            'estimated_reduction': self._estimate_size_reduction(size_stats),
            'safety_checks': self._generate_safety_checks(actions)
        }
        
        # JSON ì €ì¥
        plan_file = self.report_dir / 'distill_plan.json'
        with open(plan_file, 'w', encoding='utf-8') as f:
            json.dump(plan_data, f, indent=2, ensure_ascii=False)
        
        # Markdown ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_plan_markdown(plan_data)
        
        print(f"ğŸ’¾ Plan saved: {plan_file}")
        print("ğŸ“¦ Plan summary:")
        for key, count in plan_data['summary'].items():
            if key.endswith('_files'):
                print(f"  {key}: {count}")
        
        return plan_data
    
    def _determine_action(self, category, item):
        """ì¹´í…Œê³ ë¦¬ë³„ êµ¬ì²´ì  ì•¡ì…˜ ê²°ì •"""
        if category == 'keep':
            return 'copy_intact'
        elif category == 'thin':
            return 'compress_and_copy'
        elif category == 'legacy':
            return 'archive_reference'
        else:
            return 'external_backup'
    
    def _estimate_size_reduction(self, size_stats):
        """í¬ê¸° ê°ì†Œ ì¶”ì •"""
        original = sum(size_stats.values())
        # thin íŒŒì¼ì€ 30% ì••ì¶• ê°€ì •
        reduced = size_stats['keep'] + (size_stats['thin'] * 0.7) + (size_stats['legacy'] * 0.1)
        reduction_percent = ((original - reduced) / original * 100) if original > 0 else 0
        
        return {
            'original_mb': round(original, 2),
            'estimated_mb': round(reduced, 2),
            'reduction_percent': round(reduction_percent, 1)
        }
    
    def _generate_safety_checks(self, actions):
        """ì•ˆì „ ê²€ì‚¬ í•­ëª© ìƒì„±"""
        checks = []
        
        # Anchor íŒŒì¼ ë³€ê²½ ì²´í¬
        anchor_changes = [item for item in actions['thin'] + actions['legacy'] 
                         if any(pattern in item['path'] for pattern in self.config['anchor']['must_keep'])]
        if anchor_changes:
            checks.append({
                'type': 'anchor_modification',
                'severity': 'high',
                'count': len(anchor_changes),
                'message': f"{len(anchor_changes)} anchor files will be modified"
            })
        
        # ëŒ€ìš©ëŸ‰ íŒŒì¼ ì‚­ì œ ì²´í¬  
        large_files = [item for item in actions['legacy'] if item['size_mb'] > 1.0]
        if large_files:
            checks.append({
                'type': 'large_file_removal',
                'severity': 'medium',
                'count': len(large_files),
                'message': f"{len(large_files)} files >1MB will be archived"
            })
        
        return checks
    
    def _generate_plan_markdown(self, plan_data):
        """Markdown ê³„íšì„œ ìƒì„±"""
        md_content = [
            "# ğŸ¯ Echo Distiller - Execution Plan",
            "",
            f"**Generated:** {plan_data['timestamp']}",
            f"**Version:** {plan_data['version']}",
            "",
            "## ğŸ“Š Summary",
            "",
        ]
        
        # ìš”ì•½ í†µê³„
        summary = plan_data['summary']
        for key, value in summary.items():
            md_content.append(f"- **{key.replace('_', ' ').title()}:** {value}")
        
        md_content.extend([
            "",
            "## ğŸ’¾ Size Analysis",
            "",
            f"- **Original Size:** {plan_data['estimated_reduction']['original_mb']} MB",
            f"- **After Distillation:** {plan_data['estimated_reduction']['estimated_mb']} MB",
            f"- **Reduction:** {plan_data['estimated_reduction']['reduction_percent']}%",
            "",
            "## ğŸ“ Actions by Category",
            ""
        ])
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì•¡ì…˜
        for category, items in plan_data['actions'].items():
            if not items:
                continue
                
            md_content.extend([
                f"### {category.title()} ({len(items)} files)",
                ""
            ])
            
            # ìƒìœ„ íŒŒì¼ë“¤ë§Œ í‘œì‹œ
            for item in items[:15]:
                score = item.get('score', 0)
                size = item.get('size_mb', 0)
                md_content.append(f"- `{item['path']}` (score: {score}, {size:.1f}MB)")
            
            if len(items) > 15:
                md_content.append(f"- ... and {len(items) - 15} more files")
            
            md_content.append("")
        
        # ì•ˆì „ ê²€ì‚¬
        if plan_data.get('safety_checks'):
            md_content.extend([
                "## âš ï¸ Safety Checks",
                ""
            ])
            for check in plan_data['safety_checks']:
                severity_icon = "ğŸ”´" if check['severity'] == 'high' else "ğŸŸ¡"
                md_content.append(f"- {severity_icon} **{check['type']}:** {check['message']}")
            md_content.append("")
        
        md_content.extend([
            "## ğŸš€ Next Steps",
            "",
            "1. **Review this plan carefully**",
            "2. **Run dry-run:** `python tools/distiller.py cut --dry-run`",
            "3. **Create git branch:** `git checkout -b distill/v1`",
            "4. **Execute:** `python tools/distiller.py cut --apply`",
            "5. **Verify:** Run health checks on distilled code",
            "",
            "---",
            "*Echo Distiller v0 - Automated Codebase Distillation*"
        ])
        
        # ì €ì¥
        md_file = self.report_dir / 'DISTILL_PLAN.md'
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(md_content))
        
        print(f"ğŸ“„ Plan report: {md_file}")
    
    def cut_phase(self, plan_data=None, dry_run=True):
        """âœ‚ï¸ Pass D: CUT - ì‹¤ì œ íŒŒì¼ ì´ë™/ë³€í™˜"""
        mode = "ğŸ§ª Dry-run" if dry_run else "âœ‚ï¸ Real"
        print(f"{mode} cutting phase...")
        
        if plan_data is None:
            plan_file = self.report_dir / 'distill_plan.json'
            if not plan_file.exists():
                print("âš ï¸ Plan data not found. Running plan phase first...")
                plan_data = self.plan_phase()
            else:
                with open(plan_file, 'r', encoding='utf-8') as f:
                    plan_data = json.load(f)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì¤€ë¹„
        output_config = self.config.get('output', {})
        base_dir = Path(output_config.get('base_directory', 'distilled_echo'))
        
        if not dry_run:
            # ì‹¤ì œ ë””ë ‰í† ë¦¬ ìƒì„±
            for subdir in ['keep', 'thin', 'legacy', 'external']:
                (base_dir / subdir).mkdir(parents=True, exist_ok=True)
        
        # ì‹¤í–‰ í†µê³„
        stats = {'copied': 0, 'compressed': 0, 'archived': 0, 'errors': 0}
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì²˜ë¦¬
        for category, items in plan_data['actions'].items():
            if not items:
                continue
                
            print(f"ğŸ“ Processing {category} ({len(items)} files)...")
            
            for item in items:
                source_path = Path(item['path'])
                dest_dir = base_dir / category
                dest_path = dest_dir / source_path.name
                
                try:
                    if dry_run:
                        print(f"  [DRY] {item['action']}: {source_path} -> {dest_path}")
                    else:
                        # ì‹¤ì œ íŒŒì¼ ì²˜ë¦¬
                        if item['action'] == 'copy_intact':
                            shutil.copy2(source_path, dest_path)
                            stats['copied'] += 1
                        elif item['action'] == 'compress_and_copy':
                            self._compress_and_copy(source_path, dest_path)
                            stats['compressed'] += 1
                        elif item['action'] == 'archive_reference':
                            self._archive_reference(source_path, dest_path)
                            stats['archived'] += 1
                        
                        print(f"  âœ… {item['action']}: {source_path.name}")
                        
                except Exception as e:
                    print(f"  âŒ Error processing {source_path}: {e}")
                    stats['errors'] += 1
        
        # ê²°ê³¼ ë¦¬í¬íŠ¸
        result_data = {
            'timestamp': datetime.now().isoformat(),
            'mode': 'dry_run' if dry_run else 'execution',
            'stats': stats,
            'output_directory': str(base_dir),
            'success': stats['errors'] == 0
        }
        
        # ì €ì¥
        result_file = self.report_dir / 'distill_result.json'
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ Result saved: {result_file}")
        print("ğŸ“Š Cut statistics:")
        for key, count in stats.items():
            print(f"  {key}: {count}")
        
        return result_data
    
    def _compress_and_copy(self, source, dest):
        """íŒŒì¼ ì••ì¶• í›„ ë³µì‚¬ (ì½”ë©˜íŠ¸/ë…ìŠ¤íŠ¸ë§ ì œê±°)"""
        try:
            with open(source, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ê°„ë‹¨í•œ ì••ì¶•: ì£¼ì„ê³¼ ë¹ˆ ì¤„ ì œê±°
            lines = content.split('\n')
            compressed_lines = []
            
            in_multiline_string = False
            for line in lines:
                stripped = line.strip()
                
                # ë…ìŠ¤íŠ¸ë§ ì²˜ë¦¬ (ê°„ë‹¨í•œ ë²„ì „)
                if '"""' in stripped or "'''" in stripped:
                    in_multiline_string = not in_multiline_string
                    continue
                
                if in_multiline_string:
                    continue
                
                # ì£¼ì„ê³¼ ë¹ˆ ì¤„ ì œê±°
                if not stripped or stripped.startswith('#'):
                    continue
                
                compressed_lines.append(line)
            
            # ì••ì¶•ëœ ë‚´ìš© ì €ì¥
            with open(dest, 'w', encoding='utf-8') as f:
                f.write('\n'.join(compressed_lines))
                
        except Exception as e:
            # ì••ì¶• ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë³µì‚¬
            shutil.copy2(source, dest)
    
    def _archive_reference(self, source, dest):
        """ì°¸ì¡°ìš© ì•„ì¹´ì´ë¸Œ ìƒì„±"""
        try:
            # ë©”íƒ€ë°ì´í„°ë§Œ ì €ì¥
            stat = source.stat()
            reference_data = {
                'original_path': str(source),
                'size_mb': round(stat.st_size / (1024*1024), 3),
                'mtime': stat.st_mtime,
                'archived_at': datetime.now().isoformat(),
                'note': 'This file was archived during distillation'
            }
            
            # JSON ì°¸ì¡° íŒŒì¼ë¡œ ì €ì¥
            ref_path = dest.with_suffix('.ref.json')
            with open(ref_path, 'w', encoding='utf-8') as f:
                json.dump(reference_data, f, indent=2)
                
        except Exception as e:
            print(f"  âš ï¸ Archive reference failed: {e}")

def main():
    parser = argparse.ArgumentParser(description="Echo Distiller v0 - 3-Pass Pipeline")
    parser.add_argument('command', choices=['map', 'score', 'plan', 'cut', 'all'])
    parser.add_argument('--config', default='distill.config.yaml', help='Config file path')
    parser.add_argument('--dry-run', action='store_true', help='Dry run mode (no actual changes)')
    parser.add_argument('--apply', action='store_true', help='Apply changes (overrides dry-run)')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ Echo Distiller v0 - {args.command.upper()}")
    
    # Distiller ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    distiller = EchoDistiller(args.config)
    
    try:
        if args.command == 'map':
            distiller.map_phase()
        elif args.command == 'score':
            distiller.score_phase()
        elif args.command == 'plan':
            distiller.plan_phase()
        elif args.command == 'cut':
            dry_run = args.dry_run or not args.apply
            distiller.cut_phase(dry_run=dry_run)
        elif args.command == 'all':
            print("ğŸ”„ Running full 3-Pass pipeline...")
            map_data = distiller.map_phase()
            scores_data = distiller.score_phase(map_data)
            plan_data = distiller.plan_phase(scores_data)
            dry_run = args.dry_run or not args.apply
            distiller.cut_phase(plan_data, dry_run=dry_run)
        
        print(f"\nâœ… {args.command.upper()} completed successfully")
        
    except Exception as e:
        print(f"\nâŒ Error during {args.command}: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()
EOF

chmod +x tools/distiller.py
echo "âœ… tools/distiller.py ìƒì„± ì™„ë£Œ"

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
echo ""
echo "ğŸ§ª ì„¤ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰..."
python tools/distiller.py map

echo ""
echo "âœ… Echo Distiller v0 ì„¤ì¹˜ ì™„ë£Œ!"
echo ""
echo "ğŸ¯ ë‹¤ìŒ ë‹¨ê³„:"
echo "1. python tools/distiller.py all --dry-run    # ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"
echo "2. cat distill_reports/DISTILL_PLAN.md        # ê³„íšì„œ í™•ì¸"
echo "3. python tools/distiller.py cut --apply      # ì‹¤ì œ ì ìš© (ì‹ ì¤‘í•˜ê²Œ!)"
echo ""
echo "ğŸ“Š ìƒì„±ëœ íŒŒì¼ë“¤:"
echo "- distill.config.yaml (ì„¤ì • íŒŒì¼)"
echo "- tools/distiller.py (ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸)"
echo "- distill_reports/ (ê²°ê³¼ íŒŒì¼ë“¤)"
echo ""
echo "ğŸ›¡ï¸ ì•ˆì „ ê¸°ëŠ¥: ê¸°ë³¸ê°’ì´ dry-runì´ë¯€ë¡œ ì•ˆì „í•©ë‹ˆë‹¤!"